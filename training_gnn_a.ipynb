{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02/11 GNN SODA A.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing SODA and Training GNNs\n",
        "\n",
        "by Ding"
      ],
      "metadata": {
        "id": "mzcaDeWwDnML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Preprocessing"
      ],
      "metadata": {
        "id": "lF14MaayDoVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geopandas\n",
        "\n",
        "import numpy as np\n",
        "from netCDF4 import Dataset\n",
        "import xarray as xr\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from geopandas import GeoDataFrame\n",
        "from shapely.geometry import Point\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\", force_remount=True)\n",
        "\n",
        "!cp -a \"/gdrive/MyDrive/soda_331_pt_l5.nc\" \"/content/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdzGdalaDtWI",
        "outputId": "041525d8-27bc-4b5f-d0a7-bc8243d47eff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.7/dist-packages (0.10.2)\n",
            "Requirement already satisfied: shapely>=1.6 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.8.1.post1)\n",
            "Requirement already satisfied: pyproj>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from geopandas) (3.2.1)\n",
            "Requirement already satisfied: fiona>=1.8 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.8.21)\n",
            "Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.3.5)\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (1.15.0)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (7.1.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (2021.10.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (57.4.0)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (0.7.2)\n",
            "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (21.4.0)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (2.5.0)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->geopandas) (1.21.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->geopandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->geopandas) (2.8.2)\n",
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "soda = xr.open_dataset(\"soda_331_pt_l5.nc\", decode_times=False)\n",
        "\n",
        "soda_array = soda.to_array(dim=\"temp\")\n",
        "soda_smaller = soda_array[:,:,:,::5,::5].to_dataset(dim=\"temp\")\n",
        "\n",
        "start_year = 1980\n",
        "end_year = 2009\n",
        "target_year = end_year + +1\n",
        "start_month = (start_year - 1980) * 12\n",
        "end_month = (end_year + 1 - 1980) * 12\n",
        "\n",
        "soda_sst_1980_2009 = np.zeros((end_month-start_month+12,1,66,144)) # Include one more year.\n",
        "soda_sst_1980_2009[:,:,:,:] = soda_smaller.variables[\"temp\"][0:end_month-start_month+12,:,:,:]\n",
        "soda_sst_1980_2009 = np.squeeze(soda_sst_1980_2009, axis=1)\n",
        "print(soda_sst_1980_2009.shape)\n",
        "\n",
        "soda_sst_2010 = np.zeros((12,1,66,144))\n",
        "soda_sst_2010[:,:,:,:] = soda_smaller.variables[\"temp\"][end_month-start_month:end_month-start_month+12,:,:,:]\n",
        "soda_sst_2010 = np.squeeze(soda_sst_2010, axis=1)\n",
        "print(soda_sst_2010.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGUbMZ42LNQe",
        "outputId": "afbe5adc-a903-491f-932e-871cf7079d48"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(372, 66, 144)\n",
            "(12, 66, 144)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "soda_sst_1980_2009_transposed = soda_sst_1980_2009.transpose(1,2,0)\n",
        "soda_sst_1980_2009_flattened = soda_sst_1980_2009_transposed.reshape(soda_sst_1980_2009.shape[1] * soda_sst_1980_2009.shape[2],len(soda_sst_1980_2009))\n",
        "print(soda_sst_1980_2009_flattened.shape)\n",
        "\n",
        "soda_sst_2010_transposed = soda_sst_2010.transpose(1,2,0)\n",
        "soda_sst_2010_flattened = soda_sst_2010_transposed.reshape(soda_sst_2010.shape[1] * soda_sst_2010.shape[2],len(soda_sst_2010))\n",
        "print(soda_sst_2010_flattened.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wo4HSa6jNKww",
        "outputId": "f1015dea-44d5-448e-d9f9-a56939816f8d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9504, 372)\n",
            "(9504, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dropna(arr, *args, **kwarg):\n",
        "    assert isinstance(arr, np.ndarray)\n",
        "    dropped=pd.DataFrame(arr).dropna(*args, **kwarg).values\n",
        "    if arr.ndim==1:\n",
        "        dropped=dropped.flatten()\n",
        "    return dropped\n",
        "\n",
        "soda_sst_ocean_1980_2009_flattened = dropna(soda_sst_1980_2009_flattened)\n",
        "print(soda_sst_ocean_1980_2009_flattened.shape)\n",
        "\n",
        "soda_sst_ocean_2010_flattened = dropna(soda_sst_2010_flattened)\n",
        "print(soda_sst_ocean_2010_flattened.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7mbHccmR6ad",
        "outputId": "b208d02b-40f8-466e-bd04-bbe6b6e73c18"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6924, 372)\n",
            "(6924, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_month = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
        "#target_month = [\"Jan\"]\n",
        "\n",
        "soda_sst_ocean_1980_2009_flattened_all = []\n",
        "soda_sst_ocean_2010_mhw_all = []\n",
        "\n",
        "for month in target_month:\n",
        "\n",
        "  index = target_month.index(month)\n",
        "  soda_sst_ocean_1980_2009_flattened_all.append(soda_sst_ocean_1980_2009_flattened[:,index:(index+soda_sst_ocean_1980_2009_flattened.shape[1]-12)])\n",
        "\n",
        "  soda_sst_ocean_2010_mhw = []\n",
        "  for row in range(len(soda_sst_ocean_1980_2009_flattened)):\n",
        "    sst_node = soda_sst_ocean_1980_2009_flattened[row][index::12]\n",
        "    #print(np.mean(sst_node))\n",
        "    if soda_sst_ocean_2010_flattened[row][index] > sorted(sst_node)[-4]:\n",
        "      soda_sst_ocean_2010_mhw.append(1)\n",
        "    else:\n",
        "      soda_sst_ocean_2010_mhw.append(0)\n",
        "  \n",
        "  print(month, target_year, \":\", soda_sst_ocean_2010_mhw.count(1), \"MHW\")\n",
        "  print(month, target_year, \":\", soda_sst_ocean_2010_mhw.count(0), \"no MHW\")\n",
        "  print()\n",
        "\n",
        "  soda_sst_ocean_2010_mhw = np.array(soda_sst_ocean_2010_mhw)\n",
        "  soda_sst_ocean_2010_mhw_all.append(soda_sst_ocean_2010_mhw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr1xOR6poSbL",
        "outputId": "c35f1723-c023-477d-9880-787d17031bac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jan 2010 : 1318 MHW\n",
            "Jan 2010 : 5606 no MHW\n",
            "\n",
            "Feb 2010 : 1427 MHW\n",
            "Feb 2010 : 5497 no MHW\n",
            "\n",
            "Mar 2010 : 1542 MHW\n",
            "Mar 2010 : 5382 no MHW\n",
            "\n",
            "Apr 2010 : 1440 MHW\n",
            "Apr 2010 : 5484 no MHW\n",
            "\n",
            "May 2010 : 1370 MHW\n",
            "May 2010 : 5554 no MHW\n",
            "\n",
            "Jun 2010 : 1363 MHW\n",
            "Jun 2010 : 5561 no MHW\n",
            "\n",
            "Jul 2010 : 1283 MHW\n",
            "Jul 2010 : 5641 no MHW\n",
            "\n",
            "Aug 2010 : 1444 MHW\n",
            "Aug 2010 : 5480 no MHW\n",
            "\n",
            "Sep 2010 : 1413 MHW\n",
            "Sep 2010 : 5511 no MHW\n",
            "\n",
            "Oct 2010 : 1368 MHW\n",
            "Oct 2010 : 5556 no MHW\n",
            "\n",
            "Nov 2010 : 1251 MHW\n",
            "Nov 2010 : 5673 no MHW\n",
            "\n",
            "Dec 2010 : 1367 MHW\n",
            "Dec 2010 : 5557 no MHW\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lons, lats = np.meshgrid(soda_smaller.longitude.values, soda_smaller.latitude.values)\n",
        "\n",
        "soda_time_1 = soda_smaller.temp.isel(depth=0,time=240)\n",
        "soda_time_1_lons, soda_time_1_lats = np.meshgrid(soda_time_1.longitude.values, soda_time_1.latitude.values)\n",
        "soda_masked = soda_time_1.where(abs(soda_time_1_lons) + abs(soda_time_1_lats) > 0)\n",
        "\n",
        "soda_masked.values.flatten()[soda_masked.notnull().values.flatten()]\n",
        "\n",
        "from sklearn.metrics.pairwise import haversine_distances\n",
        "\n",
        "lons_ocean = soda_time_1_lons.flatten()[soda_masked.notnull().values.flatten()]\n",
        "lons_ocean = lons_ocean[::]\n",
        "lats_ocean = soda_time_1_lats.flatten()[soda_masked.notnull().values.flatten()]\n",
        "lats_ocean = lats_ocean[::]\n",
        "\n",
        "lons_ocean *= np.pi/180\n",
        "lats_ocean *= np.pi/180\n",
        "\n",
        "points_ocean = np.concatenate([np.expand_dims(lats_ocean.flatten(),-1), np.expand_dims(lons_ocean.flatten(),-1)],-1)\n",
        "\n",
        "distance_ocean = 6371*haversine_distances(points_ocean)\n",
        "\n",
        "distance_ocean_diag = distance_ocean\n",
        "distance_ocean_diag[distance_ocean_diag==0] = 1\n",
        "\n",
        "distance_ocean_recip = np.reciprocal(distance_ocean_diag)\n",
        "\n",
        "distance_ocean_recip.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8K8AXQwly2F",
        "outputId": "89b76007-cf8c-4d27-ac0c-87b0f3ba07a7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6924, 6924)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adjacency_matrix = distance_ocean_recip\n",
        "\n",
        "w = []\n",
        "\n",
        "for i in range(distance_ocean_recip.shape[0]):\n",
        "  for j in range(distance_ocean_recip.shape[1]):\n",
        "    if i != j:\n",
        "      w.append(distance_ocean_recip[i][j])"
      ],
      "metadata": {
        "id": "v3OsJt4omDGE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. GNN"
      ],
      "metadata": {
        "id": "vHV3fKk0ttuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl -f https://data.dgl.ai/wheels/repo.html\n",
        "\n",
        "import random\n",
        "\n",
        "import torch\n",
        "\n",
        "import dgl\n",
        "import dgl.nn as dglnn\n",
        "import dgl.data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOvjjT3gud5H",
        "outputId": "0148b490-cf32-48cf-ad4e-72045072ead8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/repo.html\n",
            "Requirement already satisfied: dgl in /usr/local/lib/python3.7/dist-packages (0.8.0.post1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.21.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from dgl) (4.63.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "node_num = len(soda_sst_ocean_1980_2009_flattened_all[index])\n",
        "\n",
        "def graph_structure():\n",
        "  u = []\n",
        "  v = []\n",
        "  for i in range(node_num):\n",
        "    for j in range(node_num):\n",
        "      if j == i:\n",
        "        pass\n",
        "      else:\n",
        "        u.append(i)\n",
        "        v.append(j)\n",
        "  return torch.tensor(u), torch.tensor(v)\n",
        "  \n",
        "u, v = graph_structure()\n",
        "graph = dgl.graph((u, v))"
      ],
      "metadata": {
        "id": "o-Z4FC2HyZow"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.tensor(w)"
      ],
      "metadata": {
        "id": "2f1bO-d1tnWE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split(a, n):\n",
        "    k, m = divmod(len(a), n)\n",
        "    return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))"
      ],
      "metadata": {
        "id": "6MEOgzyh0GfG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SAGE(nn.Module):\n",
        "    def __init__(self, in_feats, hid_feats, out_feats):\n",
        "        super().__init__()\n",
        "        self.conv1 = dglnn.SAGEConv(\n",
        "            in_feats=in_feats, out_feats=hid_feats, aggregator_type='mean')\n",
        "        self.conv2 = dglnn.SAGEConv(\n",
        "            in_feats=hid_feats, out_feats=out_feats, aggregator_type='mean')\n",
        "\n",
        "    def forward(self, graph, inputs):\n",
        "        # Inputs are features of nodes.\n",
        "        h = self.conv1(graph, inputs)\n",
        "        h = torch.tanh(h) # sigmoid(h), relu(h)\n",
        "        h = self.conv2(graph, h)\n",
        "        return h\n",
        "\n",
        "def evaluate(model, graph, features, labels, mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(graph, features)\n",
        "        logits = logits[mask]\n",
        "        labels = labels[mask]\n",
        "        _, indices = torch.max(logits, dim=1)\n",
        "        #print(\"Validation / test observed class:\", labels.tolist(), \"| predicted class:\", indices.tolist())\n",
        "        #correct = torch.sum(indices == labels)\n",
        "        combine = indices + labels\n",
        "        accuracy = torch.sum(indices == labels).item() * 1.0 / len(labels)\n",
        "        precision = (torch.sum(combine == 2).item() * 1.0) / (torch.sum(labels == 1).item() * 1.0)\n",
        "\n",
        "        tp = (torch.sum(combine == 2).item() * 1.0)\n",
        "        fp = (torch.sum(labels == 1).item() * 1.0) - (torch.sum(combine == 2).item() * 1.0)\n",
        "        tn = (torch.sum(combine == 0).item() * 1.0)\n",
        "        fn = (torch.sum(indices == 1).item() * 1.0) - (torch.sum(combine == 2).item() * 1.0)\n",
        "        kappa = (2 * (tp * tn - fn * fp)) / ((tp + fp) * (fp + tn) + (tp + fn) * (fn + tn))\n",
        "\n",
        "        return accuracy, precision, kappa"
      ],
      "metadata": {
        "id": "RRWvhmmF2VHB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = []\n",
        "\n",
        "for month in target_month:\n",
        "  \n",
        "  print(\"--------------------\")\n",
        "  print(\"Start training for\", month, \".\")\n",
        "  print()\n",
        "\n",
        "  index = target_month.index(month)\n",
        "  node_num = len(soda_sst_ocean_1980_2009_flattened_all[index])\n",
        "\n",
        "  node_feature = torch.tensor(soda_sst_ocean_1980_2009_flattened_all[index])\n",
        "  node_class = torch.tensor(soda_sst_ocean_2010_mhw_all[index])\n",
        "\n",
        "  graph.ndata[\"feat\"] = node_feature\n",
        "  graph.ndata[\"label\"] = node_class\n",
        "\n",
        "  graph.edata['w'] = weights\n",
        "\n",
        "  def masks():\n",
        "    train = [True] * node_num\n",
        "    val = [False] * node_num\n",
        "    test = [False] * node_num\n",
        "    randomlist = []\n",
        "    for i in range(0, round(node_num * 0.3)):\n",
        "      n = random.randint(0, node_num-1) # Need to substract 1 to be in the list index range.\n",
        "      randomlist.append(n)\n",
        "    #print(randomlist)\n",
        "    randomlist_split = list(split(randomlist, 3))\n",
        "    for i in randomlist_split[0]:\n",
        "      val[i] = True\n",
        "    for i in randomlist_split[1]:\n",
        "      test[i] = True\n",
        "    for i in randomlist_split[2]:\n",
        "      test[i] = True\n",
        "    for i in randomlist:\n",
        "      train[i] = False\n",
        "    #print(\"Training mask:\")\n",
        "    #print(train)\n",
        "    return torch.tensor(train), torch.tensor(val), torch.tensor(test)\n",
        "\n",
        "  graph.ndata[\"train_mask\"], graph.ndata[\"val_mask\"], graph.ndata[\"test_mask\"] = masks()\n",
        "  #print(graph.ndata)\n",
        "\n",
        "  node_features = graph.ndata['feat']\n",
        "  node_labels = graph.ndata['label']\n",
        "  train_mask = graph.ndata['train_mask']\n",
        "  valid_mask = graph.ndata['val_mask']\n",
        "  test_mask = graph.ndata['test_mask']\n",
        "  n_features = node_features.shape[1]\n",
        "  n_labels = int(node_labels.max().item() + 1)\n",
        "\n",
        "  model = SAGE(in_feats=n_features, hid_feats=200, out_feats=n_labels)\n",
        "  #opt = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "  opt = torch.optim.SGD(model.parameters(), lr=0.0001) #momentum=0.9\n",
        "\n",
        "  for epoch in range(5):\n",
        "      print(\"Epoch:\", epoch+1)\n",
        "      model.train()\n",
        "      # Forward propagation by using all nodes\n",
        "      logits = model(graph, node_features.float())\n",
        "      # Compute loss.\n",
        "      loss = F.cross_entropy(logits[train_mask], node_labels[train_mask])\n",
        "      # Compute validation accuracy.\n",
        "      acc, pre, kappa = evaluate(model, graph, node_features.float(), node_labels, valid_mask)\n",
        "      # Backward propagation\n",
        "      opt.zero_grad()\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "      print(\"Training loss: %.4f\" % loss.item(), \"| validation accuracy: %.4f\" % acc, \"| validation precision: %.4f\" % pre, \"| test kappa statistics: %.4f\" % kappa)\n",
        "      print()\n",
        "\n",
        "  print(\"----------\")\n",
        "  test_acc, test_pre, test_kappa = evaluate(model, graph, node_features.float(), node_labels, test_mask)\n",
        "  print(\"Test accuracy: %.4f\" % test_acc, \"| test precision: %.4f\" % test_pre, \"| test kappa statistics: %.4f\" % test_kappa)\n",
        "  \n",
        "  test_results.append([test_acc, test_pre, test_kappa])\n",
        "  print(\"The test results for\", month, \"have been appended.\")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwUfkbrvttSw",
        "outputId": "41451c22-4829-42f7-ab91-50a677e4de5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------\n",
            "Start training for Jan .\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.0955 | validation accuracy: 0.4614 | validation precision: 0.4690 | test kappa statistics: -0.0389\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.0196 | validation accuracy: 0.4841 | validation precision: 0.4690 | test kappa statistics: -0.0246\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.9540 | validation accuracy: 0.4887 | validation precision: 0.4336 | test kappa statistics: -0.0382\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.8997 | validation accuracy: 0.4932 | validation precision: 0.3982 | test kappa statistics: -0.0524\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.8553 | validation accuracy: 0.5038 | validation precision: 0.3805 | test kappa statistics: -0.0544\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.5239 | test precision: 0.3739 | test kappa statistics: -0.0453\n",
            "The test results for Jan have been appended.\n",
            "\n",
            "--------------------\n",
            "Start training for Feb .\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.9468 | validation accuracy: 0.5015 | validation precision: 0.7308 | test kappa statistics: 0.1046\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.8771 | validation accuracy: 0.5197 | validation precision: 0.7179 | test kappa statistics: 0.1170\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.8174 | validation accuracy: 0.5515 | validation precision: 0.6923 | test kappa statistics: 0.1388\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.7730 | validation accuracy: 0.5667 | validation precision: 0.6410 | test kappa statistics: 0.1333\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.7391 | validation accuracy: 0.5894 | validation precision: 0.5769 | test kappa statistics: 0.1301\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.6053 | test precision: 0.5020 | test kappa statistics: 0.0989\n",
            "The test results for Feb have been appended.\n",
            "\n",
            "--------------------\n",
            "Start training for Mar .\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 3.9007 | validation accuracy: 0.3394 | validation precision: 0.8345 | test kappa statistics: 0.0209\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 3.7121 | validation accuracy: 0.3560 | validation precision: 0.8345 | test kappa statistics: 0.0317\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 3.5244 | validation accuracy: 0.3635 | validation precision: 0.8273 | test kappa statistics: 0.0341\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 3.3133 | validation accuracy: 0.3695 | validation precision: 0.8273 | test kappa statistics: 0.0381\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 3.0948 | validation accuracy: 0.3710 | validation precision: 0.8201 | test kappa statistics: 0.0366\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.3944 | test precision: 0.8047 | test kappa statistics: 0.0406\n",
            "The test results for Mar have been appended.\n",
            "\n",
            "--------------------\n",
            "Start training for Apr .\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.8941 | validation accuracy: 0.7534 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.8810 | validation accuracy: 0.7534 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.8672 | validation accuracy: 0.7534 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.8533 | validation accuracy: 0.7534 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.8396 | validation accuracy: 0.7534 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.7825 | test precision: 0.0000 | test kappa statistics: 0.0000\n",
            "The test results for Apr have been appended.\n",
            "\n",
            "--------------------\n",
            "Start training for May .\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.3721 | validation accuracy: 0.4189 | validation precision: 0.7929 | test kappa statistics: 0.0603\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.1974 | validation accuracy: 0.4354 | validation precision: 0.7786 | test kappa statistics: 0.0673\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 1.0757 | validation accuracy: 0.4429 | validation precision: 0.7643 | test kappa statistics: 0.0676\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.9806 | validation accuracy: 0.4610 | validation precision: 0.7643 | test kappa statistics: 0.0817\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.9035 | validation accuracy: 0.4685 | validation precision: 0.7214 | test kappa statistics: 0.0711\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.5325 | test precision: 0.6877 | test kappa statistics: 0.1106\n",
            "The test results for May have been appended.\n",
            "\n",
            "--------------------\n",
            "Start training for Jun .\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.6249 | validation accuracy: 0.7897 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.5983 | validation accuracy: 0.7897 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 1.5686 | validation accuracy: 0.7897 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 1.5398 | validation accuracy: 0.7897 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 1.5150 | validation accuracy: 0.7897 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.8207 | test precision: 0.0000 | test kappa statistics: 0.0000\n",
            "The test results for Jun have been appended.\n",
            "\n",
            "--------------------\n",
            "Start training for Jul .\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.5906 | validation accuracy: 0.7948 | validation precision: 0.0080 | test kappa statistics: -0.0168\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.5802 | validation accuracy: 0.7963 | validation precision: 0.0080 | test kappa statistics: -0.0139\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.5673 | validation accuracy: 0.7963 | validation precision: 0.0080 | test kappa statistics: -0.0139\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.5561 | validation accuracy: 0.7979 | validation precision: 0.0080 | test kappa statistics: -0.0111\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.5486 | validation accuracy: 0.7979 | validation precision: 0.0080 | test kappa statistics: -0.0111\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.8002 | test precision: 0.0224 | test kappa statistics: -0.0117\n",
            "The test results for Jul have been appended.\n",
            "\n",
            "--------------------\n",
            "Start training for Aug .\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.0235 | validation accuracy: 0.7994 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.0094 | validation accuracy: 0.7994 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.9952 | validation accuracy: 0.7994 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.9814 | validation accuracy: 0.7994 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.9684 | validation accuracy: 0.7994 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.8033 | test precision: 0.0000 | test kappa statistics: 0.0000\n",
            "The test results for Aug have been appended.\n",
            "\n",
            "--------------------\n",
            "Start training for Sep .\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 2.7685 | validation accuracy: 0.2311 | validation precision: 0.9565 | test kappa statistics: -0.0015\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 2.4621 | validation accuracy: 0.3278 | validation precision: 0.8188 | test kappa statistics: 0.0084\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 2.2276 | validation accuracy: 0.3444 | validation precision: 0.7971 | test kappa statistics: 0.0111\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 2.0034 | validation accuracy: 0.3656 | validation precision: 0.7899 | test kappa statistics: 0.0222\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 1.8355 | validation accuracy: 0.3897 | validation precision: 0.7826 | test kappa statistics: 0.0359\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.3956 | test precision: 0.7787 | test kappa statistics: 0.0393\n",
            "The test results for Sep have been appended.\n",
            "\n",
            "--------------------\n",
            "Start training for Oct .\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.5879 | validation accuracy: 0.7102 | validation precision: 0.1719 | test kappa statistics: 0.0108\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.5796 | validation accuracy: 0.7417 | validation precision: 0.0625 | test kappa statistics: -0.0428\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.5729 | validation accuracy: 0.7658 | validation precision: 0.0391 | test kappa statistics: -0.0304\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.5676 | validation accuracy: 0.7718 | validation precision: 0.0312 | test kappa statistics: -0.0291\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.5631 | validation accuracy: 0.7778 | validation precision: 0.0234 | test kappa statistics: -0.0277\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.7380 | test precision: 0.0250 | test kappa statistics: -0.0479\n",
            "The test results for Oct have been appended.\n",
            "\n",
            "--------------------\n",
            "Start training for Nov .\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.0904 | validation accuracy: 0.8197 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.0741 | validation accuracy: 0.8197 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 1.0576 | validation accuracy: 0.8197 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 1.0410 | validation accuracy: 0.8197 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 1.0246 | validation accuracy: 0.8197 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.8123 | test precision: 0.0000 | test kappa statistics: 0.0000\n",
            "The test results for Nov have been appended.\n",
            "\n",
            "--------------------\n",
            "Start training for Dec .\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.8945 | validation accuracy: 0.5091 | validation precision: 0.4242 | test kappa statistics: -0.0305\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.8496 | validation accuracy: 0.5302 | validation precision: 0.4015 | test kappa statistics: -0.0252\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.8124 | validation accuracy: 0.5363 | validation precision: 0.3939 | test kappa statistics: -0.0242\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.7813 | validation accuracy: 0.5604 | validation precision: 0.3939 | test kappa statistics: -0.0030\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.7549 | validation accuracy: 0.5770 | validation precision: 0.3864 | test kappa statistics: 0.0082\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.5742 | test precision: 0.3448 | test kappa statistics: -0.0209\n",
            "The test results for Dec have been appended.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for element in test_results:\n",
        "  print(element)\n",
        "for element in test_results:\n",
        "  print(round(element[0], 4), \"&\", round(element[1], 4), \"&\", round(element[2], 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6wV6K3D6UTk",
        "outputId": "a4aa44a2-8dff-4f07-cfc6-23a6bc3dae68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5238853503184714, 0.3739495798319328, -0.0452566771690362]\n",
            "[0.6053054662379421, 0.5019762845849802, 0.09888054196639856]\n",
            "[0.39437751004016064, 0.8047138047138047, 0.04059481833512188]\n",
            "[0.7824701195219124, 0.0, 0.0]\n",
            "[0.5324881141045958, 0.6877470355731226, 0.11057330501489585]\n",
            "[0.8207171314741036, 0.0, 0.0]\n",
            "[0.8001605136436597, 0.02242152466367713, -0.011726418355301346]\n",
            "[0.8033175355450237, 0.0, 0.0]\n",
            "[0.39564867042707497, 0.7786561264822134, 0.039332905338037155]\n",
            "[0.7379636937647988, 0.025, -0.04788476964391588]\n",
            "[0.8123003194888179, 0.0, 0.0]\n",
            "[0.5741626794258373, 0.3448275862068966, -0.020929764540148922]\n",
            "0.5239 & 0.3739 & -0.0453\n",
            "0.6053 & 0.502 & 0.0989\n",
            "0.3944 & 0.8047 & 0.0406\n",
            "0.7825 & 0.0 & 0.0\n",
            "0.5325 & 0.6877 & 0.1106\n",
            "0.8207 & 0.0 & 0.0\n",
            "0.8002 & 0.0224 & -0.0117\n",
            "0.8033 & 0.0 & 0.0\n",
            "0.3956 & 0.7787 & 0.0393\n",
            "0.738 & 0.025 & -0.0479\n",
            "0.8123 & 0.0 & 0.0\n",
            "0.5742 & 0.3448 & -0.0209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "d = {'Accuracy':[test_results[i][0] for i in range(len(test_results))], 'Precision': [test_results[i][1] for i in range(len(test_results))]}\n",
        "result_df = pd.DataFrame(data=d)\n",
        "result_df\n",
        "\n",
        "fig, ax = plt.subplots(1, figsize=(8, 4))\n",
        "plt.bar(result_df.index, result_df['Accuracy'], color = '#337AE3', width = 0.9)\n",
        "plt.bar(result_df.index, result_df['Precision'], bottom = result_df['Accuracy'], color = '#DB4444', width = 0.9)\n",
        "plt.xlim(-0.6, 12)\n",
        "plt.ylim(0, 2)# remove spines\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['left'].set_visible(False)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['bottom'].set_visible(False)#grid\n",
        "ax.set_axisbelow(True)\n",
        "ax.yaxis.grid(color='gray', linestyle='dashed', alpha=0.7)# x ticks\n",
        "xticks_labels = ['Jan', 'Fev', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "plt.xticks(result_df.index , labels = xticks_labels)# title and legend\n",
        "legend_label = ['Accuracy', 'Precision']\n",
        "plt.legend(legend_label, ncol = 4, bbox_to_anchor=([1, 1.1, 0, 0]), frameon = False)\n",
        "plt.title('Test Evaluation Metrics\\n', loc='left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "ro5mzlzuy26M",
        "outputId": "c05f0483-93f4-4b68-8230-053e66c0b259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = []\n",
        "\n",
        "for month in target_month:\n",
        "  \n",
        "  print(\"--------------------\")\n",
        "  print(\"Start training for\", month)\n",
        "  print()\n",
        "\n",
        "  index = target_month.index(month)\n",
        "  node_num = len(soda_sst_ocean_1980_2009_flattened_all[index])\n",
        "\n",
        "  node_feature = torch.tensor(soda_sst_ocean_1980_2009_flattened_all[index])\n",
        "  node_class = torch.tensor(soda_sst_ocean_2010_mhw_all[index])\n",
        "\n",
        "  graph.ndata[\"feat\"] = node_feature\n",
        "  graph.ndata[\"label\"] = node_class\n",
        "\n",
        "  graph.edata['w'] = weights\n",
        "\n",
        "  def masks():\n",
        "    train = [True] * node_num\n",
        "    val = [False] * node_num\n",
        "    test = [False] * node_num\n",
        "    randomlist = []\n",
        "    for i in range(0, round(node_num * 0.3)):\n",
        "      n = random.randint(0, node_num-1) # Need to substract 1 to be in the list index range.\n",
        "      randomlist.append(n)\n",
        "    #print(randomlist)\n",
        "    randomlist_split = list(split(randomlist, 3))\n",
        "    for i in randomlist_split[0]:\n",
        "      val[i] = True\n",
        "    for i in randomlist_split[1]:\n",
        "      test[i] = True\n",
        "    for i in randomlist_split[2]:\n",
        "      test[i] = True\n",
        "    for i in randomlist:\n",
        "      train[i] = False\n",
        "    #print(\"Training mask:\")\n",
        "    #print(train)\n",
        "    return torch.tensor(train), torch.tensor(val), torch.tensor(test)\n",
        "\n",
        "  graph.ndata[\"train_mask\"], graph.ndata[\"val_mask\"], graph.ndata[\"test_mask\"] = masks()\n",
        "  #print(graph.ndata)\n",
        "\n",
        "  node_features = graph.ndata['feat']\n",
        "  node_labels = graph.ndata['label']\n",
        "  train_mask = graph.ndata['train_mask']\n",
        "  valid_mask = graph.ndata['val_mask']\n",
        "  test_mask = graph.ndata['test_mask']\n",
        "  n_features = node_features.shape[1]\n",
        "  n_labels = int(node_labels.max().item() + 1)\n",
        "\n",
        "  NUM_MODEL = 5\n",
        "  \n",
        "  sum_test_acc = 0\n",
        "  sum_test_pre = 0\n",
        "  sum_test_kappa = 0\n",
        "\n",
        "  for time in range(NUM_MODEL):\n",
        "    \n",
        "    print()\n",
        "    print(\"Start training Model\", str(time+1))\n",
        "    print()\n",
        "\n",
        "    model = SAGE(in_feats=n_features, hid_feats=200, out_feats=n_labels)\n",
        "    #opt = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "    opt = torch.optim.SGD(model.parameters(), lr=0.0001) #momentum=0.9\n",
        "\n",
        "    for epoch in range(5):\n",
        "        print(\"Epoch:\", epoch+1)\n",
        "        model.train()\n",
        "        # Forward propagation by using all nodes\n",
        "        logits = model(graph, node_features.float())\n",
        "        # Compute loss.\n",
        "        loss = F.cross_entropy(logits[train_mask], node_labels[train_mask])\n",
        "        # Compute validation accuracy.\n",
        "        acc, pre, kappa = evaluate(model, graph, node_features.float(), node_labels, valid_mask)\n",
        "        # Backward propagation\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        print(\"Training loss: %.4f\" % loss.item(), \"| validation accuracy: %.4f\" % acc, \"| validation precision: %.4f\" % pre, \"| test kappa statistics: %.4f\" % kappa)\n",
        "        print()\n",
        "\n",
        "    print(\"----------\")\n",
        "    test_acc, test_pre, test_kappa = evaluate(model, graph, node_features.float(), node_labels, test_mask)\n",
        "    print(\"Test accuracy: %.4f\" % test_acc, \"| test precision: %.4f\" % test_pre, \"| test kappa statistics: %.4f\" % test_kappa)\n",
        "\n",
        "    sum_test_acc += test_acc\n",
        "    sum_test_pre += test_pre\n",
        "    sum_test_kappa += test_kappa\n",
        "  \n",
        "  test_acc = sum_test_acc / NUM_MODEL\n",
        "  test_pre = sum_test_pre / NUM_MODEL\n",
        "  test_kappa = sum_test_kappa / NUM_MODEL\n",
        "\n",
        "  test_results.append([test_acc, test_pre, test_kappa])\n",
        "  print()\n",
        "  print(\"--------------------\")\n",
        "  print(\"Average test accuracy: %.4f\" % test_acc, \"| average test precision: %.4f\" % test_pre, \"| average test kappa statistics: %.4f\" % test_kappa)\n",
        "  print()\n",
        "  print(\"The test results for\", month, \"have been appended.\")\n",
        "  print()\n",
        "\n",
        "logits = model(graph, node_features.float())\n",
        "last_predicted_values = logits[test_mask]\n",
        "last_labels = node_labels[test_mask]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wWHKb9mL0mc",
        "outputId": "e5783c70-881e-4563-a080-ca3cfd4d6a62"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------\n",
            "Start training for Jan\n",
            "\n",
            "\n",
            "Start training Model 1\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.9427 | validation accuracy: 0.8009 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.9202 | validation accuracy: 0.8009 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.9028 | validation accuracy: 0.8009 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.8868 | validation accuracy: 0.8009 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.8707 | validation accuracy: 0.8009 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.8120 | test precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Start training Model 2\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.5112 | validation accuracy: 0.7994 | validation precision: 0.0000 | test kappa statistics: -0.0030\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.5088 | validation accuracy: 0.7994 | validation precision: 0.0000 | test kappa statistics: -0.0030\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.5067 | validation accuracy: 0.7994 | validation precision: 0.0000 | test kappa statistics: -0.0030\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.5050 | validation accuracy: 0.7994 | validation precision: 0.0000 | test kappa statistics: -0.0030\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.5034 | validation accuracy: 0.7994 | validation precision: 0.0000 | test kappa statistics: -0.0030\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.8104 | test precision: 0.0000 | test kappa statistics: -0.0032\n",
            "\n",
            "Start training Model 3\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.0015 | validation accuracy: 0.5008 | validation precision: 0.7652 | test kappa statistics: 0.1134\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.9056 | validation accuracy: 0.5098 | validation precision: 0.7500 | test kappa statistics: 0.1152\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.8290 | validation accuracy: 0.5354 | validation precision: 0.7348 | test kappa statistics: 0.1316\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.7676 | validation accuracy: 0.5551 | validation precision: 0.6364 | test kappa statistics: 0.1093\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.7187 | validation accuracy: 0.5882 | validation precision: 0.5455 | test kappa statistics: 0.1005\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.6120 | test precision: 0.4153 | test kappa statistics: 0.0542\n",
            "\n",
            "Start training Model 4\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 4.5281 | validation accuracy: 0.1991 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 4.3731 | validation accuracy: 0.1991 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 4.2307 | validation accuracy: 0.1991 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 4.0935 | validation accuracy: 0.1991 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 3.9586 | validation accuracy: 0.1991 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.1880 | test precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Start training Model 5\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.4805 | validation accuracy: 0.7994 | validation precision: 0.0152 | test kappa statistics: 0.0149\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.4800 | validation accuracy: 0.7994 | validation precision: 0.0152 | test kappa statistics: 0.0149\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.4795 | validation accuracy: 0.7994 | validation precision: 0.0152 | test kappa statistics: 0.0149\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.4790 | validation accuracy: 0.7994 | validation precision: 0.0152 | test kappa statistics: 0.0149\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.4785 | validation accuracy: 0.7994 | validation precision: 0.0152 | test kappa statistics: 0.0149\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.8112 | test precision: 0.0085 | test kappa statistics: 0.0089\n",
            "\n",
            "--------------------\n",
            "Average test accuracy: 0.6467 | average test precision: 0.2847 | average test kappa statistics: 0.0120\n",
            "\n",
            "The test results for Jan have been appended.\n",
            "\n",
            "--------------------\n",
            "Start training for Feb\n",
            "\n",
            "\n",
            "Start training Model 1\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 3.4448 | validation accuracy: 0.2018 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 3.2062 | validation accuracy: 0.2018 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 2.9334 | validation accuracy: 0.2018 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 2.6471 | validation accuracy: 0.2018 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 2.3903 | validation accuracy: 0.2018 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.2131 | test precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Start training Model 2\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.6790 | validation accuracy: 0.3479 | validation precision: 0.6269 | test kappa statistics: -0.0497\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.5745 | validation accuracy: 0.3705 | validation precision: 0.5746 | test kappa statistics: -0.0576\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 1.4702 | validation accuracy: 0.3991 | validation precision: 0.5224 | test kappa statistics: -0.0625\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 1.3681 | validation accuracy: 0.4307 | validation precision: 0.4851 | test kappa statistics: -0.0587\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 1.2725 | validation accuracy: 0.4608 | validation precision: 0.4701 | test kappa statistics: -0.0445\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.4832 | test precision: 0.3947 | test kappa statistics: -0.0680\n",
            "\n",
            "Start training Model 3\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.0303 | validation accuracy: 0.4217 | validation precision: 0.7463 | test kappa statistics: 0.0457\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.9160 | validation accuracy: 0.4654 | validation precision: 0.6940 | test kappa statistics: 0.0577\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.8198 | validation accuracy: 0.5000 | validation precision: 0.6493 | test kappa statistics: 0.0670\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.7545 | validation accuracy: 0.5527 | validation precision: 0.5672 | test kappa statistics: 0.0772\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.7107 | validation accuracy: 0.5904 | validation precision: 0.4104 | test kappa statistics: 0.0351\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.5913 | test precision: 0.1880 | test kappa statistics: -0.1007\n",
            "\n",
            "Start training Model 4\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.7965 | validation accuracy: 0.2033 | validation precision: 1.0000 | test kappa statistics: 0.0008\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.5111 | validation accuracy: 0.2184 | validation precision: 0.9403 | test kappa statistics: -0.0099\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 1.3083 | validation accuracy: 0.2575 | validation precision: 0.8657 | test kappa statistics: -0.0134\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 1.1595 | validation accuracy: 0.3238 | validation precision: 0.7388 | test kappa statistics: -0.0206\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 1.0538 | validation accuracy: 0.3886 | validation precision: 0.5299 | test kappa statistics: -0.0659\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.4519 | test precision: 0.4511 | test kappa statistics: -0.0629\n",
            "\n",
            "Start training Model 5\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.9005 | validation accuracy: 0.7982 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.8899 | validation accuracy: 0.7982 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.8800 | validation accuracy: 0.7982 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.8707 | validation accuracy: 0.7982 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.8619 | validation accuracy: 0.7967 | validation precision: 0.0000 | test kappa statistics: -0.0030\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.7861 | test precision: 0.0000 | test kappa statistics: -0.0016\n",
            "\n",
            "--------------------\n",
            "Average test accuracy: 0.5051 | average test precision: 0.4068 | average test kappa statistics: -0.0466\n",
            "\n",
            "The test results for Feb have been appended.\n",
            "\n",
            "--------------------\n",
            "Start training for Mar\n",
            "\n",
            "\n",
            "Start training Model 1\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 2.6283 | validation accuracy: 0.2058 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 2.1704 | validation accuracy: 0.2058 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 1.8359 | validation accuracy: 0.2073 | validation precision: 1.0000 | test kappa statistics: 0.0008\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 1.6040 | validation accuracy: 0.2073 | validation precision: 1.0000 | test kappa statistics: 0.0008\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 1.4151 | validation accuracy: 0.2088 | validation precision: 1.0000 | test kappa statistics: 0.0016\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.2305 | test precision: 0.9964 | test kappa statistics: 0.0060\n",
            "\n",
            "Start training Model 2\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.7186 | validation accuracy: 0.5457 | validation precision: 0.2519 | test kappa statistics: -0.0999\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.6992 | validation accuracy: 0.5808 | validation precision: 0.2370 | test kappa statistics: -0.0783\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.6824 | validation accuracy: 0.6113 | validation precision: 0.1704 | test kappa statistics: -0.0959\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.6680 | validation accuracy: 0.6555 | validation precision: 0.0444 | test kappa statistics: -0.1554\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.6559 | validation accuracy: 0.6707 | validation precision: 0.0296 | test kappa statistics: -0.1528\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.6827 | test precision: 0.0688 | test kappa statistics: -0.0908\n",
            "\n",
            "Start training Model 3\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.9477 | validation accuracy: 0.3872 | validation precision: 0.8000 | test kappa statistics: 0.0410\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.6504 | validation accuracy: 0.3902 | validation precision: 0.7926 | test kappa statistics: 0.0404\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 1.3386 | validation accuracy: 0.4131 | validation precision: 0.7852 | test kappa statistics: 0.0537\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 1.1527 | validation accuracy: 0.4253 | validation precision: 0.7481 | test kappa statistics: 0.0486\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 1.0416 | validation accuracy: 0.4482 | validation precision: 0.7333 | test kappa statistics: 0.0599\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.4878 | test precision: 0.7210 | test kappa statistics: 0.0873\n",
            "\n",
            "Start training Model 4\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.8212 | validation accuracy: 0.5945 | validation precision: 0.6667 | test kappa statistics: 0.1635\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.7843 | validation accuracy: 0.6021 | validation precision: 0.6667 | test kappa statistics: 0.1716\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.7541 | validation accuracy: 0.6037 | validation precision: 0.6593 | test kappa statistics: 0.1701\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.7319 | validation accuracy: 0.6098 | validation precision: 0.6593 | test kappa statistics: 0.1767\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.7147 | validation accuracy: 0.6189 | validation precision: 0.6296 | test kappa statistics: 0.1741\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.6125 | test precision: 0.6232 | test kappa statistics: 0.1698\n",
            "\n",
            "Start training Model 5\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.6367 | validation accuracy: 0.6921 | validation precision: 0.0667 | test kappa statistics: -0.0918\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.6282 | validation accuracy: 0.7012 | validation precision: 0.0593 | test kappa statistics: -0.0870\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.6208 | validation accuracy: 0.7088 | validation precision: 0.0519 | test kappa statistics: -0.0841\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.6140 | validation accuracy: 0.7149 | validation precision: 0.0519 | test kappa statistics: -0.0757\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.6077 | validation accuracy: 0.7241 | validation precision: 0.0444 | test kappa statistics: -0.0702\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.7222 | test precision: 0.0326 | test kappa statistics: -0.0698\n",
            "\n",
            "--------------------\n",
            "Average test accuracy: 0.5471 | average test precision: 0.4884 | average test kappa statistics: 0.0205\n",
            "\n",
            "The test results for Mar have been appended.\n",
            "\n",
            "--------------------\n",
            "Start training for Apr\n",
            "\n",
            "\n",
            "Start training Model 1\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 2.6487 | validation accuracy: 0.2324 | validation precision: 0.9225 | test kappa statistics: -0.0061\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 2.3383 | validation accuracy: 0.2691 | validation precision: 0.7597 | test kappa statistics: -0.0414\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 2.1407 | validation accuracy: 0.3058 | validation precision: 0.5194 | test kappa statistics: -0.1157\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 1.9864 | validation accuracy: 0.3242 | validation precision: 0.3721 | test kappa statistics: -0.1735\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 1.8672 | validation accuracy: 0.3716 | validation precision: 0.3256 | test kappa statistics: -0.1722\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.4244 | test precision: 0.3229 | test kappa statistics: -0.1577\n",
            "\n",
            "Start training Model 2\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.7394 | validation accuracy: 0.7095 | validation precision: 0.0310 | test kappa statistics: -0.1125\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.7285 | validation accuracy: 0.7095 | validation precision: 0.0310 | test kappa statistics: -0.1125\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.7180 | validation accuracy: 0.7080 | validation precision: 0.0310 | test kappa statistics: -0.1144\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.7080 | validation accuracy: 0.7080 | validation precision: 0.0310 | test kappa statistics: -0.1144\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.6987 | validation accuracy: 0.7064 | validation precision: 0.0310 | test kappa statistics: -0.1163\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.7094 | test precision: 0.0521 | test kappa statistics: -0.0568\n",
            "\n",
            "Start training Model 3\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 2.1969 | validation accuracy: 0.2003 | validation precision: 1.0000 | test kappa statistics: 0.0015\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.9510 | validation accuracy: 0.2049 | validation precision: 1.0000 | test kappa statistics: 0.0038\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 1.7224 | validation accuracy: 0.2049 | validation precision: 1.0000 | test kappa statistics: 0.0038\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 1.4915 | validation accuracy: 0.2141 | validation precision: 1.0000 | test kappa statistics: 0.0084\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 1.3176 | validation accuracy: 0.2309 | validation precision: 1.0000 | test kappa statistics: 0.0170\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.3080 | test precision: 0.9549 | test kappa statistics: 0.0352\n",
            "\n",
            "Start training Model 4\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 4.3950 | validation accuracy: 0.1972 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 3.9260 | validation accuracy: 0.1972 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 3.3764 | validation accuracy: 0.1988 | validation precision: 1.0000 | test kappa statistics: 0.0008\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 2.9853 | validation accuracy: 0.2920 | validation precision: 0.8837 | test kappa statistics: 0.0134\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 2.6945 | validation accuracy: 0.3150 | validation precision: 0.8605 | test kappa statistics: 0.0188\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.3207 | test precision: 0.8368 | test kappa statistics: 0.0026\n",
            "\n",
            "Start training Model 5\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.0326 | validation accuracy: 0.8028 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.0017 | validation accuracy: 0.8028 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.9712 | validation accuracy: 0.8028 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.9424 | validation accuracy: 0.8028 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.9156 | validation accuracy: 0.8028 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.7720 | test precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "--------------------\n",
            "Average test accuracy: 0.5069 | average test precision: 0.4333 | average test kappa statistics: -0.0353\n",
            "\n",
            "The test results for Apr have been appended.\n",
            "\n",
            "--------------------\n",
            "Start training for May\n",
            "\n",
            "\n",
            "Start training Model 1\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 2.6809 | validation accuracy: 0.3511 | validation precision: 0.7395 | test kappa statistics: 0.0020\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 2.3547 | validation accuracy: 0.3542 | validation precision: 0.7395 | test kappa statistics: 0.0037\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 2.0841 | validation accuracy: 0.3603 | validation precision: 0.7395 | test kappa statistics: 0.0072\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 1.8726 | validation accuracy: 0.3679 | validation precision: 0.7395 | test kappa statistics: 0.0116\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 1.7095 | validation accuracy: 0.3725 | validation precision: 0.7395 | test kappa statistics: 0.0143\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.3973 | test precision: 0.7668 | test kappa statistics: 0.0366\n",
            "\n",
            "Start training Model 2\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.2248 | validation accuracy: 0.3435 | validation precision: 0.4454 | test kappa statistics: -0.1184\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.0967 | validation accuracy: 0.4061 | validation precision: 0.3697 | test kappa statistics: -0.1213\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.9931 | validation accuracy: 0.4794 | validation precision: 0.3109 | test kappa statistics: -0.1091\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.9173 | validation accuracy: 0.5328 | validation precision: 0.2857 | test kappa statistics: -0.0876\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.8647 | validation accuracy: 0.6290 | validation precision: 0.1261 | test kappa statistics: -0.1196\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.6265 | test precision: 0.1146 | test kappa statistics: -0.1261\n",
            "\n",
            "Start training Model 3\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 4.7448 | validation accuracy: 0.1817 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 4.4303 | validation accuracy: 0.1817 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 4.1194 | validation accuracy: 0.1817 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 3.7826 | validation accuracy: 0.1817 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 3.4409 | validation accuracy: 0.1817 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.2006 | test precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Start training Model 4\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.0407 | validation accuracy: 0.5298 | validation precision: 0.3109 | test kappa statistics: -0.0753\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.0010 | validation accuracy: 0.5405 | validation precision: 0.3109 | test kappa statistics: -0.0675\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.9625 | validation accuracy: 0.5466 | validation precision: 0.3109 | test kappa statistics: -0.0629\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.9268 | validation accuracy: 0.5527 | validation precision: 0.3109 | test kappa statistics: -0.0582\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.8949 | validation accuracy: 0.5618 | validation precision: 0.3109 | test kappa statistics: -0.0510\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.5448 | test precision: 0.2648 | test kappa statistics: -0.0925\n",
            "\n",
            "Start training Model 5\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.2234 | validation accuracy: 0.8183 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.1797 | validation accuracy: 0.8183 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 1.1311 | validation accuracy: 0.8183 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 1.0782 | validation accuracy: 0.8183 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 1.0271 | validation accuracy: 0.8183 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.7994 | test precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "--------------------\n",
            "Average test accuracy: 0.5137 | average test precision: 0.4292 | average test kappa statistics: -0.0364\n",
            "\n",
            "The test results for May have been appended.\n",
            "\n",
            "--------------------\n",
            "Start training for Jun\n",
            "\n",
            "\n",
            "Start training Model 1\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.8323 | validation accuracy: 0.4872 | validation precision: 0.1746 | test kappa statistics: -0.1888\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.8009 | validation accuracy: 0.5128 | validation precision: 0.1349 | test kappa statistics: -0.1996\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.7728 | validation accuracy: 0.5459 | validation precision: 0.1270 | test kappa statistics: -0.1835\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.7488 | validation accuracy: 0.6331 | validation precision: 0.0714 | test kappa statistics: -0.1593\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.7287 | validation accuracy: 0.6421 | validation precision: 0.0556 | test kappa statistics: -0.1652\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.6422 | test precision: 0.0547 | test kappa statistics: -0.1607\n",
            "\n",
            "Start training Model 2\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.6040 | validation accuracy: 0.4707 | validation precision: 0.1667 | test kappa statistics: -0.2037\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.5507 | validation accuracy: 0.4722 | validation precision: 0.1667 | test kappa statistics: -0.2028\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 1.4999 | validation accuracy: 0.4752 | validation precision: 0.1667 | test kappa statistics: -0.2011\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 1.4518 | validation accuracy: 0.4797 | validation precision: 0.1667 | test kappa statistics: -0.1984\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 1.4050 | validation accuracy: 0.4842 | validation precision: 0.1667 | test kappa statistics: -0.1957\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.4996 | test precision: 0.1836 | test kappa statistics: -0.1807\n",
            "\n",
            "Start training Model 3\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.8844 | validation accuracy: 0.5203 | validation precision: 0.7698 | test kappa statistics: 0.1292\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.8402 | validation accuracy: 0.5278 | validation precision: 0.7698 | test kappa statistics: 0.1356\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.7983 | validation accuracy: 0.5549 | validation precision: 0.7698 | test kappa statistics: 0.1592\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.7601 | validation accuracy: 0.5714 | validation precision: 0.7619 | test kappa statistics: 0.1715\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.7261 | validation accuracy: 0.5940 | validation precision: 0.7540 | test kappa statistics: 0.1902\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.6191 | test precision: 0.7461 | test kappa statistics: 0.2210\n",
            "\n",
            "Start training Model 4\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.5531 | validation accuracy: 0.7474 | validation precision: 0.2937 | test kappa statistics: 0.1517\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.5512 | validation accuracy: 0.7564 | validation precision: 0.2778 | test kappa statistics: 0.1555\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.5495 | validation accuracy: 0.7594 | validation precision: 0.2778 | test kappa statistics: 0.1605\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.5480 | validation accuracy: 0.7594 | validation precision: 0.2460 | test kappa statistics: 0.1380\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.5466 | validation accuracy: 0.7654 | validation precision: 0.2302 | test kappa statistics: 0.1365\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.7737 | test precision: 0.2266 | test kappa statistics: 0.1678\n",
            "\n",
            "Start training Model 5\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.6183 | validation accuracy: 0.6632 | validation precision: 0.3175 | test kappa statistics: 0.0530\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.5935 | validation accuracy: 0.7143 | validation precision: 0.2222 | test kappa statistics: 0.0525\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.5748 | validation accuracy: 0.7203 | validation precision: 0.0794 | test kappa statistics: -0.0587\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.5603 | validation accuracy: 0.7459 | validation precision: 0.0476 | test kappa statistics: -0.0552\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.5489 | validation accuracy: 0.7654 | validation precision: 0.0397 | test kappa statistics: -0.0342\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.7641 | test precision: 0.0508 | test kappa statistics: -0.0031\n",
            "\n",
            "--------------------\n",
            "Average test accuracy: 0.6598 | average test precision: 0.2523 | average test kappa statistics: 0.0088\n",
            "\n",
            "The test results for Jun have been appended.\n",
            "\n",
            "--------------------\n",
            "Start training for Jul\n",
            "\n",
            "\n",
            "Start training Model 1\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 4.0525 | validation accuracy: 0.1804 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 3.8541 | validation accuracy: 0.1804 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 3.6431 | validation accuracy: 0.1804 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 3.3986 | validation accuracy: 0.1804 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 3.0939 | validation accuracy: 0.1804 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.1832 | test precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Start training Model 2\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.5067 | validation accuracy: 0.3700 | validation precision: 0.2203 | test kappa statistics: -0.2148\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.3931 | validation accuracy: 0.4098 | validation precision: 0.1949 | test kappa statistics: -0.2112\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 1.2834 | validation accuracy: 0.4480 | validation precision: 0.2034 | test kappa statistics: -0.1875\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 1.1844 | validation accuracy: 0.4817 | validation precision: 0.2034 | test kappa statistics: -0.1692\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 1.1144 | validation accuracy: 0.4985 | validation precision: 0.1949 | test kappa statistics: -0.1647\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.5400 | test precision: 0.1905 | test kappa statistics: -0.1420\n",
            "\n",
            "Start training Model 3\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.6715 | validation accuracy: 0.8196 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.6258 | validation accuracy: 0.8196 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.5947 | validation accuracy: 0.8196 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.5730 | validation accuracy: 0.8196 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.5569 | validation accuracy: 0.8196 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.8168 | test precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Start training Model 4\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.9053 | validation accuracy: 0.5214 | validation precision: 0.8051 | test kappa statistics: 0.1403\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.8585 | validation accuracy: 0.5245 | validation precision: 0.8051 | test kappa statistics: 0.1428\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.8176 | validation accuracy: 0.5260 | validation precision: 0.7712 | test kappa statistics: 0.1317\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.7823 | validation accuracy: 0.5245 | validation precision: 0.7119 | test kappa statistics: 0.1082\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.7521 | validation accuracy: 0.5306 | validation precision: 0.6695 | test kappa statistics: 0.0965\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.5147 | test precision: 0.6147 | test kappa statistics: 0.0619\n",
            "\n",
            "Start training Model 5\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.9975 | validation accuracy: 0.5795 | validation precision: 0.1102 | test kappa statistics: -0.1704\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.9805 | validation accuracy: 0.5795 | validation precision: 0.1102 | test kappa statistics: -0.1704\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.9618 | validation accuracy: 0.5841 | validation precision: 0.1102 | test kappa statistics: -0.1672\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.9421 | validation accuracy: 0.5872 | validation precision: 0.1102 | test kappa statistics: -0.1650\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.9226 | validation accuracy: 0.5856 | validation precision: 0.1017 | test kappa statistics: -0.1725\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.5908 | test precision: 0.1558 | test kappa statistics: -0.1286\n",
            "\n",
            "--------------------\n",
            "Average test accuracy: 0.5291 | average test precision: 0.3922 | average test kappa statistics: -0.0417\n",
            "\n",
            "The test results for Jul have been appended.\n",
            "\n",
            "--------------------\n",
            "Start training for Aug\n",
            "\n",
            "\n",
            "Start training Model 1\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.6895 | validation accuracy: 0.6687 | validation precision: 0.0738 | test kappa statistics: -0.0947\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.6717 | validation accuracy: 0.6810 | validation precision: 0.0738 | test kappa statistics: -0.0778\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.6577 | validation accuracy: 0.6887 | validation precision: 0.0738 | test kappa statistics: -0.0669\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.6462 | validation accuracy: 0.6948 | validation precision: 0.0671 | test kappa statistics: -0.0641\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.6362 | validation accuracy: 0.6994 | validation precision: 0.0671 | test kappa statistics: -0.0573\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.7372 | test precision: 0.0433 | test kappa statistics: -0.0771\n",
            "\n",
            "Start training Model 2\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 2.2596 | validation accuracy: 0.2285 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.9352 | validation accuracy: 0.2285 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 1.7278 | validation accuracy: 0.2301 | validation precision: 1.0000 | test kappa statistics: 0.0009\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 1.5487 | validation accuracy: 0.2331 | validation precision: 1.0000 | test kappa statistics: 0.0027\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 1.3962 | validation accuracy: 0.2531 | validation precision: 0.9597 | test kappa statistics: 0.0016\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.2740 | test precision: 0.7273 | test kappa statistics: -0.0438\n",
            "\n",
            "Start training Model 3\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.6432 | validation accuracy: 0.6856 | validation precision: 0.1074 | test kappa statistics: -0.0418\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.6192 | validation accuracy: 0.7086 | validation precision: 0.0872 | test kappa statistics: -0.0249\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.6015 | validation accuracy: 0.7178 | validation precision: 0.0872 | test kappa statistics: -0.0103\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.5887 | validation accuracy: 0.7193 | validation precision: 0.0738 | test kappa statistics: -0.0200\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.5791 | validation accuracy: 0.7270 | validation precision: 0.0738 | test kappa statistics: -0.0074\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.7652 | test precision: 0.0779 | test kappa statistics: -0.0018\n",
            "\n",
            "Start training Model 4\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.2477 | validation accuracy: 0.4939 | validation precision: 0.8456 | test kappa statistics: 0.1408\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.1239 | validation accuracy: 0.5077 | validation precision: 0.8188 | test kappa statistics: 0.1437\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.9941 | validation accuracy: 0.5307 | validation precision: 0.7987 | test kappa statistics: 0.1581\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.9023 | validation accuracy: 0.5552 | validation precision: 0.7919 | test kappa statistics: 0.1801\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.8383 | validation accuracy: 0.5706 | validation precision: 0.7651 | test kappa statistics: 0.1859\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.5631 | test precision: 0.6797 | test kappa statistics: 0.1297\n",
            "\n",
            "Start training Model 5\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.6614 | validation accuracy: 0.5675 | validation precision: 0.1007 | test kappa statistics: -0.1874\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.5958 | validation accuracy: 0.5690 | validation precision: 0.1007 | test kappa statistics: -0.1859\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 1.5434 | validation accuracy: 0.5706 | validation precision: 0.1007 | test kappa statistics: -0.1844\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 1.5014 | validation accuracy: 0.5767 | validation precision: 0.1007 | test kappa statistics: -0.1783\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 1.4662 | validation accuracy: 0.5767 | validation precision: 0.1007 | test kappa statistics: -0.1783\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.6198 | test precision: 0.1732 | test kappa statistics: -0.0916\n",
            "\n",
            "--------------------\n",
            "Average test accuracy: 0.5919 | average test precision: 0.3403 | average test kappa statistics: -0.0169\n",
            "\n",
            "The test results for Aug have been appended.\n",
            "\n",
            "--------------------\n",
            "Start training for Sep\n",
            "\n",
            "\n",
            "Start training Model 1\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.2635 | validation accuracy: 0.4497 | validation precision: 0.5115 | test kappa statistics: -0.0325\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.1717 | validation accuracy: 0.4970 | validation precision: 0.4122 | test kappa statistics: -0.0463\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 1.0997 | validation accuracy: 0.5351 | validation precision: 0.3053 | test kappa statistics: -0.0756\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 1.0473 | validation accuracy: 0.5534 | validation precision: 0.2443 | test kappa statistics: -0.0985\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 1.0087 | validation accuracy: 0.5762 | validation precision: 0.2290 | test kappa statistics: -0.0888\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.5795 | test precision: 0.2500 | test kappa statistics: -0.0707\n",
            "\n",
            "Start training Model 2\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.5864 | validation accuracy: 0.8003 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.5794 | validation accuracy: 0.8003 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.5740 | validation accuracy: 0.8003 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.5697 | validation accuracy: 0.8003 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.5663 | validation accuracy: 0.8003 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.7933 | test precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Start training Model 3\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.0887 | validation accuracy: 0.3308 | validation precision: 0.8397 | test kappa statistics: 0.0204\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.9821 | validation accuracy: 0.3750 | validation precision: 0.7023 | test kappa statistics: -0.0022\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.8950 | validation accuracy: 0.4604 | validation precision: 0.5649 | test kappa statistics: -0.0005\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.8315 | validation accuracy: 0.5122 | validation precision: 0.4198 | test kappa statistics: -0.0303\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.7864 | validation accuracy: 0.5640 | validation precision: 0.3740 | test kappa statistics: -0.0108\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.5898 | test precision: 0.3077 | test kappa statistics: -0.0237\n",
            "\n",
            "Start training Model 4\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 4.4689 | validation accuracy: 0.1997 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 3.9833 | validation accuracy: 0.1997 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 3.6330 | validation accuracy: 0.1997 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 3.2946 | validation accuracy: 0.1997 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 3.0042 | validation accuracy: 0.1997 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.2067 | test precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Start training Model 5\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 2.1620 | validation accuracy: 0.2363 | validation precision: 0.9237 | test kappa statistics: -0.0049\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.9116 | validation accuracy: 0.3201 | validation precision: 0.8321 | test kappa statistics: 0.0114\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 1.7178 | validation accuracy: 0.3460 | validation precision: 0.8168 | test kappa statistics: 0.0217\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 1.5529 | validation accuracy: 0.3598 | validation precision: 0.8092 | test kappa statistics: 0.0276\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 1.4118 | validation accuracy: 0.3720 | validation precision: 0.7939 | test kappa statistics: 0.0299\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.3911 | test precision: 0.7692 | test kappa statistics: 0.0322\n",
            "\n",
            "--------------------\n",
            "Average test accuracy: 0.5121 | average test precision: 0.4654 | average test kappa statistics: -0.0124\n",
            "\n",
            "The test results for Sep have been appended.\n",
            "\n",
            "--------------------\n",
            "Start training for Oct\n",
            "\n",
            "\n",
            "Start training Model 1\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 5.6248 | validation accuracy: 0.2140 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 5.2615 | validation accuracy: 0.2140 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 4.9063 | validation accuracy: 0.2140 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 4.5691 | validation accuracy: 0.2140 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 4.2981 | validation accuracy: 0.2140 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.2077 | test precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Start training Model 2\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.7842 | validation accuracy: 0.6783 | validation precision: 0.2908 | test kappa statistics: 0.0723\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.7766 | validation accuracy: 0.6829 | validation precision: 0.2553 | test kappa statistics: 0.0547\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.7694 | validation accuracy: 0.7086 | validation precision: 0.2199 | test kappa statistics: 0.0664\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.7626 | validation accuracy: 0.7269 | validation precision: 0.1773 | test kappa statistics: 0.0621\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.7561 | validation accuracy: 0.7527 | validation precision: 0.1560 | test kappa statistics: 0.0881\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.7756 | test precision: 0.1462 | test kappa statistics: 0.1128\n",
            "\n",
            "Start training Model 3\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.4246 | validation accuracy: 0.7860 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.3706 | validation accuracy: 0.7860 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 1.2969 | validation accuracy: 0.7860 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 1.2208 | validation accuracy: 0.7860 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 1.1724 | validation accuracy: 0.7860 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.7923 | test precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Start training Model 4\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 3.1366 | validation accuracy: 0.2140 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 2.8553 | validation accuracy: 0.2140 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 2.6162 | validation accuracy: 0.2140 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 2.3745 | validation accuracy: 0.2140 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 2.1246 | validation accuracy: 0.2140 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.2093 | test precision: 0.9962 | test kappa statistics: -0.0003\n",
            "\n",
            "Start training Model 5\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.5790 | validation accuracy: 0.7860 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.5674 | validation accuracy: 0.7860 | validation precision: 0.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.5606 | validation accuracy: 0.7830 | validation precision: 0.0000 | test kappa statistics: -0.0060\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.5569 | validation accuracy: 0.7815 | validation precision: 0.0000 | test kappa statistics: -0.0090\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.5546 | validation accuracy: 0.7800 | validation precision: 0.0000 | test kappa statistics: -0.0119\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.7804 | test precision: 0.0077 | test kappa statistics: -0.0144\n",
            "\n",
            "--------------------\n",
            "Average test accuracy: 0.5530 | average test precision: 0.4300 | average test kappa statistics: 0.0196\n",
            "\n",
            "The test results for Oct have been appended.\n",
            "\n",
            "--------------------\n",
            "Start training for Nov\n",
            "\n",
            "\n",
            "Start training Model 1\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.0704 | validation accuracy: 0.6796 | validation precision: 0.2417 | test kappa statistics: 0.0156\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.0469 | validation accuracy: 0.6811 | validation precision: 0.2417 | test kappa statistics: 0.0173\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 1.0266 | validation accuracy: 0.6811 | validation precision: 0.2417 | test kappa statistics: 0.0173\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 1.0099 | validation accuracy: 0.6811 | validation precision: 0.2417 | test kappa statistics: 0.0173\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.9960 | validation accuracy: 0.6826 | validation precision: 0.2417 | test kappa statistics: 0.0190\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.6895 | test precision: 0.2540 | test kappa statistics: 0.0472\n",
            "\n",
            "Start training Model 2\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.4794 | validation accuracy: 0.8114 | validation precision: 0.0000 | test kappa statistics: -0.0174\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.4782 | validation accuracy: 0.8129 | validation precision: 0.0000 | test kappa statistics: -0.0146\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.4771 | validation accuracy: 0.8144 | validation precision: 0.0000 | test kappa statistics: -0.0117\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.4760 | validation accuracy: 0.8144 | validation precision: 0.0000 | test kappa statistics: -0.0117\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.4750 | validation accuracy: 0.8144 | validation precision: 0.0000 | test kappa statistics: -0.0117\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.7987 | test precision: 0.0040 | test kappa statistics: -0.0075\n",
            "\n",
            "Start training Model 3\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 4.0994 | validation accuracy: 0.1796 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 3.6390 | validation accuracy: 0.1796 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 3.2423 | validation accuracy: 0.1796 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 2.9340 | validation accuracy: 0.1796 | validation precision: 1.0000 | test kappa statistics: 0.0000\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 2.6335 | validation accuracy: 0.1856 | validation precision: 1.0000 | test kappa statistics: 0.0026\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.2862 | test precision: 0.8710 | test kappa statistics: 0.0068\n",
            "\n",
            "Start training Model 4\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.5777 | validation accuracy: 0.6781 | validation precision: 0.1000 | test kappa statistics: -0.0956\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.5681 | validation accuracy: 0.6856 | validation precision: 0.0833 | test kappa statistics: -0.1025\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.5599 | validation accuracy: 0.6901 | validation precision: 0.0750 | test kappa statistics: -0.1053\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.5530 | validation accuracy: 0.6946 | validation precision: 0.0667 | test kappa statistics: -0.1083\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.5471 | validation accuracy: 0.7021 | validation precision: 0.0667 | test kappa statistics: -0.1002\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.7146 | test precision: 0.0806 | test kappa statistics: -0.0597\n",
            "\n",
            "Start training Model 5\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.5693 | validation accuracy: 0.4192 | validation precision: 0.4667 | test kappa statistics: -0.0675\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.4246 | validation accuracy: 0.4386 | validation precision: 0.4417 | test kappa statistics: -0.0675\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 1.3365 | validation accuracy: 0.4461 | validation precision: 0.4167 | test kappa statistics: -0.0748\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 1.2662 | validation accuracy: 0.4521 | validation precision: 0.3917 | test kappa statistics: -0.0833\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 1.2047 | validation accuracy: 0.4596 | validation precision: 0.3917 | test kappa statistics: -0.0788\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.4906 | test precision: 0.4234 | test kappa statistics: -0.0449\n",
            "\n",
            "--------------------\n",
            "Average test accuracy: 0.5959 | average test precision: 0.3266 | average test kappa statistics: -0.0116\n",
            "\n",
            "The test results for Nov have been appended.\n",
            "\n",
            "--------------------\n",
            "Start training for Dec\n",
            "\n",
            "\n",
            "Start training Model 1\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.8272 | validation accuracy: 0.7622 | validation precision: 0.0345 | test kappa statistics: 0.0046\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.8171 | validation accuracy: 0.7683 | validation precision: 0.0345 | test kappa statistics: 0.0161\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.8081 | validation accuracy: 0.7729 | validation precision: 0.0345 | test kappa statistics: 0.0249\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.7998 | validation accuracy: 0.7729 | validation precision: 0.0207 | test kappa statistics: 0.0105\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.7920 | validation accuracy: 0.7729 | validation precision: 0.0207 | test kappa statistics: 0.0105\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.7826 | test precision: 0.0158 | test kappa statistics: -0.0122\n",
            "\n",
            "Start training Model 2\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 2.0692 | validation accuracy: 0.2241 | validation precision: 0.9793 | test kappa statistics: -0.0049\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.8072 | validation accuracy: 0.2302 | validation precision: 0.9655 | test kappa statistics: -0.0058\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 1.6185 | validation accuracy: 0.2409 | validation precision: 0.9655 | test kappa statistics: 0.0003\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 1.4667 | validation accuracy: 0.2591 | validation precision: 0.9586 | test kappa statistics: 0.0089\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 1.3412 | validation accuracy: 0.2866 | validation precision: 0.9310 | test kappa statistics: 0.0165\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.2866 | test precision: 0.9012 | test kappa statistics: 0.0146\n",
            "\n",
            "Start training Model 3\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.5873 | validation accuracy: 0.2561 | validation precision: 0.9034 | test kappa statistics: -0.0113\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.4408 | validation accuracy: 0.2668 | validation precision: 0.8759 | test kappa statistics: -0.0144\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 1.3195 | validation accuracy: 0.2820 | validation precision: 0.8552 | test kappa statistics: -0.0124\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 1.2063 | validation accuracy: 0.2881 | validation precision: 0.8000 | test kappa statistics: -0.0284\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 1.0982 | validation accuracy: 0.3034 | validation precision: 0.7517 | test kappa statistics: -0.0370\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.3360 | test precision: 0.7470 | test kappa statistics: -0.0101\n",
            "\n",
            "Start training Model 4\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.5541 | validation accuracy: 0.5305 | validation precision: 0.4483 | test kappa statistics: 0.0015\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.4550 | validation accuracy: 0.5549 | validation precision: 0.4345 | test kappa statistics: 0.0179\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 1.3583 | validation accuracy: 0.5595 | validation precision: 0.4207 | test kappa statistics: 0.0150\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 1.2774 | validation accuracy: 0.5595 | validation precision: 0.3793 | test kappa statistics: -0.0080\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 1.2216 | validation accuracy: 0.5595 | validation precision: 0.3586 | test kappa statistics: -0.0199\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.5820 | test precision: 0.3834 | test kappa statistics: 0.0118\n",
            "\n",
            "Start training Model 5\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 1.6217 | validation accuracy: 0.4710 | validation precision: 0.5862 | test kappa statistics: 0.0157\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 1.5423 | validation accuracy: 0.4771 | validation precision: 0.5862 | test kappa statistics: 0.0209\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 1.4657 | validation accuracy: 0.4802 | validation precision: 0.5862 | test kappa statistics: 0.0235\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 1.3785 | validation accuracy: 0.4878 | validation precision: 0.5724 | test kappa statistics: 0.0238\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 1.2671 | validation accuracy: 0.4939 | validation precision: 0.5586 | test kappa statistics: 0.0227\n",
            "\n",
            "----------\n",
            "Test accuracy: 0.4904 | test precision: 0.5415 | test kappa statistics: 0.0119\n",
            "\n",
            "--------------------\n",
            "Average test accuracy: 0.4955 | average test precision: 0.5178 | average test kappa statistics: 0.0032\n",
            "\n",
            "The test results for Dec have been appended.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for element in test_results:\n",
        "  print(element)\n",
        "for element in test_results:\n",
        "  print(round(element[0], 4), \"&\", round(element[1], 4), \"&\", round(element[2], 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZkOG_nbdijW",
        "outputId": "d6790126-e8d3-4300-a107-fb3a6a2e48ae"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.6466932270916335, 0.2847457627118644, 0.011970314487502161]\n",
            "[0.5051282051282051, 0.4067669172932331, -0.04664131691844431]\n",
            "[0.547119179163378, 0.4884057971014493, 0.020521246903612612]\n",
            "[0.5068883610451306, 0.4333333333333334, -0.03532784654500232]\n",
            "[0.5137192704203013, 0.42924901185770753, -0.036397052790270126]\n",
            "[0.6597609561752988, 0.25234375, 0.008843405214207723]\n",
            "[0.5291038858049167, 0.3922077922077922, -0.0417405931376862]\n",
            "[0.5918530351437699, 0.34025974025974026, -0.016916502794719197]\n",
            "[0.5120826709062004, 0.4653846153846154, -0.012439193291051838]\n",
            "[0.5530351437699681, 0.43, 0.01961282557489659]\n",
            "[0.5959119496855345, 0.3266129032258064, -0.011629895583688443]\n",
            "[0.49554140127388535, 0.5177865612648221, 0.0031973784477776377]\n",
            "0.6467 & 0.2847 & 0.012\n",
            "0.5051 & 0.4068 & -0.0466\n",
            "0.5471 & 0.4884 & 0.0205\n",
            "0.5069 & 0.4333 & -0.0353\n",
            "0.5137 & 0.4292 & -0.0364\n",
            "0.6598 & 0.2523 & 0.0088\n",
            "0.5291 & 0.3922 & -0.0417\n",
            "0.5919 & 0.3403 & -0.0169\n",
            "0.5121 & 0.4654 & -0.0124\n",
            "0.553 & 0.43 & 0.0196\n",
            "0.5959 & 0.3266 & -0.0116\n",
            "0.4955 & 0.5178 & 0.0032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "d = {'Accuracy':[test_results[i][0] for i in range(len(test_results))], 'Precision': [test_results[i][1] for i in range(len(test_results))]}\n",
        "result_df = pd.DataFrame(data=d)\n",
        "result_df\n",
        "\n",
        "fig, ax = plt.subplots(1, figsize=(8, 4))\n",
        "plt.bar(result_df.index, result_df['Accuracy'], color = '#337AE3', width = 0.9)\n",
        "plt.bar(result_df.index, result_df['Precision'], bottom = result_df['Accuracy'], color = '#DB4444', width = 0.9)\n",
        "plt.xlim(-0.6, 12)\n",
        "plt.ylim(0, 2)# remove spines\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['left'].set_visible(False)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['bottom'].set_visible(False)#grid\n",
        "ax.set_axisbelow(True)\n",
        "ax.yaxis.grid(color='gray', linestyle='dashed', alpha=0.7)# x ticks\n",
        "xticks_labels = ['Jan', 'Fev', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "plt.xticks(result_df.index , labels = xticks_labels)# title and legend\n",
        "legend_label = ['Accuracy', 'Precision']\n",
        "plt.legend(legend_label, ncol = 4, bbox_to_anchor=([1, 1.1, 0, 0]), frameon = False)\n",
        "plt.title('Test Evaluation Metrics\\n', loc='left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "DwdcKKOOdl88",
        "outputId": "7b503aee-30c3-48b0-91b5-d56e976eba4e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(last_predicted_values.shape)\n",
        "print(last_labels.shape)"
      ],
      "metadata": {
        "id": "oW9eXpjWD0kV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Interpretation"
      ],
      "metadata": {
        "id": "2q7CjW1-F-B4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(soda_time_1_lons.flatten()[soda_masked.notnull().values.flatten()])\n",
        "print(soda_time_1_lats.flatten()[soda_masked.notnull().values.flatten()])\n",
        "\n",
        "lons_smaller = soda_time_1_lons.flatten()[soda_masked.notnull().values.flatten()]\n",
        "lats_smaller = soda_time_1_lats.flatten()[soda_masked.notnull().values.flatten()]\n",
        "\n",
        "soda_coordinates = []\n",
        "for index in range(len(lats_smaller)):\n",
        "  soda_coordinates.append([lons_smaller[index], lats_smaller[index]])\n",
        "\n",
        "print(soda_coordinates)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYLoPLKjDohp",
        "outputId": "5d8b4722-1f20-4521-c779-81fa45ecb8ab"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[162.75 165.25 167.75 ... 352.75 355.25 357.75]\n",
            "[-74.75 -74.75 -74.75 ...  87.75  87.75  87.75]\n",
            "[[162.75, -74.75], [165.25, -74.75], [167.75, -74.75], [170.25, -74.75], [172.75, -74.75], [175.25, -74.75], [177.75, -74.75], [180.25, -74.75], [182.75, -74.75], [185.25, -74.75], [187.75, -74.75], [190.25, -74.75], [192.75, -74.75], [195.25, -74.75], [197.75, -74.75], [200.25, -74.75], [202.75, -74.75], [205.25, -74.75], [207.75, -74.75], [210.25, -74.75], [212.75, -74.75], [215.25, -74.75], [217.75, -74.75], [220.25, -74.75], [222.75, -74.75], [225.25, -74.75], [227.75, -74.75], [300.25, -74.75], [302.75, -74.75], [305.25, -74.75], [307.75, -74.75], [310.25, -74.75], [312.75, -74.75], [315.25, -74.75], [317.75, -74.75], [320.25, -74.75], [322.75, -74.75], [325.25, -74.75], [327.75, -74.75], [330.25, -74.75], [332.75, -74.75], [335.25, -74.75], [337.75, -74.75], [170.25, -72.25], [172.75, -72.25], [175.25, -72.25], [177.75, -72.25], [180.25, -72.25], [182.75, -72.25], [185.25, -72.25], [187.75, -72.25], [190.25, -72.25], [192.75, -72.25], [195.25, -72.25], [197.75, -72.25], [200.25, -72.25], [202.75, -72.25], [205.25, -72.25], [207.75, -72.25], [210.25, -72.25], [212.75, -72.25], [215.25, -72.25], [217.75, -72.25], [220.25, -72.25], [222.75, -72.25], [225.25, -72.25], [227.75, -72.25], [230.25, -72.25], [232.75, -72.25], [235.25, -72.25], [237.75, -72.25], [240.25, -72.25], [242.75, -72.25], [245.25, -72.25], [247.75, -72.25], [250.25, -72.25], [252.75, -72.25], [255.25, -72.25], [257.75, -72.25], [265.25, -72.25], [267.75, -72.25], [270.25, -72.25], [272.75, -72.25], [275.25, -72.25], [277.75, -72.25], [280.25, -72.25], [282.75, -72.25], [285.25, -72.25], [300.25, -72.25], [302.75, -72.25], [305.25, -72.25], [307.75, -72.25], [310.25, -72.25], [312.75, -72.25], [315.25, -72.25], [317.75, -72.25], [320.25, -72.25], [322.75, -72.25], [325.25, -72.25], [327.75, -72.25], [330.25, -72.25], [332.75, -72.25], [335.25, -72.25], [337.75, -72.25], [340.25, -72.25], [342.75, -72.25], [345.25, -72.25], [347.75, -72.25], [0.25, -69.75], [2.75, -69.75], [5.25, -69.75], [7.75, -69.75], [10.25, -69.75], [12.75, -69.75], [15.25, -69.75], [17.75, -69.75], [20.25, -69.75], [22.75, -69.75], [25.25, -69.75], [27.75, -69.75], [32.75, -69.75], [37.75, -69.75], [75.25, -69.75], [160.25, -69.75], [162.75, -69.75], [165.25, -69.75], [167.75, -69.75], [170.25, -69.75], [172.75, -69.75], [175.25, -69.75], [177.75, -69.75], [180.25, -69.75], [182.75, -69.75], [185.25, -69.75], [187.75, -69.75], [190.25, -69.75], [192.75, -69.75], [195.25, -69.75], [197.75, -69.75], [200.25, -69.75], [202.75, -69.75], [205.25, -69.75], [207.75, -69.75], [210.25, -69.75], [212.75, -69.75], [215.25, -69.75], [217.75, -69.75], [220.25, -69.75], [222.75, -69.75], [225.25, -69.75], [227.75, -69.75], [230.25, -69.75], [232.75, -69.75], [235.25, -69.75], [237.75, -69.75], [240.25, -69.75], [242.75, -69.75], [245.25, -69.75], [247.75, -69.75], [250.25, -69.75], [252.75, -69.75], [255.25, -69.75], [257.75, -69.75], [260.25, -69.75], [262.75, -69.75], [265.25, -69.75], [267.75, -69.75], [270.25, -69.75], [272.75, -69.75], [275.25, -69.75], [277.75, -69.75], [280.25, -69.75], [282.75, -69.75], [285.25, -69.75], [300.25, -69.75], [302.75, -69.75], [305.25, -69.75], [307.75, -69.75], [310.25, -69.75], [312.75, -69.75], [315.25, -69.75], [317.75, -69.75], [320.25, -69.75], [322.75, -69.75], [325.25, -69.75], [327.75, -69.75], [330.25, -69.75], [332.75, -69.75], [335.25, -69.75], [337.75, -69.75], [340.25, -69.75], [342.75, -69.75], [345.25, -69.75], [347.75, -69.75], [350.25, -69.75], [352.75, -69.75], [355.25, -69.75], [357.75, -69.75], [0.25, -67.25], [2.75, -67.25], [5.25, -67.25], [7.75, -67.25], [10.25, -67.25], [12.75, -67.25], [15.25, -67.25], [17.75, -67.25], [20.25, -67.25], [22.75, -67.25], [25.25, -67.25], [27.75, -67.25], [30.25, -67.25], [32.75, -67.25], [35.25, -67.25], [37.75, -67.25], [40.25, -67.25], [42.75, -67.25], [45.25, -67.25], [47.75, -67.25], [50.25, -67.25], [57.75, -67.25], [60.25, -67.25], [62.75, -67.25], [65.25, -67.25], [67.75, -67.25], [70.25, -67.25], [72.75, -67.25], [75.25, -67.25], [77.75, -67.25], [80.25, -67.25], [82.75, -67.25], [127.75, -67.25], [145.25, -67.25], [147.75, -67.25], [150.25, -67.25], [152.75, -67.25], [155.25, -67.25], [157.75, -67.25], [160.25, -67.25], [162.75, -67.25], [165.25, -67.25], [167.75, -67.25], [170.25, -67.25], [172.75, -67.25], [175.25, -67.25], [177.75, -67.25], [180.25, -67.25], [182.75, -67.25], [185.25, -67.25], [187.75, -67.25], [190.25, -67.25], [192.75, -67.25], [195.25, -67.25], [197.75, -67.25], [200.25, -67.25], [202.75, -67.25], [205.25, -67.25], [207.75, -67.25], [210.25, -67.25], [212.75, -67.25], [215.25, -67.25], [217.75, -67.25], [220.25, -67.25], [222.75, -67.25], [225.25, -67.25], [227.75, -67.25], [230.25, -67.25], [232.75, -67.25], [235.25, -67.25], [237.75, -67.25], [240.25, -67.25], [242.75, -67.25], [245.25, -67.25], [247.75, -67.25], [250.25, -67.25], [252.75, -67.25], [255.25, -67.25], [257.75, -67.25], [260.25, -67.25], [262.75, -67.25], [265.25, -67.25], [267.75, -67.25], [270.25, -67.25], [272.75, -67.25], [275.25, -67.25], [277.75, -67.25], [280.25, -67.25], [282.75, -67.25], [285.25, -67.25], [287.75, -67.25], [290.25, -67.25], [292.75, -67.25], [300.25, -67.25], [302.75, -67.25], [305.25, -67.25], [307.75, -67.25], [310.25, -67.25], [312.75, -67.25], [315.25, -67.25], [317.75, -67.25], [320.25, -67.25], [322.75, -67.25], [325.25, -67.25], [327.75, -67.25], [330.25, -67.25], [332.75, -67.25], [335.25, -67.25], [337.75, -67.25], [340.25, -67.25], [342.75, -67.25], [345.25, -67.25], [347.75, -67.25], [350.25, -67.25], [352.75, -67.25], [355.25, -67.25], [357.75, -67.25], [0.25, -64.75], [2.75, -64.75], [5.25, -64.75], [7.75, -64.75], [10.25, -64.75], [12.75, -64.75], [15.25, -64.75], [17.75, -64.75], [20.25, -64.75], [22.75, -64.75], [25.25, -64.75], [27.75, -64.75], [30.25, -64.75], [32.75, -64.75], [35.25, -64.75], [37.75, -64.75], [40.25, -64.75], [42.75, -64.75], [45.25, -64.75], [47.75, -64.75], [50.25, -64.75], [52.75, -64.75], [55.25, -64.75], [57.75, -64.75], [60.25, -64.75], [62.75, -64.75], [65.25, -64.75], [67.75, -64.75], [70.25, -64.75], [72.75, -64.75], [75.25, -64.75], [77.75, -64.75], [80.25, -64.75], [82.75, -64.75], [85.25, -64.75], [87.75, -64.75], [90.25, -64.75], [92.75, -64.75], [95.25, -64.75], [97.75, -64.75], [100.25, -64.75], [102.75, -64.75], [105.25, -64.75], [107.75, -64.75], [110.25, -64.75], [112.75, -64.75], [115.25, -64.75], [117.75, -64.75], [120.25, -64.75], [122.75, -64.75], [125.25, -64.75], [127.75, -64.75], [130.25, -64.75], [132.75, -64.75], [135.25, -64.75], [137.75, -64.75], [140.25, -64.75], [142.75, -64.75], [145.25, -64.75], [147.75, -64.75], [150.25, -64.75], [152.75, -64.75], [155.25, -64.75], [157.75, -64.75], [160.25, -64.75], [162.75, -64.75], [165.25, -64.75], [167.75, -64.75], [170.25, -64.75], [172.75, -64.75], [175.25, -64.75], [177.75, -64.75], [180.25, -64.75], [182.75, -64.75], [185.25, -64.75], [187.75, -64.75], [190.25, -64.75], [192.75, -64.75], [195.25, -64.75], [197.75, -64.75], [200.25, -64.75], [202.75, -64.75], [205.25, -64.75], [207.75, -64.75], [210.25, -64.75], [212.75, -64.75], [215.25, -64.75], [217.75, -64.75], [220.25, -64.75], [222.75, -64.75], [225.25, -64.75], [227.75, -64.75], [230.25, -64.75], [232.75, -64.75], [235.25, -64.75], [237.75, -64.75], [240.25, -64.75], [242.75, -64.75], [245.25, -64.75], [247.75, -64.75], [250.25, -64.75], [252.75, -64.75], [255.25, -64.75], [257.75, -64.75], [260.25, -64.75], [262.75, -64.75], [265.25, -64.75], [267.75, -64.75], [270.25, -64.75], [272.75, -64.75], [275.25, -64.75], [277.75, -64.75], [280.25, -64.75], [282.75, -64.75], [285.25, -64.75], [287.75, -64.75], [290.25, -64.75], [292.75, -64.75], [295.25, -64.75], [297.75, -64.75], [300.25, -64.75], [302.75, -64.75], [305.25, -64.75], [307.75, -64.75], [310.25, -64.75], [312.75, -64.75], [315.25, -64.75], [317.75, -64.75], [320.25, -64.75], [322.75, -64.75], [325.25, -64.75], [327.75, -64.75], [330.25, -64.75], [332.75, -64.75], [335.25, -64.75], [337.75, -64.75], [340.25, -64.75], [342.75, -64.75], [345.25, -64.75], [347.75, -64.75], [350.25, -64.75], [352.75, -64.75], [355.25, -64.75], [357.75, -64.75], [0.25, -62.25], [2.75, -62.25], [5.25, -62.25], [7.75, -62.25], [10.25, -62.25], [12.75, -62.25], [15.25, -62.25], [17.75, -62.25], [20.25, -62.25], [22.75, -62.25], [25.25, -62.25], [27.75, -62.25], [30.25, -62.25], [32.75, -62.25], [35.25, -62.25], [37.75, -62.25], [40.25, -62.25], [42.75, -62.25], [45.25, -62.25], [47.75, -62.25], [50.25, -62.25], [52.75, -62.25], [55.25, -62.25], [57.75, -62.25], [60.25, -62.25], [62.75, -62.25], [65.25, -62.25], [67.75, -62.25], [70.25, -62.25], [72.75, -62.25], [75.25, -62.25], [77.75, -62.25], [80.25, -62.25], [82.75, -62.25], [85.25, -62.25], [87.75, -62.25], [90.25, -62.25], [92.75, -62.25], [95.25, -62.25], [97.75, -62.25], [100.25, -62.25], [102.75, -62.25], [105.25, -62.25], [107.75, -62.25], [110.25, -62.25], [112.75, -62.25], [115.25, -62.25], [117.75, -62.25], [120.25, -62.25], [122.75, -62.25], [125.25, -62.25], [127.75, -62.25], [130.25, -62.25], [132.75, -62.25], [135.25, -62.25], [137.75, -62.25], [140.25, -62.25], [142.75, -62.25], [145.25, -62.25], [147.75, -62.25], [150.25, -62.25], [152.75, -62.25], [155.25, -62.25], [157.75, -62.25], [160.25, -62.25], [162.75, -62.25], [165.25, -62.25], [167.75, -62.25], [170.25, -62.25], [172.75, -62.25], [175.25, -62.25], [177.75, -62.25], [180.25, -62.25], [182.75, -62.25], [185.25, -62.25], [187.75, -62.25], [190.25, -62.25], [192.75, -62.25], [195.25, -62.25], [197.75, -62.25], [200.25, -62.25], [202.75, -62.25], [205.25, -62.25], [207.75, -62.25], [210.25, -62.25], [212.75, -62.25], [215.25, -62.25], [217.75, -62.25], [220.25, -62.25], [222.75, -62.25], [225.25, -62.25], [227.75, -62.25], [230.25, -62.25], [232.75, -62.25], [235.25, -62.25], [237.75, -62.25], [240.25, -62.25], [242.75, -62.25], [245.25, -62.25], [247.75, -62.25], [250.25, -62.25], [252.75, -62.25], [255.25, -62.25], [257.75, -62.25], [260.25, -62.25], [262.75, -62.25], [265.25, -62.25], [267.75, -62.25], [270.25, -62.25], [272.75, -62.25], [275.25, -62.25], [277.75, -62.25], [280.25, -62.25], [282.75, -62.25], [285.25, -62.25], [287.75, -62.25], [290.25, -62.25], [292.75, -62.25], [295.25, -62.25], [297.75, -62.25], [300.25, -62.25], [302.75, -62.25], [305.25, -62.25], [307.75, -62.25], [310.25, -62.25], [312.75, -62.25], [315.25, -62.25], [317.75, -62.25], [320.25, -62.25], [322.75, -62.25], [325.25, -62.25], [327.75, -62.25], [330.25, -62.25], [332.75, -62.25], [335.25, -62.25], [337.75, -62.25], [340.25, -62.25], [342.75, -62.25], [345.25, -62.25], [347.75, -62.25], [350.25, -62.25], [352.75, -62.25], [355.25, -62.25], [357.75, -62.25], [0.25, -59.75], [2.75, -59.75], [5.25, -59.75], [7.75, -59.75], [10.25, -59.75], [12.75, -59.75], [15.25, -59.75], [17.75, -59.75], [20.25, -59.75], [22.75, -59.75], [25.25, -59.75], [27.75, -59.75], [30.25, -59.75], [32.75, -59.75], [35.25, -59.75], [37.75, -59.75], [40.25, -59.75], [42.75, -59.75], [45.25, -59.75], [47.75, -59.75], [50.25, -59.75], [52.75, -59.75], [55.25, -59.75], [57.75, -59.75], [60.25, -59.75], [62.75, -59.75], [65.25, -59.75], [67.75, -59.75], [70.25, -59.75], [72.75, -59.75], [75.25, -59.75], [77.75, -59.75], [80.25, -59.75], [82.75, -59.75], [85.25, -59.75], [87.75, -59.75], [90.25, -59.75], [92.75, -59.75], [95.25, -59.75], [97.75, -59.75], [100.25, -59.75], [102.75, -59.75], [105.25, -59.75], [107.75, -59.75], [110.25, -59.75], [112.75, -59.75], [115.25, -59.75], [117.75, -59.75], [120.25, -59.75], [122.75, -59.75], [125.25, -59.75], [127.75, -59.75], [130.25, -59.75], [132.75, -59.75], [135.25, -59.75], [137.75, -59.75], [140.25, -59.75], [142.75, -59.75], [145.25, -59.75], [147.75, -59.75], [150.25, -59.75], [152.75, -59.75], [155.25, -59.75], [157.75, -59.75], [160.25, -59.75], [162.75, -59.75], [165.25, -59.75], [167.75, -59.75], [170.25, -59.75], [172.75, -59.75], [175.25, -59.75], [177.75, -59.75], [180.25, -59.75], [182.75, -59.75], [185.25, -59.75], [187.75, -59.75], [190.25, -59.75], [192.75, -59.75], [195.25, -59.75], [197.75, -59.75], [200.25, -59.75], [202.75, -59.75], [205.25, -59.75], [207.75, -59.75], [210.25, -59.75], [212.75, -59.75], [215.25, -59.75], [217.75, -59.75], [220.25, -59.75], [222.75, -59.75], [225.25, -59.75], [227.75, -59.75], [230.25, -59.75], [232.75, -59.75], [235.25, -59.75], [237.75, -59.75], [240.25, -59.75], [242.75, -59.75], [245.25, -59.75], [247.75, -59.75], [250.25, -59.75], [252.75, -59.75], [255.25, -59.75], [257.75, -59.75], [260.25, -59.75], [262.75, -59.75], [265.25, -59.75], [267.75, -59.75], [270.25, -59.75], [272.75, -59.75], [275.25, -59.75], [277.75, -59.75], [280.25, -59.75], [282.75, -59.75], [285.25, -59.75], [287.75, -59.75], [290.25, -59.75], [292.75, -59.75], [295.25, -59.75], [297.75, -59.75], [300.25, -59.75], [302.75, -59.75], [305.25, -59.75], [307.75, -59.75], [310.25, -59.75], [312.75, -59.75], [315.25, -59.75], [317.75, -59.75], [320.25, -59.75], [322.75, -59.75], [325.25, -59.75], [327.75, -59.75], [330.25, -59.75], [332.75, -59.75], [335.25, -59.75], [337.75, -59.75], [340.25, -59.75], [342.75, -59.75], [345.25, -59.75], [347.75, -59.75], [350.25, -59.75], [352.75, -59.75], [355.25, -59.75], [357.75, -59.75], [0.25, -57.25], [2.75, -57.25], [5.25, -57.25], [7.75, -57.25], [10.25, -57.25], [12.75, -57.25], [15.25, -57.25], [17.75, -57.25], [20.25, -57.25], [22.75, -57.25], [25.25, -57.25], [27.75, -57.25], [30.25, -57.25], [32.75, -57.25], [35.25, -57.25], [37.75, -57.25], [40.25, -57.25], [42.75, -57.25], [45.25, -57.25], [47.75, -57.25], [50.25, -57.25], [52.75, -57.25], [55.25, -57.25], [57.75, -57.25], [60.25, -57.25], [62.75, -57.25], [65.25, -57.25], [67.75, -57.25], [70.25, -57.25], [72.75, -57.25], [75.25, -57.25], [77.75, -57.25], [80.25, -57.25], [82.75, -57.25], [85.25, -57.25], [87.75, -57.25], [90.25, -57.25], [92.75, -57.25], [95.25, -57.25], [97.75, -57.25], [100.25, -57.25], [102.75, -57.25], [105.25, -57.25], [107.75, -57.25], [110.25, -57.25], [112.75, -57.25], [115.25, -57.25], [117.75, -57.25], [120.25, -57.25], [122.75, -57.25], [125.25, -57.25], [127.75, -57.25], [130.25, -57.25], [132.75, -57.25], [135.25, -57.25], [137.75, -57.25], [140.25, -57.25], [142.75, -57.25], [145.25, -57.25], [147.75, -57.25], [150.25, -57.25], [152.75, -57.25], [155.25, -57.25], [157.75, -57.25], [160.25, -57.25], [162.75, -57.25], [165.25, -57.25], [167.75, -57.25], [170.25, -57.25], [172.75, -57.25], [175.25, -57.25], [177.75, -57.25], [180.25, -57.25], [182.75, -57.25], [185.25, -57.25], [187.75, -57.25], [190.25, -57.25], [192.75, -57.25], [195.25, -57.25], [197.75, -57.25], [200.25, -57.25], [202.75, -57.25], [205.25, -57.25], [207.75, -57.25], [210.25, -57.25], [212.75, -57.25], [215.25, -57.25], [217.75, -57.25], [220.25, -57.25], [222.75, -57.25], [225.25, -57.25], [227.75, -57.25], [230.25, -57.25], [232.75, -57.25], [235.25, -57.25], [237.75, -57.25], [240.25, -57.25], [242.75, -57.25], [245.25, -57.25], [247.75, -57.25], [250.25, -57.25], [252.75, -57.25], [255.25, -57.25], [257.75, -57.25], [260.25, -57.25], [262.75, -57.25], [265.25, -57.25], [267.75, -57.25], [270.25, -57.25], [272.75, -57.25], [275.25, -57.25], [277.75, -57.25], [280.25, -57.25], [282.75, -57.25], [285.25, -57.25], [287.75, -57.25], [290.25, -57.25], [292.75, -57.25], [295.25, -57.25], [297.75, -57.25], [300.25, -57.25], [302.75, -57.25], [305.25, -57.25], [307.75, -57.25], [310.25, -57.25], [312.75, -57.25], [315.25, -57.25], [317.75, -57.25], [320.25, -57.25], [322.75, -57.25], [325.25, -57.25], [327.75, -57.25], [330.25, -57.25], [332.75, -57.25], [335.25, -57.25], [337.75, -57.25], [340.25, -57.25], [342.75, -57.25], [345.25, -57.25], [347.75, -57.25], [350.25, -57.25], [352.75, -57.25], [355.25, -57.25], [357.75, -57.25], [0.25, -54.75], [2.75, -54.75], [5.25, -54.75], [7.75, -54.75], [10.25, -54.75], [12.75, -54.75], [15.25, -54.75], [17.75, -54.75], [20.25, -54.75], [22.75, -54.75], [25.25, -54.75], [27.75, -54.75], [30.25, -54.75], [32.75, -54.75], [35.25, -54.75], [37.75, -54.75], [40.25, -54.75], [42.75, -54.75], [45.25, -54.75], [47.75, -54.75], [50.25, -54.75], [52.75, -54.75], [55.25, -54.75], [57.75, -54.75], [60.25, -54.75], [62.75, -54.75], [65.25, -54.75], [67.75, -54.75], [70.25, -54.75], [72.75, -54.75], [75.25, -54.75], [77.75, -54.75], [80.25, -54.75], [82.75, -54.75], [85.25, -54.75], [87.75, -54.75], [90.25, -54.75], [92.75, -54.75], [95.25, -54.75], [97.75, -54.75], [100.25, -54.75], [102.75, -54.75], [105.25, -54.75], [107.75, -54.75], [110.25, -54.75], [112.75, -54.75], [115.25, -54.75], [117.75, -54.75], [120.25, -54.75], [122.75, -54.75], [125.25, -54.75], [127.75, -54.75], [130.25, -54.75], [132.75, -54.75], [135.25, -54.75], [137.75, -54.75], [140.25, -54.75], [142.75, -54.75], [145.25, -54.75], [147.75, -54.75], [150.25, -54.75], [152.75, -54.75], [155.25, -54.75], [157.75, -54.75], [160.25, -54.75], [162.75, -54.75], [165.25, -54.75], [167.75, -54.75], [170.25, -54.75], [172.75, -54.75], [175.25, -54.75], [177.75, -54.75], [180.25, -54.75], [182.75, -54.75], [185.25, -54.75], [187.75, -54.75], [190.25, -54.75], [192.75, -54.75], [195.25, -54.75], [197.75, -54.75], [200.25, -54.75], [202.75, -54.75], [205.25, -54.75], [207.75, -54.75], [210.25, -54.75], [212.75, -54.75], [215.25, -54.75], [217.75, -54.75], [220.25, -54.75], [222.75, -54.75], [225.25, -54.75], [227.75, -54.75], [230.25, -54.75], [232.75, -54.75], [235.25, -54.75], [237.75, -54.75], [240.25, -54.75], [242.75, -54.75], [245.25, -54.75], [247.75, -54.75], [250.25, -54.75], [252.75, -54.75], [255.25, -54.75], [257.75, -54.75], [260.25, -54.75], [262.75, -54.75], [265.25, -54.75], [267.75, -54.75], [270.25, -54.75], [272.75, -54.75], [275.25, -54.75], [277.75, -54.75], [280.25, -54.75], [282.75, -54.75], [285.25, -54.75], [287.75, -54.75], [290.25, -54.75], [292.75, -54.75], [295.25, -54.75], [297.75, -54.75], [300.25, -54.75], [302.75, -54.75], [305.25, -54.75], [307.75, -54.75], [310.25, -54.75], [312.75, -54.75], [315.25, -54.75], [317.75, -54.75], [320.25, -54.75], [322.75, -54.75], [325.25, -54.75], [327.75, -54.75], [330.25, -54.75], [332.75, -54.75], [335.25, -54.75], [337.75, -54.75], [340.25, -54.75], [342.75, -54.75], [345.25, -54.75], [347.75, -54.75], [350.25, -54.75], [352.75, -54.75], [355.25, -54.75], [357.75, -54.75], [0.25, -52.25], [2.75, -52.25], [5.25, -52.25], [7.75, -52.25], [10.25, -52.25], [12.75, -52.25], [15.25, -52.25], [17.75, -52.25], [20.25, -52.25], [22.75, -52.25], [25.25, -52.25], [27.75, -52.25], [30.25, -52.25], [32.75, -52.25], [35.25, -52.25], [37.75, -52.25], [40.25, -52.25], [42.75, -52.25], [45.25, -52.25], [47.75, -52.25], [50.25, -52.25], [52.75, -52.25], [55.25, -52.25], [57.75, -52.25], [60.25, -52.25], [62.75, -52.25], [65.25, -52.25], [67.75, -52.25], [70.25, -52.25], [72.75, -52.25], [75.25, -52.25], [77.75, -52.25], [80.25, -52.25], [82.75, -52.25], [85.25, -52.25], [87.75, -52.25], [90.25, -52.25], [92.75, -52.25], [95.25, -52.25], [97.75, -52.25], [100.25, -52.25], [102.75, -52.25], [105.25, -52.25], [107.75, -52.25], [110.25, -52.25], [112.75, -52.25], [115.25, -52.25], [117.75, -52.25], [120.25, -52.25], [122.75, -52.25], [125.25, -52.25], [127.75, -52.25], [130.25, -52.25], [132.75, -52.25], [135.25, -52.25], [137.75, -52.25], [140.25, -52.25], [142.75, -52.25], [145.25, -52.25], [147.75, -52.25], [150.25, -52.25], [152.75, -52.25], [155.25, -52.25], [157.75, -52.25], [160.25, -52.25], [162.75, -52.25], [165.25, -52.25], [167.75, -52.25], [170.25, -52.25], [172.75, -52.25], [175.25, -52.25], [177.75, -52.25], [180.25, -52.25], [182.75, -52.25], [185.25, -52.25], [187.75, -52.25], [190.25, -52.25], [192.75, -52.25], [195.25, -52.25], [197.75, -52.25], [200.25, -52.25], [202.75, -52.25], [205.25, -52.25], [207.75, -52.25], [210.25, -52.25], [212.75, -52.25], [215.25, -52.25], [217.75, -52.25], [220.25, -52.25], [222.75, -52.25], [225.25, -52.25], [227.75, -52.25], [230.25, -52.25], [232.75, -52.25], [235.25, -52.25], [237.75, -52.25], [240.25, -52.25], [242.75, -52.25], [245.25, -52.25], [247.75, -52.25], [250.25, -52.25], [252.75, -52.25], [255.25, -52.25], [257.75, -52.25], [260.25, -52.25], [262.75, -52.25], [265.25, -52.25], [267.75, -52.25], [270.25, -52.25], [272.75, -52.25], [275.25, -52.25], [277.75, -52.25], [280.25, -52.25], [282.75, -52.25], [285.25, -52.25], [290.25, -52.25], [292.75, -52.25], [295.25, -52.25], [297.75, -52.25], [300.25, -52.25], [302.75, -52.25], [305.25, -52.25], [307.75, -52.25], [310.25, -52.25], [312.75, -52.25], [315.25, -52.25], [317.75, -52.25], [320.25, -52.25], [322.75, -52.25], [325.25, -52.25], [327.75, -52.25], [330.25, -52.25], [332.75, -52.25], [335.25, -52.25], [337.75, -52.25], [340.25, -52.25], [342.75, -52.25], [345.25, -52.25], [347.75, -52.25], [350.25, -52.25], [352.75, -52.25], [355.25, -52.25], [357.75, -52.25], [0.25, -49.75], [2.75, -49.75], [5.25, -49.75], [7.75, -49.75], [10.25, -49.75], [12.75, -49.75], [15.25, -49.75], [17.75, -49.75], [20.25, -49.75], [22.75, -49.75], [25.25, -49.75], [27.75, -49.75], [30.25, -49.75], [32.75, -49.75], [35.25, -49.75], [37.75, -49.75], [40.25, -49.75], [42.75, -49.75], [45.25, -49.75], [47.75, -49.75], [50.25, -49.75], [52.75, -49.75], [55.25, -49.75], [57.75, -49.75], [60.25, -49.75], [62.75, -49.75], [65.25, -49.75], [67.75, -49.75], [70.25, -49.75], [72.75, -49.75], [75.25, -49.75], [77.75, -49.75], [80.25, -49.75], [82.75, -49.75], [85.25, -49.75], [87.75, -49.75], [90.25, -49.75], [92.75, -49.75], [95.25, -49.75], [97.75, -49.75], [100.25, -49.75], [102.75, -49.75], [105.25, -49.75], [107.75, -49.75], [110.25, -49.75], [112.75, -49.75], [115.25, -49.75], [117.75, -49.75], [120.25, -49.75], [122.75, -49.75], [125.25, -49.75], [127.75, -49.75], [130.25, -49.75], [132.75, -49.75], [135.25, -49.75], [137.75, -49.75], [140.25, -49.75], [142.75, -49.75], [145.25, -49.75], [147.75, -49.75], [150.25, -49.75], [152.75, -49.75], [155.25, -49.75], [157.75, -49.75], [160.25, -49.75], [162.75, -49.75], [165.25, -49.75], [167.75, -49.75], [170.25, -49.75], [172.75, -49.75], [175.25, -49.75], [177.75, -49.75], [180.25, -49.75], [182.75, -49.75], [185.25, -49.75], [187.75, -49.75], [190.25, -49.75], [192.75, -49.75], [195.25, -49.75], [197.75, -49.75], [200.25, -49.75], [202.75, -49.75], [205.25, -49.75], [207.75, -49.75], [210.25, -49.75], [212.75, -49.75], [215.25, -49.75], [217.75, -49.75], [220.25, -49.75], [222.75, -49.75], [225.25, -49.75], [227.75, -49.75], [230.25, -49.75], [232.75, -49.75], [235.25, -49.75], [237.75, -49.75], [240.25, -49.75], [242.75, -49.75], [245.25, -49.75], [247.75, -49.75], [250.25, -49.75], [252.75, -49.75], [255.25, -49.75], [257.75, -49.75], [260.25, -49.75], [262.75, -49.75], [265.25, -49.75], [267.75, -49.75], [270.25, -49.75], [272.75, -49.75], [275.25, -49.75], [277.75, -49.75], [280.25, -49.75], [282.75, -49.75], [285.25, -49.75], [292.75, -49.75], [295.25, -49.75], [297.75, -49.75], [300.25, -49.75], [302.75, -49.75], [305.25, -49.75], [307.75, -49.75], [310.25, -49.75], [312.75, -49.75], [315.25, -49.75], [317.75, -49.75], [320.25, -49.75], [322.75, -49.75], [325.25, -49.75], [327.75, -49.75], [330.25, -49.75], [332.75, -49.75], [335.25, -49.75], [337.75, -49.75], [340.25, -49.75], [342.75, -49.75], [345.25, -49.75], [347.75, -49.75], [350.25, -49.75], [352.75, -49.75], [355.25, -49.75], [357.75, -49.75], [0.25, -47.25], [2.75, -47.25], [5.25, -47.25], [7.75, -47.25], [10.25, -47.25], [12.75, -47.25], [15.25, -47.25], [17.75, -47.25], [20.25, -47.25], [22.75, -47.25], [25.25, -47.25], [27.75, -47.25], [30.25, -47.25], [32.75, -47.25], [35.25, -47.25], [37.75, -47.25], [40.25, -47.25], [42.75, -47.25], [45.25, -47.25], [47.75, -47.25], [50.25, -47.25], [52.75, -47.25], [55.25, -47.25], [57.75, -47.25], [60.25, -47.25], [62.75, -47.25], [65.25, -47.25], [67.75, -47.25], [70.25, -47.25], [72.75, -47.25], [75.25, -47.25], [77.75, -47.25], [80.25, -47.25], [82.75, -47.25], [85.25, -47.25], [87.75, -47.25], [90.25, -47.25], [92.75, -47.25], [95.25, -47.25], [97.75, -47.25], [100.25, -47.25], [102.75, -47.25], [105.25, -47.25], [107.75, -47.25], [110.25, -47.25], [112.75, -47.25], [115.25, -47.25], [117.75, -47.25], [120.25, -47.25], [122.75, -47.25], [125.25, -47.25], [127.75, -47.25], [130.25, -47.25], [132.75, -47.25], [135.25, -47.25], [137.75, -47.25], [140.25, -47.25], [142.75, -47.25], [145.25, -47.25], [147.75, -47.25], [150.25, -47.25], [152.75, -47.25], [155.25, -47.25], [157.75, -47.25], [160.25, -47.25], [162.75, -47.25], [165.25, -47.25], [167.75, -47.25], [170.25, -47.25], [172.75, -47.25], [175.25, -47.25], [177.75, -47.25], [180.25, -47.25], [182.75, -47.25], [185.25, -47.25], [187.75, -47.25], [190.25, -47.25], [192.75, -47.25], [195.25, -47.25], [197.75, -47.25], [200.25, -47.25], [202.75, -47.25], [205.25, -47.25], [207.75, -47.25], [210.25, -47.25], [212.75, -47.25], [215.25, -47.25], [217.75, -47.25], [220.25, -47.25], [222.75, -47.25], [225.25, -47.25], [227.75, -47.25], [230.25, -47.25], [232.75, -47.25], [235.25, -47.25], [237.75, -47.25], [240.25, -47.25], [242.75, -47.25], [245.25, -47.25], [247.75, -47.25], [250.25, -47.25], [252.75, -47.25], [255.25, -47.25], [257.75, -47.25], [260.25, -47.25], [262.75, -47.25], [265.25, -47.25], [267.75, -47.25], [270.25, -47.25], [272.75, -47.25], [275.25, -47.25], [277.75, -47.25], [280.25, -47.25], [282.75, -47.25], [285.25, -47.25], [295.25, -47.25], [297.75, -47.25], [300.25, -47.25], [302.75, -47.25], [305.25, -47.25], [307.75, -47.25], [310.25, -47.25], [312.75, -47.25], [315.25, -47.25], [317.75, -47.25], [320.25, -47.25], [322.75, -47.25], [325.25, -47.25], [327.75, -47.25], [330.25, -47.25], [332.75, -47.25], [335.25, -47.25], [337.75, -47.25], [340.25, -47.25], [342.75, -47.25], [345.25, -47.25], [347.75, -47.25], [350.25, -47.25], [352.75, -47.25], [355.25, -47.25], [357.75, -47.25], [0.25, -44.75], [2.75, -44.75], [5.25, -44.75], [7.75, -44.75], [10.25, -44.75], [12.75, -44.75], [15.25, -44.75], [17.75, -44.75], [20.25, -44.75], [22.75, -44.75], [25.25, -44.75], [27.75, -44.75], [30.25, -44.75], [32.75, -44.75], [35.25, -44.75], [37.75, -44.75], [40.25, -44.75], [42.75, -44.75], [45.25, -44.75], [47.75, -44.75], [50.25, -44.75], [52.75, -44.75], [55.25, -44.75], [57.75, -44.75], [60.25, -44.75], [62.75, -44.75], [65.25, -44.75], [67.75, -44.75], [70.25, -44.75], [72.75, -44.75], [75.25, -44.75], [77.75, -44.75], [80.25, -44.75], [82.75, -44.75], [85.25, -44.75], [87.75, -44.75], [90.25, -44.75], [92.75, -44.75], [95.25, -44.75], [97.75, -44.75], [100.25, -44.75], [102.75, -44.75], [105.25, -44.75], [107.75, -44.75], [110.25, -44.75], [112.75, -44.75], [115.25, -44.75], [117.75, -44.75], [120.25, -44.75], [122.75, -44.75], [125.25, -44.75], [127.75, -44.75], [130.25, -44.75], [132.75, -44.75], [135.25, -44.75], [137.75, -44.75], [140.25, -44.75], [142.75, -44.75], [145.25, -44.75], [147.75, -44.75], [150.25, -44.75], [152.75, -44.75], [155.25, -44.75], [157.75, -44.75], [160.25, -44.75], [162.75, -44.75], [165.25, -44.75], [167.75, -44.75], [172.75, -44.75], [175.25, -44.75], [177.75, -44.75], [180.25, -44.75], [182.75, -44.75], [185.25, -44.75], [187.75, -44.75], [190.25, -44.75], [192.75, -44.75], [195.25, -44.75], [197.75, -44.75], [200.25, -44.75], [202.75, -44.75], [205.25, -44.75], [207.75, -44.75], [210.25, -44.75], [212.75, -44.75], [215.25, -44.75], [217.75, -44.75], [220.25, -44.75], [222.75, -44.75], [225.25, -44.75], [227.75, -44.75], [230.25, -44.75], [232.75, -44.75], [235.25, -44.75], [237.75, -44.75], [240.25, -44.75], [242.75, -44.75], [245.25, -44.75], [247.75, -44.75], [250.25, -44.75], [252.75, -44.75], [255.25, -44.75], [257.75, -44.75], [260.25, -44.75], [262.75, -44.75], [265.25, -44.75], [267.75, -44.75], [270.25, -44.75], [272.75, -44.75], [275.25, -44.75], [277.75, -44.75], [280.25, -44.75], [282.75, -44.75], [285.25, -44.75], [295.25, -44.75], [297.75, -44.75], [300.25, -44.75], [302.75, -44.75], [305.25, -44.75], [307.75, -44.75], [310.25, -44.75], [312.75, -44.75], [315.25, -44.75], [317.75, -44.75], [320.25, -44.75], [322.75, -44.75], [325.25, -44.75], [327.75, -44.75], [330.25, -44.75], [332.75, -44.75], [335.25, -44.75], [337.75, -44.75], [340.25, -44.75], [342.75, -44.75], [345.25, -44.75], [347.75, -44.75], [350.25, -44.75], [352.75, -44.75], [355.25, -44.75], [357.75, -44.75], [0.25, -42.25], [2.75, -42.25], [5.25, -42.25], [7.75, -42.25], [10.25, -42.25], [12.75, -42.25], [15.25, -42.25], [17.75, -42.25], [20.25, -42.25], [22.75, -42.25], [25.25, -42.25], [27.75, -42.25], [30.25, -42.25], [32.75, -42.25], [35.25, -42.25], [37.75, -42.25], [40.25, -42.25], [42.75, -42.25], [45.25, -42.25], [47.75, -42.25], [50.25, -42.25], [52.75, -42.25], [55.25, -42.25], [57.75, -42.25], [60.25, -42.25], [62.75, -42.25], [65.25, -42.25], [67.75, -42.25], [70.25, -42.25], [72.75, -42.25], [75.25, -42.25], [77.75, -42.25], [80.25, -42.25], [82.75, -42.25], [85.25, -42.25], [87.75, -42.25], [90.25, -42.25], [92.75, -42.25], [95.25, -42.25], [97.75, -42.25], [100.25, -42.25], [102.75, -42.25], [105.25, -42.25], [107.75, -42.25], [110.25, -42.25], [112.75, -42.25], [115.25, -42.25], [117.75, -42.25], [120.25, -42.25], [122.75, -42.25], [125.25, -42.25], [127.75, -42.25], [130.25, -42.25], [132.75, -42.25], [135.25, -42.25], [137.75, -42.25], [140.25, -42.25], [142.75, -42.25], [145.25, -42.25], [147.75, -42.25], [150.25, -42.25], [152.75, -42.25], [155.25, -42.25], [157.75, -42.25], [160.25, -42.25], [162.75, -42.25], [165.25, -42.25], [167.75, -42.25], [170.25, -42.25], [175.25, -42.25], [177.75, -42.25], [180.25, -42.25], [182.75, -42.25], [185.25, -42.25], [187.75, -42.25], [190.25, -42.25], [192.75, -42.25], [195.25, -42.25], [197.75, -42.25], [200.25, -42.25], [202.75, -42.25], [205.25, -42.25], [207.75, -42.25], [210.25, -42.25], [212.75, -42.25], [215.25, -42.25], [217.75, -42.25], [220.25, -42.25], [222.75, -42.25], [225.25, -42.25], [227.75, -42.25], [230.25, -42.25], [232.75, -42.25], [235.25, -42.25], [237.75, -42.25], [240.25, -42.25], [242.75, -42.25], [245.25, -42.25], [247.75, -42.25], [250.25, -42.25], [252.75, -42.25], [255.25, -42.25], [257.75, -42.25], [260.25, -42.25], [262.75, -42.25], [265.25, -42.25], [267.75, -42.25], [270.25, -42.25], [272.75, -42.25], [275.25, -42.25], [277.75, -42.25], [280.25, -42.25], [282.75, -42.25], [285.25, -42.25], [295.25, -42.25], [297.75, -42.25], [300.25, -42.25], [302.75, -42.25], [305.25, -42.25], [307.75, -42.25], [310.25, -42.25], [312.75, -42.25], [315.25, -42.25], [317.75, -42.25], [320.25, -42.25], [322.75, -42.25], [325.25, -42.25], [327.75, -42.25], [330.25, -42.25], [332.75, -42.25], [335.25, -42.25], [337.75, -42.25], [340.25, -42.25], [342.75, -42.25], [345.25, -42.25], [347.75, -42.25], [350.25, -42.25], [352.75, -42.25], [355.25, -42.25], [357.75, -42.25], [0.25, -39.75], [2.75, -39.75], [5.25, -39.75], [7.75, -39.75], [10.25, -39.75], [12.75, -39.75], [15.25, -39.75], [17.75, -39.75], [20.25, -39.75], [22.75, -39.75], [25.25, -39.75], [27.75, -39.75], [30.25, -39.75], [32.75, -39.75], [35.25, -39.75], [37.75, -39.75], [40.25, -39.75], [42.75, -39.75], [45.25, -39.75], [47.75, -39.75], [50.25, -39.75], [52.75, -39.75], [55.25, -39.75], [57.75, -39.75], [60.25, -39.75], [62.75, -39.75], [65.25, -39.75], [67.75, -39.75], [70.25, -39.75], [72.75, -39.75], [75.25, -39.75], [77.75, -39.75], [80.25, -39.75], [82.75, -39.75], [85.25, -39.75], [87.75, -39.75], [90.25, -39.75], [92.75, -39.75], [95.25, -39.75], [97.75, -39.75], [100.25, -39.75], [102.75, -39.75], [105.25, -39.75], [107.75, -39.75], [110.25, -39.75], [112.75, -39.75], [115.25, -39.75], [117.75, -39.75], [120.25, -39.75], [122.75, -39.75], [125.25, -39.75], [127.75, -39.75], [130.25, -39.75], [132.75, -39.75], [135.25, -39.75], [137.75, -39.75], [140.25, -39.75], [142.75, -39.75], [145.25, -39.75], [147.75, -39.75], [150.25, -39.75], [152.75, -39.75], [155.25, -39.75], [157.75, -39.75], [160.25, -39.75], [162.75, -39.75], [165.25, -39.75], [167.75, -39.75], [170.25, -39.75], [172.75, -39.75], [177.75, -39.75], [180.25, -39.75], [182.75, -39.75], [185.25, -39.75], [187.75, -39.75], [190.25, -39.75], [192.75, -39.75], [195.25, -39.75], [197.75, -39.75], [200.25, -39.75], [202.75, -39.75], [205.25, -39.75], [207.75, -39.75], [210.25, -39.75], [212.75, -39.75], [215.25, -39.75], [217.75, -39.75], [220.25, -39.75], [222.75, -39.75], [225.25, -39.75], [227.75, -39.75], [230.25, -39.75], [232.75, -39.75], [235.25, -39.75], [237.75, -39.75], [240.25, -39.75], [242.75, -39.75], [245.25, -39.75], [247.75, -39.75], [250.25, -39.75], [252.75, -39.75], [255.25, -39.75], [257.75, -39.75], [260.25, -39.75], [262.75, -39.75], [265.25, -39.75], [267.75, -39.75], [270.25, -39.75], [272.75, -39.75], [275.25, -39.75], [277.75, -39.75], [280.25, -39.75], [282.75, -39.75], [285.25, -39.75], [297.75, -39.75], [300.25, -39.75], [302.75, -39.75], [305.25, -39.75], [307.75, -39.75], [310.25, -39.75], [312.75, -39.75], [315.25, -39.75], [317.75, -39.75], [320.25, -39.75], [322.75, -39.75], [325.25, -39.75], [327.75, -39.75], [330.25, -39.75], [332.75, -39.75], [335.25, -39.75], [337.75, -39.75], [340.25, -39.75], [342.75, -39.75], [345.25, -39.75], [347.75, -39.75], [350.25, -39.75], [352.75, -39.75], [355.25, -39.75], [357.75, -39.75], [0.25, -37.25], [2.75, -37.25], [5.25, -37.25], [7.75, -37.25], [10.25, -37.25], [12.75, -37.25], [15.25, -37.25], [17.75, -37.25], [20.25, -37.25], [22.75, -37.25], [25.25, -37.25], [27.75, -37.25], [30.25, -37.25], [32.75, -37.25], [35.25, -37.25], [37.75, -37.25], [40.25, -37.25], [42.75, -37.25], [45.25, -37.25], [47.75, -37.25], [50.25, -37.25], [52.75, -37.25], [55.25, -37.25], [57.75, -37.25], [60.25, -37.25], [62.75, -37.25], [65.25, -37.25], [67.75, -37.25], [70.25, -37.25], [72.75, -37.25], [75.25, -37.25], [77.75, -37.25], [80.25, -37.25], [82.75, -37.25], [85.25, -37.25], [87.75, -37.25], [90.25, -37.25], [92.75, -37.25], [95.25, -37.25], [97.75, -37.25], [100.25, -37.25], [102.75, -37.25], [105.25, -37.25], [107.75, -37.25], [110.25, -37.25], [112.75, -37.25], [115.25, -37.25], [117.75, -37.25], [120.25, -37.25], [122.75, -37.25], [125.25, -37.25], [127.75, -37.25], [130.25, -37.25], [132.75, -37.25], [135.25, -37.25], [137.75, -37.25], [140.25, -37.25], [150.25, -37.25], [152.75, -37.25], [155.25, -37.25], [157.75, -37.25], [160.25, -37.25], [162.75, -37.25], [165.25, -37.25], [167.75, -37.25], [170.25, -37.25], [172.75, -37.25], [177.75, -37.25], [180.25, -37.25], [182.75, -37.25], [185.25, -37.25], [187.75, -37.25], [190.25, -37.25], [192.75, -37.25], [195.25, -37.25], [197.75, -37.25], [200.25, -37.25], [202.75, -37.25], [205.25, -37.25], [207.75, -37.25], [210.25, -37.25], [212.75, -37.25], [215.25, -37.25], [217.75, -37.25], [220.25, -37.25], [222.75, -37.25], [225.25, -37.25], [227.75, -37.25], [230.25, -37.25], [232.75, -37.25], [235.25, -37.25], [237.75, -37.25], [240.25, -37.25], [242.75, -37.25], [245.25, -37.25], [247.75, -37.25], [250.25, -37.25], [252.75, -37.25], [255.25, -37.25], [257.75, -37.25], [260.25, -37.25], [262.75, -37.25], [265.25, -37.25], [267.75, -37.25], [270.25, -37.25], [272.75, -37.25], [275.25, -37.25], [277.75, -37.25], [280.25, -37.25], [282.75, -37.25], [285.25, -37.25], [302.75, -37.25], [305.25, -37.25], [307.75, -37.25], [310.25, -37.25], [312.75, -37.25], [315.25, -37.25], [317.75, -37.25], [320.25, -37.25], [322.75, -37.25], [325.25, -37.25], [327.75, -37.25], [330.25, -37.25], [332.75, -37.25], [335.25, -37.25], [337.75, -37.25], [340.25, -37.25], [342.75, -37.25], [345.25, -37.25], [347.75, -37.25], [350.25, -37.25], [352.75, -37.25], [355.25, -37.25], [357.75, -37.25], [0.25, -34.75], [2.75, -34.75], [5.25, -34.75], [7.75, -34.75], [10.25, -34.75], [12.75, -34.75], [15.25, -34.75], [17.75, -34.75], [20.25, -34.75], [22.75, -34.75], [25.25, -34.75], [27.75, -34.75], [30.25, -34.75], [32.75, -34.75], [35.25, -34.75], [37.75, -34.75], [40.25, -34.75], [42.75, -34.75], [45.25, -34.75], [47.75, -34.75], [50.25, -34.75], [52.75, -34.75], [55.25, -34.75], [57.75, -34.75], [60.25, -34.75], [62.75, -34.75], [65.25, -34.75], [67.75, -34.75], [70.25, -34.75], [72.75, -34.75], [75.25, -34.75], [77.75, -34.75], [80.25, -34.75], [82.75, -34.75], [85.25, -34.75], [87.75, -34.75], [90.25, -34.75], [92.75, -34.75], [95.25, -34.75], [97.75, -34.75], [100.25, -34.75], [102.75, -34.75], [105.25, -34.75], [107.75, -34.75], [110.25, -34.75], [112.75, -34.75], [115.25, -34.75], [120.25, -34.75], [122.75, -34.75], [125.25, -34.75], [127.75, -34.75], [130.25, -34.75], [132.75, -34.75], [135.25, -34.75], [137.75, -34.75], [152.75, -34.75], [155.25, -34.75], [157.75, -34.75], [160.25, -34.75], [162.75, -34.75], [165.25, -34.75], [167.75, -34.75], [170.25, -34.75], [172.75, -34.75], [175.25, -34.75], [177.75, -34.75], [180.25, -34.75], [182.75, -34.75], [185.25, -34.75], [187.75, -34.75], [190.25, -34.75], [192.75, -34.75], [195.25, -34.75], [197.75, -34.75], [200.25, -34.75], [202.75, -34.75], [205.25, -34.75], [207.75, -34.75], [210.25, -34.75], [212.75, -34.75], [215.25, -34.75], [217.75, -34.75], [220.25, -34.75], [222.75, -34.75], [225.25, -34.75], [227.75, -34.75], [230.25, -34.75], [232.75, -34.75], [235.25, -34.75], [237.75, -34.75], [240.25, -34.75], [242.75, -34.75], [245.25, -34.75], [247.75, -34.75], [250.25, -34.75], [252.75, -34.75], [255.25, -34.75], [257.75, -34.75], [260.25, -34.75], [262.75, -34.75], [265.25, -34.75], [267.75, -34.75], [270.25, -34.75], [272.75, -34.75], [275.25, -34.75], [277.75, -34.75], [280.25, -34.75], [282.75, -34.75], [285.25, -34.75], [287.75, -34.75], [302.75, -34.75], [305.25, -34.75], [307.75, -34.75], [310.25, -34.75], [312.75, -34.75], [315.25, -34.75], [317.75, -34.75], [320.25, -34.75], [322.75, -34.75], [325.25, -34.75], [327.75, -34.75], [330.25, -34.75], [332.75, -34.75], [335.25, -34.75], [337.75, -34.75], [340.25, -34.75], [342.75, -34.75], [345.25, -34.75], [347.75, -34.75], [350.25, -34.75], [352.75, -34.75], [355.25, -34.75], [357.75, -34.75], [0.25, -32.25], [2.75, -32.25], [5.25, -32.25], [7.75, -32.25], [10.25, -32.25], [12.75, -32.25], [15.25, -32.25], [17.75, -32.25], [30.25, -32.25], [32.75, -32.25], [35.25, -32.25], [37.75, -32.25], [40.25, -32.25], [42.75, -32.25], [45.25, -32.25], [47.75, -32.25], [50.25, -32.25], [52.75, -32.25], [55.25, -32.25], [57.75, -32.25], [60.25, -32.25], [62.75, -32.25], [65.25, -32.25], [67.75, -32.25], [70.25, -32.25], [72.75, -32.25], [75.25, -32.25], [77.75, -32.25], [80.25, -32.25], [82.75, -32.25], [85.25, -32.25], [87.75, -32.25], [90.25, -32.25], [92.75, -32.25], [95.25, -32.25], [97.75, -32.25], [100.25, -32.25], [102.75, -32.25], [105.25, -32.25], [107.75, -32.25], [110.25, -32.25], [112.75, -32.25], [115.25, -32.25], [127.75, -32.25], [130.25, -32.25], [132.75, -32.25], [152.75, -32.25], [155.25, -32.25], [157.75, -32.25], [160.25, -32.25], [162.75, -32.25], [165.25, -32.25], [167.75, -32.25], [170.25, -32.25], [172.75, -32.25], [175.25, -32.25], [177.75, -32.25], [180.25, -32.25], [182.75, -32.25], [185.25, -32.25], [187.75, -32.25], [190.25, -32.25], [192.75, -32.25], [195.25, -32.25], [197.75, -32.25], [200.25, -32.25], [202.75, -32.25], [205.25, -32.25], [207.75, -32.25], [210.25, -32.25], [212.75, -32.25], [215.25, -32.25], [217.75, -32.25], [220.25, -32.25], [222.75, -32.25], [225.25, -32.25], [227.75, -32.25], [230.25, -32.25], [232.75, -32.25], [235.25, -32.25], [237.75, -32.25], [240.25, -32.25], [242.75, -32.25], [245.25, -32.25], [247.75, -32.25], [250.25, -32.25], [252.75, -32.25], [255.25, -32.25], [257.75, -32.25], [260.25, -32.25], [262.75, -32.25], [265.25, -32.25], [267.75, -32.25], [270.25, -32.25], [272.75, -32.25], [275.25, -32.25], [277.75, -32.25], [280.25, -32.25], [282.75, -32.25], [285.25, -32.25], [287.75, -32.25], [307.75, -32.25], [310.25, -32.25], [312.75, -32.25], [315.25, -32.25], [317.75, -32.25], [320.25, -32.25], [322.75, -32.25], [325.25, -32.25], [327.75, -32.25], [330.25, -32.25], [332.75, -32.25], [335.25, -32.25], [337.75, -32.25], [340.25, -32.25], [342.75, -32.25], [345.25, -32.25], [347.75, -32.25], [350.25, -32.25], [352.75, -32.25], [355.25, -32.25], [357.75, -32.25], [0.25, -29.75], [2.75, -29.75], [5.25, -29.75], [7.75, -29.75], [10.25, -29.75], [12.75, -29.75], [15.25, -29.75], [32.75, -29.75], [35.25, -29.75], [37.75, -29.75], [40.25, -29.75], [42.75, -29.75], [45.25, -29.75], [47.75, -29.75], [50.25, -29.75], [52.75, -29.75], [55.25, -29.75], [57.75, -29.75], [60.25, -29.75], [62.75, -29.75], [65.25, -29.75], [67.75, -29.75], [70.25, -29.75], [72.75, -29.75], [75.25, -29.75], [77.75, -29.75], [80.25, -29.75], [82.75, -29.75], [85.25, -29.75], [87.75, -29.75], [90.25, -29.75], [92.75, -29.75], [95.25, -29.75], [97.75, -29.75], [100.25, -29.75], [102.75, -29.75], [105.25, -29.75], [107.75, -29.75], [110.25, -29.75], [112.75, -29.75], [155.25, -29.75], [157.75, -29.75], [160.25, -29.75], [162.75, -29.75], [165.25, -29.75], [167.75, -29.75], [170.25, -29.75], [172.75, -29.75], [175.25, -29.75], [177.75, -29.75], [180.25, -29.75], [182.75, -29.75], [185.25, -29.75], [187.75, -29.75], [190.25, -29.75], [192.75, -29.75], [195.25, -29.75], [197.75, -29.75], [200.25, -29.75], [202.75, -29.75], [205.25, -29.75], [207.75, -29.75], [210.25, -29.75], [212.75, -29.75], [215.25, -29.75], [217.75, -29.75], [220.25, -29.75], [222.75, -29.75], [225.25, -29.75], [227.75, -29.75], [230.25, -29.75], [232.75, -29.75], [235.25, -29.75], [237.75, -29.75], [240.25, -29.75], [242.75, -29.75], [245.25, -29.75], [247.75, -29.75], [250.25, -29.75], [252.75, -29.75], [255.25, -29.75], [257.75, -29.75], [260.25, -29.75], [262.75, -29.75], [265.25, -29.75], [267.75, -29.75], [270.25, -29.75], [272.75, -29.75], [275.25, -29.75], [277.75, -29.75], [280.25, -29.75], [282.75, -29.75], [285.25, -29.75], [287.75, -29.75], [310.25, -29.75], [312.75, -29.75], [315.25, -29.75], [317.75, -29.75], [320.25, -29.75], [322.75, -29.75], [325.25, -29.75], [327.75, -29.75], [330.25, -29.75], [332.75, -29.75], [335.25, -29.75], [337.75, -29.75], [340.25, -29.75], [342.75, -29.75], [345.25, -29.75], [347.75, -29.75], [350.25, -29.75], [352.75, -29.75], [355.25, -29.75], [357.75, -29.75], [0.25, -27.25], [2.75, -27.25], [5.25, -27.25], [7.75, -27.25], [10.25, -27.25], [12.75, -27.25], [15.25, -27.25], [32.75, -27.25], [35.25, -27.25], [37.75, -27.25], [40.25, -27.25], [42.75, -27.25], [45.25, -27.25], [47.75, -27.25], [50.25, -27.25], [52.75, -27.25], [55.25, -27.25], [57.75, -27.25], [60.25, -27.25], [62.75, -27.25], [65.25, -27.25], [67.75, -27.25], [70.25, -27.25], [72.75, -27.25], [75.25, -27.25], [77.75, -27.25], [80.25, -27.25], [82.75, -27.25], [85.25, -27.25], [87.75, -27.25], [90.25, -27.25], [92.75, -27.25], [95.25, -27.25], [97.75, -27.25], [100.25, -27.25], [102.75, -27.25], [105.25, -27.25], [107.75, -27.25], [110.25, -27.25], [112.75, -27.25], [155.25, -27.25], [157.75, -27.25], [160.25, -27.25], [162.75, -27.25], [165.25, -27.25], [167.75, -27.25], [170.25, -27.25], [172.75, -27.25], [175.25, -27.25], [177.75, -27.25], [180.25, -27.25], [182.75, -27.25], [185.25, -27.25], [187.75, -27.25], [190.25, -27.25], [192.75, -27.25], [195.25, -27.25], [197.75, -27.25], [200.25, -27.25], [202.75, -27.25], [205.25, -27.25], [207.75, -27.25], [210.25, -27.25], [212.75, -27.25], [215.25, -27.25], [217.75, -27.25], [220.25, -27.25], [222.75, -27.25], [225.25, -27.25], [227.75, -27.25], [230.25, -27.25], [232.75, -27.25], [235.25, -27.25], [237.75, -27.25], [240.25, -27.25], [242.75, -27.25], [245.25, -27.25], [247.75, -27.25], [250.25, -27.25], [252.75, -27.25], [255.25, -27.25], [257.75, -27.25], [260.25, -27.25], [262.75, -27.25], [265.25, -27.25], [267.75, -27.25], [270.25, -27.25], [272.75, -27.25], [275.25, -27.25], [277.75, -27.25], [280.25, -27.25], [282.75, -27.25], [285.25, -27.25], [287.75, -27.25], [312.75, -27.25], [315.25, -27.25], [317.75, -27.25], [320.25, -27.25], [322.75, -27.25], [325.25, -27.25], [327.75, -27.25], [330.25, -27.25], [332.75, -27.25], [335.25, -27.25], [337.75, -27.25], [340.25, -27.25], [342.75, -27.25], [345.25, -27.25], [347.75, -27.25], [350.25, -27.25], [352.75, -27.25], [355.25, -27.25], [357.75, -27.25], [0.25, -24.75], [2.75, -24.75], [5.25, -24.75], [7.75, -24.75], [10.25, -24.75], [12.75, -24.75], [35.25, -24.75], [37.75, -24.75], [40.25, -24.75], [42.75, -24.75], [47.75, -24.75], [50.25, -24.75], [52.75, -24.75], [55.25, -24.75], [57.75, -24.75], [60.25, -24.75], [62.75, -24.75], [65.25, -24.75], [67.75, -24.75], [70.25, -24.75], [72.75, -24.75], [75.25, -24.75], [77.75, -24.75], [80.25, -24.75], [82.75, -24.75], [85.25, -24.75], [87.75, -24.75], [90.25, -24.75], [92.75, -24.75], [95.25, -24.75], [97.75, -24.75], [100.25, -24.75], [102.75, -24.75], [105.25, -24.75], [107.75, -24.75], [110.25, -24.75], [112.75, -24.75], [152.75, -24.75], [155.25, -24.75], [157.75, -24.75], [160.25, -24.75], [162.75, -24.75], [165.25, -24.75], [167.75, -24.75], [170.25, -24.75], [172.75, -24.75], [175.25, -24.75], [177.75, -24.75], [180.25, -24.75], [182.75, -24.75], [185.25, -24.75], [187.75, -24.75], [190.25, -24.75], [192.75, -24.75], [195.25, -24.75], [197.75, -24.75], [200.25, -24.75], [202.75, -24.75], [205.25, -24.75], [207.75, -24.75], [210.25, -24.75], [212.75, -24.75], [215.25, -24.75], [217.75, -24.75], [220.25, -24.75], [222.75, -24.75], [225.25, -24.75], [227.75, -24.75], [230.25, -24.75], [232.75, -24.75], [235.25, -24.75], [237.75, -24.75], [240.25, -24.75], [242.75, -24.75], [245.25, -24.75], [247.75, -24.75], [250.25, -24.75], [252.75, -24.75], [255.25, -24.75], [257.75, -24.75], [260.25, -24.75], [262.75, -24.75], [265.25, -24.75], [267.75, -24.75], [270.25, -24.75], [272.75, -24.75], [275.25, -24.75], [277.75, -24.75], [280.25, -24.75], [282.75, -24.75], [285.25, -24.75], [287.75, -24.75], [312.75, -24.75], [315.25, -24.75], [317.75, -24.75], [320.25, -24.75], [322.75, -24.75], [325.25, -24.75], [327.75, -24.75], [330.25, -24.75], [332.75, -24.75], [335.25, -24.75], [337.75, -24.75], [340.25, -24.75], [342.75, -24.75], [345.25, -24.75], [347.75, -24.75], [350.25, -24.75], [352.75, -24.75], [355.25, -24.75], [357.75, -24.75], [0.25, -22.25], [2.75, -22.25], [5.25, -22.25], [7.75, -22.25], [10.25, -22.25], [12.75, -22.25], [35.25, -22.25], [37.75, -22.25], [40.25, -22.25], [42.75, -22.25], [47.75, -22.25], [50.25, -22.25], [52.75, -22.25], [55.25, -22.25], [57.75, -22.25], [60.25, -22.25], [62.75, -22.25], [65.25, -22.25], [67.75, -22.25], [70.25, -22.25], [72.75, -22.25], [75.25, -22.25], [77.75, -22.25], [80.25, -22.25], [82.75, -22.25], [85.25, -22.25], [87.75, -22.25], [90.25, -22.25], [92.75, -22.25], [95.25, -22.25], [97.75, -22.25], [100.25, -22.25], [102.75, -22.25], [105.25, -22.25], [107.75, -22.25], [110.25, -22.25], [112.75, -22.25], [150.25, -22.25], [152.75, -22.25], [155.25, -22.25], [157.75, -22.25], [160.25, -22.25], [162.75, -22.25], [165.25, -22.25], [167.75, -22.25], [170.25, -22.25], [172.75, -22.25], [175.25, -22.25], [177.75, -22.25], [180.25, -22.25], [182.75, -22.25], [185.25, -22.25], [187.75, -22.25], [190.25, -22.25], [192.75, -22.25], [195.25, -22.25], [197.75, -22.25], [200.25, -22.25], [202.75, -22.25], [205.25, -22.25], [207.75, -22.25], [210.25, -22.25], [212.75, -22.25], [215.25, -22.25], [217.75, -22.25], [220.25, -22.25], [222.75, -22.25], [225.25, -22.25], [227.75, -22.25], [230.25, -22.25], [232.75, -22.25], [235.25, -22.25], [237.75, -22.25], [240.25, -22.25], [242.75, -22.25], [245.25, -22.25], [247.75, -22.25], [250.25, -22.25], [252.75, -22.25], [255.25, -22.25], [257.75, -22.25], [260.25, -22.25], [262.75, -22.25], [265.25, -22.25], [267.75, -22.25], [270.25, -22.25], [272.75, -22.25], [275.25, -22.25], [277.75, -22.25], [280.25, -22.25], [282.75, -22.25], [285.25, -22.25], [287.75, -22.25], [320.25, -22.25], [322.75, -22.25], [325.25, -22.25], [327.75, -22.25], [330.25, -22.25], [332.75, -22.25], [335.25, -22.25], [337.75, -22.25], [340.25, -22.25], [342.75, -22.25], [345.25, -22.25], [347.75, -22.25], [350.25, -22.25], [352.75, -22.25], [355.25, -22.25], [357.75, -22.25], [0.25, -19.75], [2.75, -19.75], [5.25, -19.75], [7.75, -19.75], [10.25, -19.75], [12.75, -19.75], [35.25, -19.75], [37.75, -19.75], [40.25, -19.75], [42.75, -19.75], [50.25, -19.75], [52.75, -19.75], [55.25, -19.75], [57.75, -19.75], [60.25, -19.75], [62.75, -19.75], [65.25, -19.75], [67.75, -19.75], [70.25, -19.75], [72.75, -19.75], [75.25, -19.75], [77.75, -19.75], [80.25, -19.75], [82.75, -19.75], [85.25, -19.75], [87.75, -19.75], [90.25, -19.75], [92.75, -19.75], [95.25, -19.75], [97.75, -19.75], [100.25, -19.75], [102.75, -19.75], [105.25, -19.75], [107.75, -19.75], [110.25, -19.75], [112.75, -19.75], [115.25, -19.75], [117.75, -19.75], [120.25, -19.75], [147.75, -19.75], [150.25, -19.75], [152.75, -19.75], [155.25, -19.75], [157.75, -19.75], [160.25, -19.75], [162.75, -19.75], [165.25, -19.75], [167.75, -19.75], [170.25, -19.75], [172.75, -19.75], [175.25, -19.75], [177.75, -19.75], [180.25, -19.75], [182.75, -19.75], [185.25, -19.75], [187.75, -19.75], [190.25, -19.75], [192.75, -19.75], [195.25, -19.75], [197.75, -19.75], [200.25, -19.75], [202.75, -19.75], [205.25, -19.75], [207.75, -19.75], [210.25, -19.75], [212.75, -19.75], [215.25, -19.75], [217.75, -19.75], [220.25, -19.75], [222.75, -19.75], [225.25, -19.75], [227.75, -19.75], [230.25, -19.75], [232.75, -19.75], [235.25, -19.75], [237.75, -19.75], [240.25, -19.75], [242.75, -19.75], [245.25, -19.75], [247.75, -19.75], [250.25, -19.75], [252.75, -19.75], [255.25, -19.75], [257.75, -19.75], [260.25, -19.75], [262.75, -19.75], [265.25, -19.75], [267.75, -19.75], [270.25, -19.75], [272.75, -19.75], [275.25, -19.75], [277.75, -19.75], [280.25, -19.75], [282.75, -19.75], [285.25, -19.75], [287.75, -19.75], [320.25, -19.75], [322.75, -19.75], [325.25, -19.75], [327.75, -19.75], [330.25, -19.75], [332.75, -19.75], [335.25, -19.75], [337.75, -19.75], [340.25, -19.75], [342.75, -19.75], [345.25, -19.75], [347.75, -19.75], [350.25, -19.75], [352.75, -19.75], [355.25, -19.75], [357.75, -19.75], [0.25, -17.25], [2.75, -17.25], [5.25, -17.25], [7.75, -17.25], [10.25, -17.25], [37.75, -17.25], [40.25, -17.25], [42.75, -17.25], [50.25, -17.25], [52.75, -17.25], [55.25, -17.25], [57.75, -17.25], [60.25, -17.25], [62.75, -17.25], [65.25, -17.25], [67.75, -17.25], [70.25, -17.25], [72.75, -17.25], [75.25, -17.25], [77.75, -17.25], [80.25, -17.25], [82.75, -17.25], [85.25, -17.25], [87.75, -17.25], [90.25, -17.25], [92.75, -17.25], [95.25, -17.25], [97.75, -17.25], [100.25, -17.25], [102.75, -17.25], [105.25, -17.25], [107.75, -17.25], [110.25, -17.25], [112.75, -17.25], [115.25, -17.25], [117.75, -17.25], [120.25, -17.25], [140.25, -17.25], [147.75, -17.25], [150.25, -17.25], [152.75, -17.25], [155.25, -17.25], [157.75, -17.25], [160.25, -17.25], [162.75, -17.25], [165.25, -17.25], [167.75, -17.25], [170.25, -17.25], [172.75, -17.25], [175.25, -17.25], [177.75, -17.25], [180.25, -17.25], [182.75, -17.25], [185.25, -17.25], [187.75, -17.25], [190.25, -17.25], [192.75, -17.25], [195.25, -17.25], [197.75, -17.25], [200.25, -17.25], [202.75, -17.25], [205.25, -17.25], [207.75, -17.25], [210.25, -17.25], [212.75, -17.25], [215.25, -17.25], [217.75, -17.25], [220.25, -17.25], [222.75, -17.25], [225.25, -17.25], [227.75, -17.25], [230.25, -17.25], [232.75, -17.25], [235.25, -17.25], [237.75, -17.25], [240.25, -17.25], [242.75, -17.25], [245.25, -17.25], [247.75, -17.25], [250.25, -17.25], [252.75, -17.25], [255.25, -17.25], [257.75, -17.25], [260.25, -17.25], [262.75, -17.25], [265.25, -17.25], [267.75, -17.25], [270.25, -17.25], [272.75, -17.25], [275.25, -17.25], [277.75, -17.25], [280.25, -17.25], [282.75, -17.25], [285.25, -17.25], [287.75, -17.25], [322.75, -17.25], [325.25, -17.25], [327.75, -17.25], [330.25, -17.25], [332.75, -17.25], [335.25, -17.25], [337.75, -17.25], [340.25, -17.25], [342.75, -17.25], [345.25, -17.25], [347.75, -17.25], [350.25, -17.25], [352.75, -17.25], [355.25, -17.25], [357.75, -17.25], [0.25, -14.75], [2.75, -14.75], [5.25, -14.75], [7.75, -14.75], [10.25, -14.75], [42.75, -14.75], [45.25, -14.75], [47.75, -14.75], [50.25, -14.75], [52.75, -14.75], [55.25, -14.75], [57.75, -14.75], [60.25, -14.75], [62.75, -14.75], [65.25, -14.75], [67.75, -14.75], [70.25, -14.75], [72.75, -14.75], [75.25, -14.75], [77.75, -14.75], [80.25, -14.75], [82.75, -14.75], [85.25, -14.75], [87.75, -14.75], [90.25, -14.75], [92.75, -14.75], [95.25, -14.75], [97.75, -14.75], [100.25, -14.75], [102.75, -14.75], [105.25, -14.75], [107.75, -14.75], [110.25, -14.75], [112.75, -14.75], [115.25, -14.75], [117.75, -14.75], [120.25, -14.75], [122.75, -14.75], [125.25, -14.75], [127.75, -14.75], [135.25, -14.75], [137.75, -14.75], [140.25, -14.75], [145.25, -14.75], [147.75, -14.75], [150.25, -14.75], [152.75, -14.75], [155.25, -14.75], [157.75, -14.75], [160.25, -14.75], [162.75, -14.75], [165.25, -14.75], [167.75, -14.75], [170.25, -14.75], [172.75, -14.75], [175.25, -14.75], [177.75, -14.75], [180.25, -14.75], [182.75, -14.75], [185.25, -14.75], [187.75, -14.75], [190.25, -14.75], [192.75, -14.75], [195.25, -14.75], [197.75, -14.75], [200.25, -14.75], [202.75, -14.75], [205.25, -14.75], [207.75, -14.75], [210.25, -14.75], [212.75, -14.75], [215.25, -14.75], [217.75, -14.75], [220.25, -14.75], [222.75, -14.75], [225.25, -14.75], [227.75, -14.75], [230.25, -14.75], [232.75, -14.75], [235.25, -14.75], [237.75, -14.75], [240.25, -14.75], [242.75, -14.75], [245.25, -14.75], [247.75, -14.75], [250.25, -14.75], [252.75, -14.75], [255.25, -14.75], [257.75, -14.75], [260.25, -14.75], [262.75, -14.75], [265.25, -14.75], [267.75, -14.75], [270.25, -14.75], [272.75, -14.75], [275.25, -14.75], [277.75, -14.75], [280.25, -14.75], [282.75, -14.75], [322.75, -14.75], [325.25, -14.75], [327.75, -14.75], [330.25, -14.75], [332.75, -14.75], [335.25, -14.75], [337.75, -14.75], [340.25, -14.75], [342.75, -14.75], [345.25, -14.75], [347.75, -14.75], [350.25, -14.75], [352.75, -14.75], [355.25, -14.75], [357.75, -14.75], [0.25, -12.25], [2.75, -12.25], [5.25, -12.25], [7.75, -12.25], [10.25, -12.25], [12.75, -12.25], [42.75, -12.25], [45.25, -12.25], [47.75, -12.25], [50.25, -12.25], [52.75, -12.25], [55.25, -12.25], [57.75, -12.25], [60.25, -12.25], [62.75, -12.25], [65.25, -12.25], [67.75, -12.25], [70.25, -12.25], [72.75, -12.25], [75.25, -12.25], [77.75, -12.25], [80.25, -12.25], [82.75, -12.25], [85.25, -12.25], [87.75, -12.25], [90.25, -12.25], [92.75, -12.25], [95.25, -12.25], [97.75, -12.25], [100.25, -12.25], [102.75, -12.25], [105.25, -12.25], [107.75, -12.25], [110.25, -12.25], [112.75, -12.25], [115.25, -12.25], [117.75, -12.25], [120.25, -12.25], [122.75, -12.25], [125.25, -12.25], [127.75, -12.25], [130.25, -12.25], [132.75, -12.25], [135.25, -12.25], [137.75, -12.25], [140.25, -12.25], [145.25, -12.25], [147.75, -12.25], [150.25, -12.25], [152.75, -12.25], [155.25, -12.25], [157.75, -12.25], [160.25, -12.25], [162.75, -12.25], [165.25, -12.25], [167.75, -12.25], [170.25, -12.25], [172.75, -12.25], [175.25, -12.25], [177.75, -12.25], [180.25, -12.25], [182.75, -12.25], [185.25, -12.25], [187.75, -12.25], [190.25, -12.25], [192.75, -12.25], [195.25, -12.25], [197.75, -12.25], [200.25, -12.25], [202.75, -12.25], [205.25, -12.25], [207.75, -12.25], [210.25, -12.25], [212.75, -12.25], [215.25, -12.25], [217.75, -12.25], [220.25, -12.25], [222.75, -12.25], [225.25, -12.25], [227.75, -12.25], [230.25, -12.25], [232.75, -12.25], [235.25, -12.25], [237.75, -12.25], [240.25, -12.25], [242.75, -12.25], [245.25, -12.25], [247.75, -12.25], [250.25, -12.25], [252.75, -12.25], [255.25, -12.25], [257.75, -12.25], [260.25, -12.25], [262.75, -12.25], [265.25, -12.25], [267.75, -12.25], [270.25, -12.25], [272.75, -12.25], [275.25, -12.25], [277.75, -12.25], [280.25, -12.25], [282.75, -12.25], [322.75, -12.25], [325.25, -12.25], [327.75, -12.25], [330.25, -12.25], [332.75, -12.25], [335.25, -12.25], [337.75, -12.25], [340.25, -12.25], [342.75, -12.25], [345.25, -12.25], [347.75, -12.25], [350.25, -12.25], [352.75, -12.25], [355.25, -12.25], [357.75, -12.25], [0.25, -9.75], [2.75, -9.75], [5.25, -9.75], [7.75, -9.75], [10.25, -9.75], [12.75, -9.75], [40.25, -9.75], [42.75, -9.75], [45.25, -9.75], [47.75, -9.75], [50.25, -9.75], [52.75, -9.75], [55.25, -9.75], [57.75, -9.75], [60.25, -9.75], [62.75, -9.75], [65.25, -9.75], [67.75, -9.75], [70.25, -9.75], [72.75, -9.75], [75.25, -9.75], [77.75, -9.75], [80.25, -9.75], [82.75, -9.75], [85.25, -9.75], [87.75, -9.75], [90.25, -9.75], [92.75, -9.75], [95.25, -9.75], [97.75, -9.75], [100.25, -9.75], [102.75, -9.75], [105.25, -9.75], [107.75, -9.75], [110.25, -9.75], [112.75, -9.75], [115.25, -9.75], [117.75, -9.75], [120.25, -9.75], [122.75, -9.75], [125.25, -9.75], [127.75, -9.75], [130.25, -9.75], [132.75, -9.75], [135.25, -9.75], [137.75, -9.75], [140.25, -9.75], [142.75, -9.75], [145.25, -9.75], [147.75, -9.75], [150.25, -9.75], [152.75, -9.75], [155.25, -9.75], [157.75, -9.75], [160.25, -9.75], [162.75, -9.75], [165.25, -9.75], [167.75, -9.75], [170.25, -9.75], [172.75, -9.75], [175.25, -9.75], [177.75, -9.75], [180.25, -9.75], [182.75, -9.75], [185.25, -9.75], [187.75, -9.75], [190.25, -9.75], [192.75, -9.75], [195.25, -9.75], [197.75, -9.75], [200.25, -9.75], [202.75, -9.75], [205.25, -9.75], [207.75, -9.75], [210.25, -9.75], [212.75, -9.75], [215.25, -9.75], [217.75, -9.75], [220.25, -9.75], [222.75, -9.75], [225.25, -9.75], [227.75, -9.75], [230.25, -9.75], [232.75, -9.75], [235.25, -9.75], [237.75, -9.75], [240.25, -9.75], [242.75, -9.75], [245.25, -9.75], [247.75, -9.75], [250.25, -9.75], [252.75, -9.75], [255.25, -9.75], [257.75, -9.75], [260.25, -9.75], [262.75, -9.75], [265.25, -9.75], [267.75, -9.75], [270.25, -9.75], [272.75, -9.75], [275.25, -9.75], [277.75, -9.75], [280.25, -9.75], [325.25, -9.75], [327.75, -9.75], [330.25, -9.75], [332.75, -9.75], [335.25, -9.75], [337.75, -9.75], [340.25, -9.75], [342.75, -9.75], [345.25, -9.75], [347.75, -9.75], [350.25, -9.75], [352.75, -9.75], [355.25, -9.75], [357.75, -9.75], [0.25, -7.25], [2.75, -7.25], [5.25, -7.25], [7.75, -7.25], [10.25, -7.25], [12.75, -7.25], [40.25, -7.25], [42.75, -7.25], [45.25, -7.25], [47.75, -7.25], [50.25, -7.25], [52.75, -7.25], [55.25, -7.25], [57.75, -7.25], [60.25, -7.25], [62.75, -7.25], [65.25, -7.25], [67.75, -7.25], [70.25, -7.25], [72.75, -7.25], [75.25, -7.25], [77.75, -7.25], [80.25, -7.25], [82.75, -7.25], [85.25, -7.25], [87.75, -7.25], [90.25, -7.25], [92.75, -7.25], [95.25, -7.25], [97.75, -7.25], [100.25, -7.25], [102.75, -7.25], [105.25, -7.25], [107.75, -7.25], [112.75, -7.25], [115.25, -7.25], [117.75, -7.25], [120.25, -7.25], [122.75, -7.25], [125.25, -7.25], [127.75, -7.25], [130.25, -7.25], [132.75, -7.25], [135.25, -7.25], [137.75, -7.25], [147.75, -7.25], [150.25, -7.25], [152.75, -7.25], [155.25, -7.25], [157.75, -7.25], [160.25, -7.25], [162.75, -7.25], [165.25, -7.25], [167.75, -7.25], [170.25, -7.25], [172.75, -7.25], [175.25, -7.25], [177.75, -7.25], [180.25, -7.25], [182.75, -7.25], [185.25, -7.25], [187.75, -7.25], [190.25, -7.25], [192.75, -7.25], [195.25, -7.25], [197.75, -7.25], [200.25, -7.25], [202.75, -7.25], [205.25, -7.25], [207.75, -7.25], [210.25, -7.25], [212.75, -7.25], [215.25, -7.25], [217.75, -7.25], [220.25, -7.25], [222.75, -7.25], [225.25, -7.25], [227.75, -7.25], [230.25, -7.25], [232.75, -7.25], [235.25, -7.25], [237.75, -7.25], [240.25, -7.25], [242.75, -7.25], [245.25, -7.25], [247.75, -7.25], [250.25, -7.25], [252.75, -7.25], [255.25, -7.25], [257.75, -7.25], [260.25, -7.25], [262.75, -7.25], [265.25, -7.25], [267.75, -7.25], [270.25, -7.25], [272.75, -7.25], [275.25, -7.25], [277.75, -7.25], [280.25, -7.25], [325.25, -7.25], [327.75, -7.25], [330.25, -7.25], [332.75, -7.25], [335.25, -7.25], [337.75, -7.25], [340.25, -7.25], [342.75, -7.25], [345.25, -7.25], [347.75, -7.25], [350.25, -7.25], [352.75, -7.25], [355.25, -7.25], [357.75, -7.25], [0.25, -4.75], [2.75, -4.75], [5.25, -4.75], [7.75, -4.75], [10.25, -4.75], [40.25, -4.75], [42.75, -4.75], [45.25, -4.75], [47.75, -4.75], [50.25, -4.75], [52.75, -4.75], [55.25, -4.75], [57.75, -4.75], [60.25, -4.75], [62.75, -4.75], [65.25, -4.75], [67.75, -4.75], [70.25, -4.75], [72.75, -4.75], [75.25, -4.75], [77.75, -4.75], [80.25, -4.75], [82.75, -4.75], [85.25, -4.75], [87.75, -4.75], [90.25, -4.75], [92.75, -4.75], [95.25, -4.75], [97.75, -4.75], [100.25, -4.75], [102.75, -4.75], [107.75, -4.75], [110.25, -4.75], [112.75, -4.75], [115.25, -4.75], [117.75, -4.75], [120.25, -4.75], [122.75, -4.75], [125.25, -4.75], [127.75, -4.75], [130.25, -4.75], [132.75, -4.75], [135.25, -4.75], [137.75, -4.75], [147.75, -4.75], [150.25, -4.75], [152.75, -4.75], [155.25, -4.75], [157.75, -4.75], [160.25, -4.75], [162.75, -4.75], [165.25, -4.75], [167.75, -4.75], [170.25, -4.75], [172.75, -4.75], [175.25, -4.75], [177.75, -4.75], [180.25, -4.75], [182.75, -4.75], [185.25, -4.75], [187.75, -4.75], [190.25, -4.75], [192.75, -4.75], [195.25, -4.75], [197.75, -4.75], [200.25, -4.75], [202.75, -4.75], [205.25, -4.75], [207.75, -4.75], [210.25, -4.75], [212.75, -4.75], [215.25, -4.75], [217.75, -4.75], [220.25, -4.75], [222.75, -4.75], [225.25, -4.75], [227.75, -4.75], [230.25, -4.75], [232.75, -4.75], [235.25, -4.75], [237.75, -4.75], [240.25, -4.75], [242.75, -4.75], [245.25, -4.75], [247.75, -4.75], [250.25, -4.75], [252.75, -4.75], [255.25, -4.75], [257.75, -4.75], [260.25, -4.75], [262.75, -4.75], [265.25, -4.75], [267.75, -4.75], [270.25, -4.75], [272.75, -4.75], [275.25, -4.75], [277.75, -4.75], [322.75, -4.75], [325.25, -4.75], [327.75, -4.75], [330.25, -4.75], [332.75, -4.75], [335.25, -4.75], [337.75, -4.75], [340.25, -4.75], [342.75, -4.75], [345.25, -4.75], [347.75, -4.75], [350.25, -4.75], [352.75, -4.75], [355.25, -4.75], [357.75, -4.75], [0.25, -2.25], [2.75, -2.25], [5.25, -2.25], [7.75, -2.25], [40.25, -2.25], [42.75, -2.25], [45.25, -2.25], [47.75, -2.25], [50.25, -2.25], [52.75, -2.25], [55.25, -2.25], [57.75, -2.25], [60.25, -2.25], [62.75, -2.25], [65.25, -2.25], [67.75, -2.25], [70.25, -2.25], [72.75, -2.25], [75.25, -2.25], [77.75, -2.25], [80.25, -2.25], [82.75, -2.25], [85.25, -2.25], [87.75, -2.25], [90.25, -2.25], [92.75, -2.25], [95.25, -2.25], [97.75, -2.25], [100.25, -2.25], [105.25, -2.25], [107.75, -2.25], [110.25, -2.25], [117.75, -2.25], [122.75, -2.25], [125.25, -2.25], [127.75, -2.25], [130.25, -2.25], [132.75, -2.25], [135.25, -2.25], [140.25, -2.25], [142.75, -2.25], [145.25, -2.25], [147.75, -2.25], [150.25, -2.25], [152.75, -2.25], [155.25, -2.25], [157.75, -2.25], [160.25, -2.25], [162.75, -2.25], [165.25, -2.25], [167.75, -2.25], [170.25, -2.25], [172.75, -2.25], [175.25, -2.25], [177.75, -2.25], [180.25, -2.25], [182.75, -2.25], [185.25, -2.25], [187.75, -2.25], [190.25, -2.25], [192.75, -2.25], [195.25, -2.25], [197.75, -2.25], [200.25, -2.25], [202.75, -2.25], [205.25, -2.25], [207.75, -2.25], [210.25, -2.25], [212.75, -2.25], [215.25, -2.25], [217.75, -2.25], [220.25, -2.25], [222.75, -2.25], [225.25, -2.25], [227.75, -2.25], [230.25, -2.25], [232.75, -2.25], [235.25, -2.25], [237.75, -2.25], [240.25, -2.25], [242.75, -2.25], [245.25, -2.25], [247.75, -2.25], [250.25, -2.25], [252.75, -2.25], [255.25, -2.25], [257.75, -2.25], [260.25, -2.25], [262.75, -2.25], [265.25, -2.25], [267.75, -2.25], [270.25, -2.25], [272.75, -2.25], [275.25, -2.25], [277.75, -2.25], [280.25, -2.25], [315.25, -2.25], [317.75, -2.25], [320.25, -2.25], [322.75, -2.25], [325.25, -2.25], [327.75, -2.25], [330.25, -2.25], [332.75, -2.25], [335.25, -2.25], [337.75, -2.25], [340.25, -2.25], [342.75, -2.25], [345.25, -2.25], [347.75, -2.25], [350.25, -2.25], [352.75, -2.25], [355.25, -2.25], [357.75, -2.25], [0.25, 0.25], [2.75, 0.25], [5.25, 0.25], [7.75, 0.25], [45.25, 0.25], [47.75, 0.25], [50.25, 0.25], [52.75, 0.25], [55.25, 0.25], [57.75, 0.25], [60.25, 0.25], [62.75, 0.25], [65.25, 0.25], [67.75, 0.25], [70.25, 0.25], [72.75, 0.25], [75.25, 0.25], [77.75, 0.25], [80.25, 0.25], [82.75, 0.25], [85.25, 0.25], [87.75, 0.25], [90.25, 0.25], [92.75, 0.25], [95.25, 0.25], [97.75, 0.25], [105.25, 0.25], [107.75, 0.25], [117.75, 0.25], [120.25, 0.25], [122.75, 0.25], [125.25, 0.25], [127.75, 0.25], [130.25, 0.25], [132.75, 0.25], [135.25, 0.25], [137.75, 0.25], [140.25, 0.25], [142.75, 0.25], [145.25, 0.25], [147.75, 0.25], [150.25, 0.25], [152.75, 0.25], [155.25, 0.25], [157.75, 0.25], [160.25, 0.25], [162.75, 0.25], [165.25, 0.25], [167.75, 0.25], [170.25, 0.25], [172.75, 0.25], [175.25, 0.25], [177.75, 0.25], [180.25, 0.25], [182.75, 0.25], [185.25, 0.25], [187.75, 0.25], [190.25, 0.25], [192.75, 0.25], [195.25, 0.25], [197.75, 0.25], [200.25, 0.25], [202.75, 0.25], [205.25, 0.25], [207.75, 0.25], [210.25, 0.25], [212.75, 0.25], [215.25, 0.25], [217.75, 0.25], [220.25, 0.25], [222.75, 0.25], [225.25, 0.25], [227.75, 0.25], [230.25, 0.25], [232.75, 0.25], [235.25, 0.25], [237.75, 0.25], [240.25, 0.25], [242.75, 0.25], [245.25, 0.25], [247.75, 0.25], [250.25, 0.25], [252.75, 0.25], [255.25, 0.25], [257.75, 0.25], [260.25, 0.25], [262.75, 0.25], [265.25, 0.25], [267.75, 0.25], [270.25, 0.25], [272.75, 0.25], [275.25, 0.25], [277.75, 0.25], [310.25, 0.25], [312.75, 0.25], [315.25, 0.25], [317.75, 0.25], [320.25, 0.25], [322.75, 0.25], [325.25, 0.25], [327.75, 0.25], [330.25, 0.25], [332.75, 0.25], [335.25, 0.25], [337.75, 0.25], [340.25, 0.25], [342.75, 0.25], [345.25, 0.25], [347.75, 0.25], [350.25, 0.25], [352.75, 0.25], [355.25, 0.25], [357.75, 0.25], [0.25, 2.75], [2.75, 2.75], [5.25, 2.75], [7.75, 2.75], [47.75, 2.75], [50.25, 2.75], [52.75, 2.75], [55.25, 2.75], [57.75, 2.75], [60.25, 2.75], [62.75, 2.75], [65.25, 2.75], [67.75, 2.75], [70.25, 2.75], [72.75, 2.75], [75.25, 2.75], [77.75, 2.75], [80.25, 2.75], [82.75, 2.75], [85.25, 2.75], [87.75, 2.75], [90.25, 2.75], [92.75, 2.75], [95.25, 2.75], [97.75, 2.75], [100.25, 2.75], [105.25, 2.75], [107.75, 2.75], [110.25, 2.75], [112.75, 2.75], [117.75, 2.75], [120.25, 2.75], [122.75, 2.75], [125.25, 2.75], [127.75, 2.75], [130.25, 2.75], [132.75, 2.75], [135.25, 2.75], [137.75, 2.75], [140.25, 2.75], [142.75, 2.75], [145.25, 2.75], [147.75, 2.75], [150.25, 2.75], [152.75, 2.75], [155.25, 2.75], [157.75, 2.75], [160.25, 2.75], [162.75, 2.75], [165.25, 2.75], [167.75, 2.75], [170.25, 2.75], [172.75, 2.75], [175.25, 2.75], [177.75, 2.75], [180.25, 2.75], [182.75, 2.75], [185.25, 2.75], [187.75, 2.75], [190.25, 2.75], [192.75, 2.75], [195.25, 2.75], [197.75, 2.75], [200.25, 2.75], [202.75, 2.75], [205.25, 2.75], [207.75, 2.75], [210.25, 2.75], [212.75, 2.75], [215.25, 2.75], [217.75, 2.75], [220.25, 2.75], [222.75, 2.75], [225.25, 2.75], [227.75, 2.75], [230.25, 2.75], [232.75, 2.75], [235.25, 2.75], [237.75, 2.75], [240.25, 2.75], [242.75, 2.75], [245.25, 2.75], [247.75, 2.75], [250.25, 2.75], [252.75, 2.75], [255.25, 2.75], [257.75, 2.75], [260.25, 2.75], [262.75, 2.75], [265.25, 2.75], [267.75, 2.75], [270.25, 2.75], [272.75, 2.75], [275.25, 2.75], [277.75, 2.75], [280.25, 2.75], [310.25, 2.75], [312.75, 2.75], [315.25, 2.75], [317.75, 2.75], [320.25, 2.75], [322.75, 2.75], [325.25, 2.75], [327.75, 2.75], [330.25, 2.75], [332.75, 2.75], [335.25, 2.75], [337.75, 2.75], [340.25, 2.75], [342.75, 2.75], [345.25, 2.75], [347.75, 2.75], [350.25, 2.75], [352.75, 2.75], [355.25, 2.75], [357.75, 2.75], [0.25, 5.25], [2.75, 5.25], [5.25, 5.25], [50.25, 5.25], [52.75, 5.25], [55.25, 5.25], [57.75, 5.25], [60.25, 5.25], [62.75, 5.25], [65.25, 5.25], [67.75, 5.25], [70.25, 5.25], [72.75, 5.25], [75.25, 5.25], [77.75, 5.25], [80.25, 5.25], [82.75, 5.25], [85.25, 5.25], [87.75, 5.25], [90.25, 5.25], [92.75, 5.25], [95.25, 5.25], [97.75, 5.25], [100.25, 5.25], [102.75, 5.25], [105.25, 5.25], [107.75, 5.25], [110.25, 5.25], [112.75, 5.25], [115.25, 5.25], [120.25, 5.25], [122.75, 5.25], [125.25, 5.25], [127.75, 5.25], [130.25, 5.25], [132.75, 5.25], [135.25, 5.25], [137.75, 5.25], [140.25, 5.25], [142.75, 5.25], [145.25, 5.25], [147.75, 5.25], [150.25, 5.25], [152.75, 5.25], [155.25, 5.25], [157.75, 5.25], [160.25, 5.25], [162.75, 5.25], [165.25, 5.25], [167.75, 5.25], [170.25, 5.25], [172.75, 5.25], [175.25, 5.25], [177.75, 5.25], [180.25, 5.25], [182.75, 5.25], [185.25, 5.25], [187.75, 5.25], [190.25, 5.25], [192.75, 5.25], [195.25, 5.25], [197.75, 5.25], [200.25, 5.25], [202.75, 5.25], [205.25, 5.25], [207.75, 5.25], [210.25, 5.25], [212.75, 5.25], [215.25, 5.25], [217.75, 5.25], [220.25, 5.25], [222.75, 5.25], [225.25, 5.25], [227.75, 5.25], [230.25, 5.25], [232.75, 5.25], [235.25, 5.25], [237.75, 5.25], [240.25, 5.25], [242.75, 5.25], [245.25, 5.25], [247.75, 5.25], [250.25, 5.25], [252.75, 5.25], [255.25, 5.25], [257.75, 5.25], [260.25, 5.25], [262.75, 5.25], [265.25, 5.25], [267.75, 5.25], [270.25, 5.25], [272.75, 5.25], [275.25, 5.25], [277.75, 5.25], [280.25, 5.25], [282.75, 5.25], [307.75, 5.25], [310.25, 5.25], [312.75, 5.25], [315.25, 5.25], [317.75, 5.25], [320.25, 5.25], [322.75, 5.25], [325.25, 5.25], [327.75, 5.25], [330.25, 5.25], [332.75, 5.25], [335.25, 5.25], [337.75, 5.25], [340.25, 5.25], [342.75, 5.25], [345.25, 5.25], [347.75, 5.25], [350.25, 5.25], [355.25, 5.25], [50.25, 7.75], [52.75, 7.75], [55.25, 7.75], [57.75, 7.75], [60.25, 7.75], [62.75, 7.75], [65.25, 7.75], [67.75, 7.75], [70.25, 7.75], [72.75, 7.75], [75.25, 7.75], [77.75, 7.75], [82.75, 7.75], [85.25, 7.75], [87.75, 7.75], [90.25, 7.75], [92.75, 7.75], [95.25, 7.75], [97.75, 7.75], [100.25, 7.75], [102.75, 7.75], [105.25, 7.75], [107.75, 7.75], [110.25, 7.75], [112.75, 7.75], [115.25, 7.75], [117.75, 7.75], [120.25, 7.75], [122.75, 7.75], [127.75, 7.75], [130.25, 7.75], [132.75, 7.75], [135.25, 7.75], [137.75, 7.75], [140.25, 7.75], [142.75, 7.75], [145.25, 7.75], [147.75, 7.75], [150.25, 7.75], [152.75, 7.75], [155.25, 7.75], [157.75, 7.75], [160.25, 7.75], [162.75, 7.75], [165.25, 7.75], [167.75, 7.75], [170.25, 7.75], [172.75, 7.75], [175.25, 7.75], [177.75, 7.75], [180.25, 7.75], [182.75, 7.75], [185.25, 7.75], [187.75, 7.75], [190.25, 7.75], [192.75, 7.75], [195.25, 7.75], [197.75, 7.75], [200.25, 7.75], [202.75, 7.75], [205.25, 7.75], [207.75, 7.75], [210.25, 7.75], [212.75, 7.75], [215.25, 7.75], [217.75, 7.75], [220.25, 7.75], [222.75, 7.75], [225.25, 7.75], [227.75, 7.75], [230.25, 7.75], [232.75, 7.75], [235.25, 7.75], [237.75, 7.75], [240.25, 7.75], [242.75, 7.75], [245.25, 7.75], [247.75, 7.75], [250.25, 7.75], [252.75, 7.75], [255.25, 7.75], [257.75, 7.75], [260.25, 7.75], [262.75, 7.75], [265.25, 7.75], [267.75, 7.75], [270.25, 7.75], [272.75, 7.75], [275.25, 7.75], [277.75, 7.75], [280.25, 7.75], [302.75, 7.75], [305.25, 7.75], [307.75, 7.75], [310.25, 7.75], [312.75, 7.75], [315.25, 7.75], [317.75, 7.75], [320.25, 7.75], [322.75, 7.75], [325.25, 7.75], [327.75, 7.75], [330.25, 7.75], [332.75, 7.75], [335.25, 7.75], [337.75, 7.75], [340.25, 7.75], [342.75, 7.75], [345.25, 7.75], [45.25, 10.25], [52.75, 10.25], [55.25, 10.25], [57.75, 10.25], [60.25, 10.25], [62.75, 10.25], [65.25, 10.25], [67.75, 10.25], [70.25, 10.25], [72.75, 10.25], [75.25, 10.25], [80.25, 10.25], [82.75, 10.25], [85.25, 10.25], [87.75, 10.25], [90.25, 10.25], [92.75, 10.25], [95.25, 10.25], [97.75, 10.25], [100.25, 10.25], [102.75, 10.25], [107.75, 10.25], [110.25, 10.25], [112.75, 10.25], [115.25, 10.25], [117.75, 10.25], [120.25, 10.25], [122.75, 10.25], [125.25, 10.25], [127.75, 10.25], [130.25, 10.25], [132.75, 10.25], [135.25, 10.25], [137.75, 10.25], [140.25, 10.25], [142.75, 10.25], [145.25, 10.25], [147.75, 10.25], [150.25, 10.25], [152.75, 10.25], [155.25, 10.25], [157.75, 10.25], [160.25, 10.25], [162.75, 10.25], [165.25, 10.25], [167.75, 10.25], [170.25, 10.25], [172.75, 10.25], [175.25, 10.25], [177.75, 10.25], [180.25, 10.25], [182.75, 10.25], [185.25, 10.25], [187.75, 10.25], [190.25, 10.25], [192.75, 10.25], [195.25, 10.25], [197.75, 10.25], [200.25, 10.25], [202.75, 10.25], [205.25, 10.25], [207.75, 10.25], [210.25, 10.25], [212.75, 10.25], [215.25, 10.25], [217.75, 10.25], [220.25, 10.25], [222.75, 10.25], [225.25, 10.25], [227.75, 10.25], [230.25, 10.25], [232.75, 10.25], [235.25, 10.25], [237.75, 10.25], [240.25, 10.25], [242.75, 10.25], [245.25, 10.25], [247.75, 10.25], [250.25, 10.25], [252.75, 10.25], [255.25, 10.25], [257.75, 10.25], [260.25, 10.25], [262.75, 10.25], [265.25, 10.25], [267.75, 10.25], [270.25, 10.25], [272.75, 10.25], [277.75, 10.25], [280.25, 10.25], [282.75, 10.25], [292.75, 10.25], [295.25, 10.25], [297.75, 10.25], [300.25, 10.25], [302.75, 10.25], [305.25, 10.25], [307.75, 10.25], [310.25, 10.25], [312.75, 10.25], [315.25, 10.25], [317.75, 10.25], [320.25, 10.25], [322.75, 10.25], [325.25, 10.25], [327.75, 10.25], [330.25, 10.25], [332.75, 10.25], [335.25, 10.25], [337.75, 10.25], [340.25, 10.25], [342.75, 10.25], [345.25, 10.25], [42.75, 12.75], [45.25, 12.75], [47.75, 12.75], [50.25, 12.75], [52.75, 12.75], [55.25, 12.75], [57.75, 12.75], [60.25, 12.75], [62.75, 12.75], [65.25, 12.75], [67.75, 12.75], [70.25, 12.75], [72.75, 12.75], [80.25, 12.75], [82.75, 12.75], [85.25, 12.75], [87.75, 12.75], [90.25, 12.75], [92.75, 12.75], [95.25, 12.75], [97.75, 12.75], [100.25, 12.75], [110.25, 12.75], [112.75, 12.75], [115.25, 12.75], [117.75, 12.75], [120.25, 12.75], [122.75, 12.75], [125.25, 12.75], [127.75, 12.75], [130.25, 12.75], [132.75, 12.75], [135.25, 12.75], [137.75, 12.75], [140.25, 12.75], [142.75, 12.75], [145.25, 12.75], [147.75, 12.75], [150.25, 12.75], [152.75, 12.75], [155.25, 12.75], [157.75, 12.75], [160.25, 12.75], [162.75, 12.75], [165.25, 12.75], [167.75, 12.75], [170.25, 12.75], [172.75, 12.75], [175.25, 12.75], [177.75, 12.75], [180.25, 12.75], [182.75, 12.75], [185.25, 12.75], [187.75, 12.75], [190.25, 12.75], [192.75, 12.75], [195.25, 12.75], [197.75, 12.75], [200.25, 12.75], [202.75, 12.75], [205.25, 12.75], [207.75, 12.75], [210.25, 12.75], [212.75, 12.75], [215.25, 12.75], [217.75, 12.75], [220.25, 12.75], [222.75, 12.75], [225.25, 12.75], [227.75, 12.75], [230.25, 12.75], [232.75, 12.75], [235.25, 12.75], [237.75, 12.75], [240.25, 12.75], [242.75, 12.75], [245.25, 12.75], [247.75, 12.75], [250.25, 12.75], [252.75, 12.75], [255.25, 12.75], [257.75, 12.75], [260.25, 12.75], [262.75, 12.75], [265.25, 12.75], [267.75, 12.75], [270.25, 12.75], [272.75, 12.75], [277.75, 12.75], [280.25, 12.75], [282.75, 12.75], [285.25, 12.75], [287.75, 12.75], [290.25, 12.75], [292.75, 12.75], [295.25, 12.75], [297.75, 12.75], [300.25, 12.75], [302.75, 12.75], [305.25, 12.75], [307.75, 12.75], [310.25, 12.75], [312.75, 12.75], [315.25, 12.75], [317.75, 12.75], [320.25, 12.75], [322.75, 12.75], [325.25, 12.75], [327.75, 12.75], [330.25, 12.75], [332.75, 12.75], [335.25, 12.75], [337.75, 12.75], [340.25, 12.75], [342.75, 12.75], [40.25, 15.25], [42.75, 15.25], [50.25, 15.25], [52.75, 15.25], [55.25, 15.25], [57.75, 15.25], [60.25, 15.25], [62.75, 15.25], [65.25, 15.25], [67.75, 15.25], [70.25, 15.25], [72.75, 15.25], [80.25, 15.25], [82.75, 15.25], [85.25, 15.25], [87.75, 15.25], [90.25, 15.25], [92.75, 15.25], [95.25, 15.25], [97.75, 15.25], [110.25, 15.25], [112.75, 15.25], [115.25, 15.25], [117.75, 15.25], [120.25, 15.25], [122.75, 15.25], [125.25, 15.25], [127.75, 15.25], [130.25, 15.25], [132.75, 15.25], [135.25, 15.25], [137.75, 15.25], [140.25, 15.25], [142.75, 15.25], [145.25, 15.25], [147.75, 15.25], [150.25, 15.25], [152.75, 15.25], [155.25, 15.25], [157.75, 15.25], [160.25, 15.25], [162.75, 15.25], [165.25, 15.25], [167.75, 15.25], [170.25, 15.25], [172.75, 15.25], [175.25, 15.25], [177.75, 15.25], [180.25, 15.25], [182.75, 15.25], [185.25, 15.25], [187.75, 15.25], [190.25, 15.25], [192.75, 15.25], [195.25, 15.25], [197.75, 15.25], [200.25, 15.25], [202.75, 15.25], [205.25, 15.25], [207.75, 15.25], [210.25, 15.25], [212.75, 15.25], [215.25, 15.25], [217.75, 15.25], [220.25, 15.25], [222.75, 15.25], [225.25, 15.25], [227.75, 15.25], [230.25, 15.25], [232.75, 15.25], [235.25, 15.25], [237.75, 15.25], [240.25, 15.25], [242.75, 15.25], [245.25, 15.25], [247.75, 15.25], [250.25, 15.25], [252.75, 15.25], [255.25, 15.25], [257.75, 15.25], [260.25, 15.25], [262.75, 15.25], [265.25, 15.25], [277.75, 15.25], [280.25, 15.25], [282.75, 15.25], [285.25, 15.25], [287.75, 15.25], [290.25, 15.25], [292.75, 15.25], [295.25, 15.25], [297.75, 15.25], [300.25, 15.25], [302.75, 15.25], [305.25, 15.25], [307.75, 15.25], [310.25, 15.25], [312.75, 15.25], [315.25, 15.25], [317.75, 15.25], [320.25, 15.25], [322.75, 15.25], [325.25, 15.25], [327.75, 15.25], [330.25, 15.25], [332.75, 15.25], [335.25, 15.25], [337.75, 15.25], [340.25, 15.25], [342.75, 15.25], [40.25, 17.75], [55.25, 17.75], [57.75, 17.75], [60.25, 17.75], [62.75, 17.75], [65.25, 17.75], [67.75, 17.75], [70.25, 17.75], [72.75, 17.75], [85.25, 17.75], [87.75, 17.75], [90.25, 17.75], [92.75, 17.75], [107.75, 17.75], [110.25, 17.75], [112.75, 17.75], [115.25, 17.75], [117.75, 17.75], [120.25, 17.75], [122.75, 17.75], [125.25, 17.75], [127.75, 17.75], [130.25, 17.75], [132.75, 17.75], [135.25, 17.75], [137.75, 17.75], [140.25, 17.75], [142.75, 17.75], [145.25, 17.75], [147.75, 17.75], [150.25, 17.75], [152.75, 17.75], [155.25, 17.75], [157.75, 17.75], [160.25, 17.75], [162.75, 17.75], [165.25, 17.75], [167.75, 17.75], [170.25, 17.75], [172.75, 17.75], [175.25, 17.75], [177.75, 17.75], [180.25, 17.75], [182.75, 17.75], [185.25, 17.75], [187.75, 17.75], [190.25, 17.75], [192.75, 17.75], [195.25, 17.75], [197.75, 17.75], [200.25, 17.75], [202.75, 17.75], [205.25, 17.75], [207.75, 17.75], [210.25, 17.75], [212.75, 17.75], [215.25, 17.75], [217.75, 17.75], [220.25, 17.75], [222.75, 17.75], [225.25, 17.75], [227.75, 17.75], [230.25, 17.75], [232.75, 17.75], [235.25, 17.75], [237.75, 17.75], [240.25, 17.75], [242.75, 17.75], [245.25, 17.75], [247.75, 17.75], [250.25, 17.75], [252.75, 17.75], [255.25, 17.75], [257.75, 17.75], [272.75, 17.75], [275.25, 17.75], [277.75, 17.75], [280.25, 17.75], [282.75, 17.75], [285.25, 17.75], [287.75, 17.75], [290.25, 17.75], [292.75, 17.75], [295.25, 17.75], [297.75, 17.75], [300.25, 17.75], [302.75, 17.75], [305.25, 17.75], [307.75, 17.75], [310.25, 17.75], [312.75, 17.75], [315.25, 17.75], [317.75, 17.75], [320.25, 17.75], [322.75, 17.75], [325.25, 17.75], [327.75, 17.75], [330.25, 17.75], [332.75, 17.75], [335.25, 17.75], [337.75, 17.75], [340.25, 17.75], [342.75, 17.75], [37.75, 20.25], [40.25, 20.25], [57.75, 20.25], [60.25, 20.25], [62.75, 20.25], [65.25, 20.25], [67.75, 20.25], [70.25, 20.25], [72.75, 20.25], [87.75, 20.25], [90.25, 20.25], [92.75, 20.25], [107.75, 20.25], [110.25, 20.25], [112.75, 20.25], [115.25, 20.25], [117.75, 20.25], [120.25, 20.25], [122.75, 20.25], [125.25, 20.25], [127.75, 20.25], [130.25, 20.25], [132.75, 20.25], [135.25, 20.25], [137.75, 20.25], [140.25, 20.25], [142.75, 20.25], [145.25, 20.25], [147.75, 20.25], [150.25, 20.25], [152.75, 20.25], [155.25, 20.25], [157.75, 20.25], [160.25, 20.25], [162.75, 20.25], [165.25, 20.25], [167.75, 20.25], [170.25, 20.25], [172.75, 20.25], [175.25, 20.25], [177.75, 20.25], [180.25, 20.25], [182.75, 20.25], [185.25, 20.25], [187.75, 20.25], [190.25, 20.25], [192.75, 20.25], [195.25, 20.25], [197.75, 20.25], [200.25, 20.25], [202.75, 20.25], [205.25, 20.25], [207.75, 20.25], [210.25, 20.25], [212.75, 20.25], [215.25, 20.25], [217.75, 20.25], [220.25, 20.25], [222.75, 20.25], [225.25, 20.25], [227.75, 20.25], [230.25, 20.25], [232.75, 20.25], [235.25, 20.25], [237.75, 20.25], [240.25, 20.25], [242.75, 20.25], [245.25, 20.25], [247.75, 20.25], [250.25, 20.25], [252.75, 20.25], [265.25, 20.25], [267.75, 20.25], [272.75, 20.25], [275.25, 20.25], [277.75, 20.25], [280.25, 20.25], [282.75, 20.25], [285.25, 20.25], [287.75, 20.25], [290.25, 20.25], [292.75, 20.25], [295.25, 20.25], [297.75, 20.25], [300.25, 20.25], [302.75, 20.25], [305.25, 20.25], [307.75, 20.25], [310.25, 20.25], [312.75, 20.25], [315.25, 20.25], [317.75, 20.25], [320.25, 20.25], [322.75, 20.25], [325.25, 20.25], [327.75, 20.25], [330.25, 20.25], [332.75, 20.25], [335.25, 20.25], [337.75, 20.25], [340.25, 20.25], [342.75, 20.25], [37.75, 22.75], [60.25, 22.75], [62.75, 22.75], [65.25, 22.75], [67.75, 22.75], [115.25, 22.75], [117.75, 22.75], [120.25, 22.75], [122.75, 22.75], [125.25, 22.75], [127.75, 22.75], [130.25, 22.75], [132.75, 22.75], [135.25, 22.75], [137.75, 22.75], [140.25, 22.75], [142.75, 22.75], [145.25, 22.75], [147.75, 22.75], [150.25, 22.75], [152.75, 22.75], [155.25, 22.75], [157.75, 22.75], [160.25, 22.75], [162.75, 22.75], [165.25, 22.75], [167.75, 22.75], [170.25, 22.75], [172.75, 22.75], [175.25, 22.75], [177.75, 22.75], [180.25, 22.75], [182.75, 22.75], [185.25, 22.75], [187.75, 22.75], [190.25, 22.75], [192.75, 22.75], [195.25, 22.75], [197.75, 22.75], [200.25, 22.75], [202.75, 22.75], [205.25, 22.75], [207.75, 22.75], [210.25, 22.75], [212.75, 22.75], [215.25, 22.75], [217.75, 22.75], [220.25, 22.75], [222.75, 22.75], [225.25, 22.75], [227.75, 22.75], [230.25, 22.75], [232.75, 22.75], [235.25, 22.75], [237.75, 22.75], [240.25, 22.75], [242.75, 22.75], [245.25, 22.75], [247.75, 22.75], [250.25, 22.75], [252.75, 22.75], [262.75, 22.75], [265.25, 22.75], [267.75, 22.75], [270.25, 22.75], [272.75, 22.75], [275.25, 22.75], [277.75, 22.75], [280.25, 22.75], [282.75, 22.75], [285.25, 22.75], [287.75, 22.75], [290.25, 22.75], [292.75, 22.75], [295.25, 22.75], [297.75, 22.75], [300.25, 22.75], [302.75, 22.75], [305.25, 22.75], [307.75, 22.75], [310.25, 22.75], [312.75, 22.75], [315.25, 22.75], [317.75, 22.75], [320.25, 22.75], [322.75, 22.75], [325.25, 22.75], [327.75, 22.75], [330.25, 22.75], [332.75, 22.75], [335.25, 22.75], [337.75, 22.75], [340.25, 22.75], [342.75, 22.75], [35.25, 25.25], [50.25, 25.25], [52.75, 25.25], [55.25, 25.25], [57.75, 25.25], [60.25, 25.25], [62.75, 25.25], [65.25, 25.25], [120.25, 25.25], [122.75, 25.25], [125.25, 25.25], [127.75, 25.25], [130.25, 25.25], [132.75, 25.25], [135.25, 25.25], [137.75, 25.25], [140.25, 25.25], [142.75, 25.25], [145.25, 25.25], [147.75, 25.25], [150.25, 25.25], [152.75, 25.25], [155.25, 25.25], [157.75, 25.25], [160.25, 25.25], [162.75, 25.25], [165.25, 25.25], [167.75, 25.25], [170.25, 25.25], [172.75, 25.25], [175.25, 25.25], [177.75, 25.25], [180.25, 25.25], [182.75, 25.25], [185.25, 25.25], [187.75, 25.25], [190.25, 25.25], [192.75, 25.25], [195.25, 25.25], [197.75, 25.25], [200.25, 25.25], [202.75, 25.25], [205.25, 25.25], [207.75, 25.25], [210.25, 25.25], [212.75, 25.25], [215.25, 25.25], [217.75, 25.25], [220.25, 25.25], [222.75, 25.25], [225.25, 25.25], [227.75, 25.25], [230.25, 25.25], [232.75, 25.25], [235.25, 25.25], [237.75, 25.25], [240.25, 25.25], [242.75, 25.25], [245.25, 25.25], [247.75, 25.25], [250.25, 25.25], [262.75, 25.25], [265.25, 25.25], [267.75, 25.25], [270.25, 25.25], [272.75, 25.25], [275.25, 25.25], [277.75, 25.25], [280.25, 25.25], [282.75, 25.25], [285.25, 25.25], [287.75, 25.25], [290.25, 25.25], [292.75, 25.25], [295.25, 25.25], [297.75, 25.25], [300.25, 25.25], [302.75, 25.25], [305.25, 25.25], [307.75, 25.25], [310.25, 25.25], [312.75, 25.25], [315.25, 25.25], [317.75, 25.25], [320.25, 25.25], [322.75, 25.25], [325.25, 25.25], [327.75, 25.25], [330.25, 25.25], [332.75, 25.25], [335.25, 25.25], [337.75, 25.25], [340.25, 25.25], [342.75, 25.25], [345.25, 25.25], [35.25, 27.75], [50.25, 27.75], [122.75, 27.75], [125.25, 27.75], [127.75, 27.75], [130.25, 27.75], [132.75, 27.75], [135.25, 27.75], [137.75, 27.75], [140.25, 27.75], [142.75, 27.75], [145.25, 27.75], [147.75, 27.75], [150.25, 27.75], [152.75, 27.75], [155.25, 27.75], [157.75, 27.75], [160.25, 27.75], [162.75, 27.75], [165.25, 27.75], [167.75, 27.75], [170.25, 27.75], [172.75, 27.75], [175.25, 27.75], [177.75, 27.75], [180.25, 27.75], [182.75, 27.75], [185.25, 27.75], [187.75, 27.75], [190.25, 27.75], [192.75, 27.75], [195.25, 27.75], [197.75, 27.75], [200.25, 27.75], [202.75, 27.75], [205.25, 27.75], [207.75, 27.75], [210.25, 27.75], [212.75, 27.75], [215.25, 27.75], [217.75, 27.75], [220.25, 27.75], [222.75, 27.75], [225.25, 27.75], [227.75, 27.75], [230.25, 27.75], [232.75, 27.75], [235.25, 27.75], [237.75, 27.75], [240.25, 27.75], [242.75, 27.75], [245.25, 27.75], [247.75, 27.75], [262.75, 27.75], [265.25, 27.75], [267.75, 27.75], [270.25, 27.75], [272.75, 27.75], [275.25, 27.75], [277.75, 27.75], [280.25, 27.75], [282.75, 27.75], [285.25, 27.75], [287.75, 27.75], [290.25, 27.75], [292.75, 27.75], [295.25, 27.75], [297.75, 27.75], [300.25, 27.75], [302.75, 27.75], [305.25, 27.75], [307.75, 27.75], [310.25, 27.75], [312.75, 27.75], [315.25, 27.75], [317.75, 27.75], [320.25, 27.75], [322.75, 27.75], [325.25, 27.75], [327.75, 27.75], [330.25, 27.75], [332.75, 27.75], [335.25, 27.75], [337.75, 27.75], [340.25, 27.75], [342.75, 27.75], [345.25, 27.75], [20.25, 30.25], [50.25, 30.25], [122.75, 30.25], [125.25, 30.25], [127.75, 30.25], [130.25, 30.25], [132.75, 30.25], [135.25, 30.25], [137.75, 30.25], [140.25, 30.25], [142.75, 30.25], [145.25, 30.25], [147.75, 30.25], [150.25, 30.25], [152.75, 30.25], [155.25, 30.25], [157.75, 30.25], [160.25, 30.25], [162.75, 30.25], [165.25, 30.25], [167.75, 30.25], [170.25, 30.25], [172.75, 30.25], [175.25, 30.25], [177.75, 30.25], [180.25, 30.25], [182.75, 30.25], [185.25, 30.25], [187.75, 30.25], [190.25, 30.25], [192.75, 30.25], [195.25, 30.25], [197.75, 30.25], [200.25, 30.25], [202.75, 30.25], [205.25, 30.25], [207.75, 30.25], [210.25, 30.25], [212.75, 30.25], [215.25, 30.25], [217.75, 30.25], [220.25, 30.25], [222.75, 30.25], [225.25, 30.25], [227.75, 30.25], [230.25, 30.25], [232.75, 30.25], [235.25, 30.25], [237.75, 30.25], [240.25, 30.25], [242.75, 30.25], [245.25, 30.25], [270.25, 30.25], [272.75, 30.25], [275.25, 30.25], [280.25, 30.25], [282.75, 30.25], [285.25, 30.25], [287.75, 30.25], [290.25, 30.25], [292.75, 30.25], [295.25, 30.25], [297.75, 30.25], [300.25, 30.25], [302.75, 30.25], [305.25, 30.25], [307.75, 30.25], [310.25, 30.25], [312.75, 30.25], [315.25, 30.25], [317.75, 30.25], [320.25, 30.25], [322.75, 30.25], [325.25, 30.25], [327.75, 30.25], [330.25, 30.25], [332.75, 30.25], [335.25, 30.25], [337.75, 30.25], [340.25, 30.25], [342.75, 30.25], [345.25, 30.25], [347.75, 30.25], [350.25, 30.25], [12.75, 32.75], [15.25, 32.75], [17.75, 32.75], [20.25, 32.75], [22.75, 32.75], [25.25, 32.75], [27.75, 32.75], [30.25, 32.75], [32.75, 32.75], [35.25, 32.75], [122.75, 32.75], [125.25, 32.75], [127.75, 32.75], [130.25, 32.75], [132.75, 32.75], [135.25, 32.75], [137.75, 32.75], [140.25, 32.75], [142.75, 32.75], [145.25, 32.75], [147.75, 32.75], [150.25, 32.75], [152.75, 32.75], [155.25, 32.75], [157.75, 32.75], [160.25, 32.75], [162.75, 32.75], [165.25, 32.75], [167.75, 32.75], [170.25, 32.75], [172.75, 32.75], [175.25, 32.75], [177.75, 32.75], [180.25, 32.75], [182.75, 32.75], [185.25, 32.75], [187.75, 32.75], [190.25, 32.75], [192.75, 32.75], [195.25, 32.75], [197.75, 32.75], [200.25, 32.75], [202.75, 32.75], [205.25, 32.75], [207.75, 32.75], [210.25, 32.75], [212.75, 32.75], [215.25, 32.75], [217.75, 32.75], [220.25, 32.75], [222.75, 32.75], [225.25, 32.75], [227.75, 32.75], [230.25, 32.75], [232.75, 32.75], [235.25, 32.75], [237.75, 32.75], [240.25, 32.75], [242.75, 32.75], [280.25, 32.75], [282.75, 32.75], [285.25, 32.75], [287.75, 32.75], [290.25, 32.75], [292.75, 32.75], [295.25, 32.75], [297.75, 32.75], [300.25, 32.75], [302.75, 32.75], [305.25, 32.75], [307.75, 32.75], [310.25, 32.75], [312.75, 32.75], [315.25, 32.75], [317.75, 32.75], [320.25, 32.75], [322.75, 32.75], [325.25, 32.75], [327.75, 32.75], [330.25, 32.75], [332.75, 32.75], [335.25, 32.75], [337.75, 32.75], [340.25, 32.75], [342.75, 32.75], [345.25, 32.75], [347.75, 32.75], [350.25, 32.75], [12.75, 35.25], [15.25, 35.25], [17.75, 35.25], [20.25, 35.25], [22.75, 35.25], [25.25, 35.25], [27.75, 35.25], [30.25, 35.25], [32.75, 35.25], [35.25, 35.25], [120.25, 35.25], [122.75, 35.25], [125.25, 35.25], [130.25, 35.25], [132.75, 35.25], [135.25, 35.25], [140.25, 35.25], [142.75, 35.25], [145.25, 35.25], [147.75, 35.25], [150.25, 35.25], [152.75, 35.25], [155.25, 35.25], [157.75, 35.25], [160.25, 35.25], [162.75, 35.25], [165.25, 35.25], [167.75, 35.25], [170.25, 35.25], [172.75, 35.25], [175.25, 35.25], [177.75, 35.25], [180.25, 35.25], [182.75, 35.25], [185.25, 35.25], [187.75, 35.25], [190.25, 35.25], [192.75, 35.25], [195.25, 35.25], [197.75, 35.25], [200.25, 35.25], [202.75, 35.25], [205.25, 35.25], [207.75, 35.25], [210.25, 35.25], [212.75, 35.25], [215.25, 35.25], [217.75, 35.25], [220.25, 35.25], [222.75, 35.25], [225.25, 35.25], [227.75, 35.25], [230.25, 35.25], [232.75, 35.25], [235.25, 35.25], [237.75, 35.25], [285.25, 35.25], [287.75, 35.25], [290.25, 35.25], [292.75, 35.25], [295.25, 35.25], [297.75, 35.25], [300.25, 35.25], [302.75, 35.25], [305.25, 35.25], [307.75, 35.25], [310.25, 35.25], [312.75, 35.25], [315.25, 35.25], [317.75, 35.25], [320.25, 35.25], [322.75, 35.25], [325.25, 35.25], [327.75, 35.25], [330.25, 35.25], [332.75, 35.25], [335.25, 35.25], [337.75, 35.25], [340.25, 35.25], [342.75, 35.25], [345.25, 35.25], [347.75, 35.25], [350.25, 35.25], [352.75, 35.25], [355.25, 35.25], [357.75, 35.25], [0.25, 37.75], [2.75, 37.75], [5.25, 37.75], [7.75, 37.75], [10.25, 37.75], [12.75, 37.75], [15.25, 37.75], [17.75, 37.75], [20.25, 37.75], [25.25, 37.75], [120.25, 37.75], [122.75, 37.75], [125.25, 37.75], [130.25, 37.75], [132.75, 37.75], [135.25, 37.75], [137.75, 37.75], [142.75, 37.75], [145.25, 37.75], [147.75, 37.75], [150.25, 37.75], [152.75, 37.75], [155.25, 37.75], [157.75, 37.75], [160.25, 37.75], [162.75, 37.75], [165.25, 37.75], [167.75, 37.75], [170.25, 37.75], [172.75, 37.75], [175.25, 37.75], [177.75, 37.75], [180.25, 37.75], [182.75, 37.75], [185.25, 37.75], [187.75, 37.75], [190.25, 37.75], [192.75, 37.75], [195.25, 37.75], [197.75, 37.75], [200.25, 37.75], [202.75, 37.75], [205.25, 37.75], [207.75, 37.75], [210.25, 37.75], [212.75, 37.75], [215.25, 37.75], [217.75, 37.75], [220.25, 37.75], [222.75, 37.75], [225.25, 37.75], [227.75, 37.75], [230.25, 37.75], [232.75, 37.75], [235.25, 37.75], [285.25, 37.75], [287.75, 37.75], [290.25, 37.75], [292.75, 37.75], [295.25, 37.75], [297.75, 37.75], [300.25, 37.75], [302.75, 37.75], [305.25, 37.75], [307.75, 37.75], [310.25, 37.75], [312.75, 37.75], [315.25, 37.75], [317.75, 37.75], [320.25, 37.75], [322.75, 37.75], [325.25, 37.75], [327.75, 37.75], [330.25, 37.75], [332.75, 37.75], [335.25, 37.75], [337.75, 37.75], [340.25, 37.75], [342.75, 37.75], [345.25, 37.75], [347.75, 37.75], [350.25, 37.75], [0.25, 40.25], [2.75, 40.25], [5.25, 40.25], [7.75, 40.25], [10.25, 40.25], [12.75, 40.25], [15.25, 40.25], [17.75, 40.25], [22.75, 40.25], [25.25, 40.25], [27.75, 40.25], [120.25, 40.25], [127.75, 40.25], [130.25, 40.25], [132.75, 40.25], [135.25, 40.25], [137.75, 40.25], [142.75, 40.25], [145.25, 40.25], [147.75, 40.25], [150.25, 40.25], [152.75, 40.25], [155.25, 40.25], [157.75, 40.25], [160.25, 40.25], [162.75, 40.25], [165.25, 40.25], [167.75, 40.25], [170.25, 40.25], [172.75, 40.25], [175.25, 40.25], [177.75, 40.25], [180.25, 40.25], [182.75, 40.25], [185.25, 40.25], [187.75, 40.25], [190.25, 40.25], [192.75, 40.25], [195.25, 40.25], [197.75, 40.25], [200.25, 40.25], [202.75, 40.25], [205.25, 40.25], [207.75, 40.25], [210.25, 40.25], [212.75, 40.25], [215.25, 40.25], [217.75, 40.25], [220.25, 40.25], [222.75, 40.25], [225.25, 40.25], [227.75, 40.25], [230.25, 40.25], [232.75, 40.25], [235.25, 40.25], [287.75, 40.25], [290.25, 40.25], [292.75, 40.25], [295.25, 40.25], [297.75, 40.25], [300.25, 40.25], [302.75, 40.25], [305.25, 40.25], [307.75, 40.25], [310.25, 40.25], [312.75, 40.25], [315.25, 40.25], [317.75, 40.25], [320.25, 40.25], [322.75, 40.25], [325.25, 40.25], [327.75, 40.25], [330.25, 40.25], [332.75, 40.25], [335.25, 40.25], [337.75, 40.25], [340.25, 40.25], [342.75, 40.25], [345.25, 40.25], [347.75, 40.25], [350.25, 40.25], [5.25, 42.75], [7.75, 42.75], [10.25, 42.75], [15.25, 42.75], [17.75, 42.75], [27.75, 42.75], [30.25, 42.75], [32.75, 42.75], [35.25, 42.75], [37.75, 42.75], [40.25, 42.75], [132.75, 42.75], [135.25, 42.75], [137.75, 42.75], [140.25, 42.75], [145.25, 42.75], [147.75, 42.75], [150.25, 42.75], [152.75, 42.75], [155.25, 42.75], [157.75, 42.75], [160.25, 42.75], [162.75, 42.75], [165.25, 42.75], [167.75, 42.75], [170.25, 42.75], [172.75, 42.75], [175.25, 42.75], [177.75, 42.75], [180.25, 42.75], [182.75, 42.75], [185.25, 42.75], [187.75, 42.75], [190.25, 42.75], [192.75, 42.75], [195.25, 42.75], [197.75, 42.75], [200.25, 42.75], [202.75, 42.75], [205.25, 42.75], [207.75, 42.75], [210.25, 42.75], [212.75, 42.75], [215.25, 42.75], [217.75, 42.75], [220.25, 42.75], [222.75, 42.75], [225.25, 42.75], [227.75, 42.75], [230.25, 42.75], [232.75, 42.75], [235.25, 42.75], [290.25, 42.75], [292.75, 42.75], [295.25, 42.75], [297.75, 42.75], [300.25, 42.75], [302.75, 42.75], [305.25, 42.75], [307.75, 42.75], [310.25, 42.75], [312.75, 42.75], [315.25, 42.75], [317.75, 42.75], [320.25, 42.75], [322.75, 42.75], [325.25, 42.75], [327.75, 42.75], [330.25, 42.75], [332.75, 42.75], [335.25, 42.75], [337.75, 42.75], [340.25, 42.75], [342.75, 42.75], [345.25, 42.75], [347.75, 42.75], [350.25, 42.75], [12.75, 45.25], [30.25, 45.25], [32.75, 45.25], [35.25, 45.25], [137.75, 45.25], [140.25, 45.25], [142.75, 45.25], [145.25, 45.25], [147.75, 45.25], [150.25, 45.25], [152.75, 45.25], [155.25, 45.25], [157.75, 45.25], [160.25, 45.25], [162.75, 45.25], [165.25, 45.25], [167.75, 45.25], [170.25, 45.25], [172.75, 45.25], [175.25, 45.25], [177.75, 45.25], [180.25, 45.25], [182.75, 45.25], [185.25, 45.25], [187.75, 45.25], [190.25, 45.25], [192.75, 45.25], [195.25, 45.25], [197.75, 45.25], [200.25, 45.25], [202.75, 45.25], [205.25, 45.25], [207.75, 45.25], [210.25, 45.25], [212.75, 45.25], [215.25, 45.25], [217.75, 45.25], [220.25, 45.25], [222.75, 45.25], [225.25, 45.25], [227.75, 45.25], [230.25, 45.25], [232.75, 45.25], [235.25, 45.25], [295.25, 45.25], [297.75, 45.25], [300.25, 45.25], [302.75, 45.25], [305.25, 45.25], [307.75, 45.25], [310.25, 45.25], [312.75, 45.25], [315.25, 45.25], [317.75, 45.25], [320.25, 45.25], [322.75, 45.25], [325.25, 45.25], [327.75, 45.25], [330.25, 45.25], [332.75, 45.25], [335.25, 45.25], [337.75, 45.25], [340.25, 45.25], [342.75, 45.25], [345.25, 45.25], [347.75, 45.25], [350.25, 45.25], [352.75, 45.25], [355.25, 45.25], [357.75, 45.25], [140.25, 47.75], [142.75, 47.75], [145.25, 47.75], [147.75, 47.75], [150.25, 47.75], [152.75, 47.75], [155.25, 47.75], [157.75, 47.75], [160.25, 47.75], [162.75, 47.75], [165.25, 47.75], [167.75, 47.75], [170.25, 47.75], [172.75, 47.75], [175.25, 47.75], [177.75, 47.75], [180.25, 47.75], [182.75, 47.75], [185.25, 47.75], [187.75, 47.75], [190.25, 47.75], [192.75, 47.75], [195.25, 47.75], [197.75, 47.75], [200.25, 47.75], [202.75, 47.75], [205.25, 47.75], [207.75, 47.75], [210.25, 47.75], [212.75, 47.75], [215.25, 47.75], [217.75, 47.75], [220.25, 47.75], [222.75, 47.75], [225.25, 47.75], [227.75, 47.75], [230.25, 47.75], [232.75, 47.75], [235.25, 47.75], [295.25, 47.75], [297.75, 47.75], [300.25, 47.75], [302.75, 47.75], [307.75, 47.75], [310.25, 47.75], [312.75, 47.75], [315.25, 47.75], [317.75, 47.75], [320.25, 47.75], [322.75, 47.75], [325.25, 47.75], [327.75, 47.75], [330.25, 47.75], [332.75, 47.75], [335.25, 47.75], [337.75, 47.75], [340.25, 47.75], [342.75, 47.75], [345.25, 47.75], [347.75, 47.75], [350.25, 47.75], [352.75, 47.75], [355.25, 47.75], [0.25, 50.25], [140.25, 50.25], [145.25, 50.25], [147.75, 50.25], [150.25, 50.25], [152.75, 50.25], [155.25, 50.25], [157.75, 50.25], [160.25, 50.25], [162.75, 50.25], [165.25, 50.25], [167.75, 50.25], [170.25, 50.25], [172.75, 50.25], [175.25, 50.25], [177.75, 50.25], [180.25, 50.25], [182.75, 50.25], [185.25, 50.25], [187.75, 50.25], [190.25, 50.25], [192.75, 50.25], [195.25, 50.25], [197.75, 50.25], [200.25, 50.25], [202.75, 50.25], [205.25, 50.25], [207.75, 50.25], [210.25, 50.25], [212.75, 50.25], [215.25, 50.25], [217.75, 50.25], [220.25, 50.25], [222.75, 50.25], [225.25, 50.25], [227.75, 50.25], [230.25, 50.25], [232.75, 50.25], [295.25, 50.25], [297.75, 50.25], [300.25, 50.25], [302.75, 50.25], [305.25, 50.25], [307.75, 50.25], [310.25, 50.25], [312.75, 50.25], [315.25, 50.25], [317.75, 50.25], [320.25, 50.25], [322.75, 50.25], [325.25, 50.25], [327.75, 50.25], [330.25, 50.25], [332.75, 50.25], [335.25, 50.25], [337.75, 50.25], [340.25, 50.25], [342.75, 50.25], [345.25, 50.25], [347.75, 50.25], [350.25, 50.25], [352.75, 50.25], [355.25, 50.25], [357.75, 50.25], [2.75, 52.75], [5.25, 52.75], [145.25, 52.75], [147.75, 52.75], [150.25, 52.75], [152.75, 52.75], [155.25, 52.75], [160.25, 52.75], [162.75, 52.75], [165.25, 52.75], [167.75, 52.75], [170.25, 52.75], [172.75, 52.75], [175.25, 52.75], [177.75, 52.75], [180.25, 52.75], [182.75, 52.75], [185.25, 52.75], [187.75, 52.75], [190.25, 52.75], [192.75, 52.75], [195.25, 52.75], [197.75, 52.75], [200.25, 52.75], [202.75, 52.75], [205.25, 52.75], [207.75, 52.75], [210.25, 52.75], [212.75, 52.75], [215.25, 52.75], [217.75, 52.75], [220.25, 52.75], [222.75, 52.75], [225.25, 52.75], [227.75, 52.75], [230.25, 52.75], [277.75, 52.75], [280.25, 52.75], [305.25, 52.75], [307.75, 52.75], [310.25, 52.75], [312.75, 52.75], [315.25, 52.75], [317.75, 52.75], [320.25, 52.75], [322.75, 52.75], [325.25, 52.75], [327.75, 52.75], [330.25, 52.75], [332.75, 52.75], [335.25, 52.75], [337.75, 52.75], [340.25, 52.75], [342.75, 52.75], [345.25, 52.75], [347.75, 52.75], [350.25, 52.75], [355.25, 52.75], [0.25, 55.25], [2.75, 55.25], [5.25, 55.25], [7.75, 55.25], [10.25, 55.25], [12.75, 55.25], [15.25, 55.25], [17.75, 55.25], [20.25, 55.25], [135.25, 55.25], [137.75, 55.25], [140.25, 55.25], [142.75, 55.25], [145.25, 55.25], [147.75, 55.25], [150.25, 55.25], [152.75, 55.25], [155.25, 55.25], [162.75, 55.25], [165.25, 55.25], [167.75, 55.25], [170.25, 55.25], [172.75, 55.25], [175.25, 55.25], [177.75, 55.25], [180.25, 55.25], [182.75, 55.25], [185.25, 55.25], [187.75, 55.25], [190.25, 55.25], [192.75, 55.25], [195.25, 55.25], [197.75, 55.25], [200.25, 55.25], [202.75, 55.25], [205.25, 55.25], [207.75, 55.25], [210.25, 55.25], [212.75, 55.25], [215.25, 55.25], [217.75, 55.25], [220.25, 55.25], [222.75, 55.25], [225.25, 55.25], [275.25, 55.25], [277.75, 55.25], [280.25, 55.25], [282.75, 55.25], [300.25, 55.25], [302.75, 55.25], [305.25, 55.25], [307.75, 55.25], [310.25, 55.25], [312.75, 55.25], [315.25, 55.25], [317.75, 55.25], [320.25, 55.25], [322.75, 55.25], [325.25, 55.25], [327.75, 55.25], [330.25, 55.25], [332.75, 55.25], [335.25, 55.25], [337.75, 55.25], [340.25, 55.25], [342.75, 55.25], [345.25, 55.25], [347.75, 55.25], [350.25, 55.25], [352.75, 55.25], [355.25, 55.25], [0.25, 57.75], [2.75, 57.75], [5.25, 57.75], [7.75, 57.75], [10.25, 57.75], [17.75, 57.75], [20.25, 57.75], [22.75, 57.75], [140.25, 57.75], [142.75, 57.75], [145.25, 57.75], [147.75, 57.75], [150.25, 57.75], [152.75, 57.75], [155.25, 57.75], [157.75, 57.75], [162.75, 57.75], [165.25, 57.75], [167.75, 57.75], [170.25, 57.75], [172.75, 57.75], [175.25, 57.75], [177.75, 57.75], [180.25, 57.75], [182.75, 57.75], [185.25, 57.75], [187.75, 57.75], [190.25, 57.75], [192.75, 57.75], [195.25, 57.75], [197.75, 57.75], [200.25, 57.75], [205.25, 57.75], [207.75, 57.75], [210.25, 57.75], [212.75, 57.75], [215.25, 57.75], [217.75, 57.75], [220.25, 57.75], [222.75, 57.75], [267.75, 57.75], [270.25, 57.75], [272.75, 57.75], [275.25, 57.75], [277.75, 57.75], [280.25, 57.75], [282.75, 57.75], [297.75, 57.75], [300.25, 57.75], [302.75, 57.75], [305.25, 57.75], [307.75, 57.75], [310.25, 57.75], [312.75, 57.75], [315.25, 57.75], [317.75, 57.75], [320.25, 57.75], [322.75, 57.75], [325.25, 57.75], [327.75, 57.75], [330.25, 57.75], [332.75, 57.75], [335.25, 57.75], [337.75, 57.75], [340.25, 57.75], [342.75, 57.75], [345.25, 57.75], [347.75, 57.75], [350.25, 57.75], [352.75, 57.75], [357.75, 57.75], [0.25, 60.25], [2.75, 60.25], [5.25, 60.25], [20.25, 60.25], [22.75, 60.25], [25.25, 60.25], [27.75, 60.25], [155.25, 60.25], [157.75, 60.25], [160.25, 60.25], [165.25, 60.25], [167.75, 60.25], [170.25, 60.25], [172.75, 60.25], [175.25, 60.25], [177.75, 60.25], [180.25, 60.25], [182.75, 60.25], [185.25, 60.25], [187.75, 60.25], [190.25, 60.25], [192.75, 60.25], [195.25, 60.25], [197.75, 60.25], [207.75, 60.25], [212.75, 60.25], [215.25, 60.25], [217.75, 60.25], [265.25, 60.25], [267.75, 60.25], [270.25, 60.25], [272.75, 60.25], [275.25, 60.25], [277.75, 60.25], [280.25, 60.25], [282.75, 60.25], [290.25, 60.25], [292.75, 60.25], [295.25, 60.25], [297.75, 60.25], [300.25, 60.25], [302.75, 60.25], [305.25, 60.25], [307.75, 60.25], [310.25, 60.25], [312.75, 60.25], [315.25, 60.25], [317.75, 60.25], [320.25, 60.25], [322.75, 60.25], [325.25, 60.25], [327.75, 60.25], [330.25, 60.25], [332.75, 60.25], [335.25, 60.25], [337.75, 60.25], [340.25, 60.25], [342.75, 60.25], [345.25, 60.25], [347.75, 60.25], [350.25, 60.25], [352.75, 60.25], [355.25, 60.25], [357.75, 60.25], [0.25, 62.75], [2.75, 62.75], [5.25, 62.75], [17.75, 62.75], [20.25, 62.75], [165.25, 62.75], [177.75, 62.75], [180.25, 62.75], [182.75, 62.75], [185.25, 62.75], [187.75, 62.75], [190.25, 62.75], [192.75, 62.75], [195.25, 62.75], [267.75, 62.75], [270.25, 62.75], [272.75, 62.75], [275.25, 62.75], [277.75, 62.75], [280.25, 62.75], [282.75, 62.75], [285.25, 62.75], [287.75, 62.75], [290.25, 62.75], [292.75, 62.75], [295.25, 62.75], [297.75, 62.75], [300.25, 62.75], [302.75, 62.75], [305.25, 62.75], [307.75, 62.75], [317.75, 62.75], [320.25, 62.75], [322.75, 62.75], [325.25, 62.75], [327.75, 62.75], [330.25, 62.75], [332.75, 62.75], [335.25, 62.75], [337.75, 62.75], [340.25, 62.75], [342.75, 62.75], [345.25, 62.75], [347.75, 62.75], [350.25, 62.75], [352.75, 62.75], [355.25, 62.75], [357.75, 62.75], [0.25, 65.25], [2.75, 65.25], [5.25, 65.25], [7.75, 65.25], [10.25, 65.25], [22.75, 65.25], [25.25, 65.25], [35.25, 65.25], [37.75, 65.25], [40.25, 65.25], [180.25, 65.25], [182.75, 65.25], [187.75, 65.25], [190.25, 65.25], [192.75, 65.25], [292.75, 65.25], [295.25, 65.25], [297.75, 65.25], [300.25, 65.25], [302.75, 65.25], [305.25, 65.25], [307.75, 65.25], [320.25, 65.25], [322.75, 65.25], [325.25, 65.25], [327.75, 65.25], [330.25, 65.25], [332.75, 65.25], [335.25, 65.25], [347.75, 65.25], [350.25, 65.25], [352.75, 65.25], [355.25, 65.25], [357.75, 65.25], [0.25, 67.75], [2.75, 67.75], [5.25, 67.75], [7.75, 67.75], [10.25, 67.75], [12.75, 67.75], [15.25, 67.75], [40.25, 67.75], [42.75, 67.75], [45.25, 67.75], [47.75, 67.75], [50.25, 67.75], [185.25, 67.75], [187.75, 67.75], [190.25, 67.75], [192.75, 67.75], [195.25, 67.75], [295.25, 67.75], [297.75, 67.75], [300.25, 67.75], [302.75, 67.75], [305.25, 67.75], [327.75, 67.75], [330.25, 67.75], [332.75, 67.75], [335.25, 67.75], [337.75, 67.75], [340.25, 67.75], [342.75, 67.75], [345.25, 67.75], [347.75, 67.75], [350.25, 67.75], [352.75, 67.75], [355.25, 67.75], [357.75, 67.75], [0.25, 70.25], [2.75, 70.25], [5.25, 70.25], [7.75, 70.25], [10.25, 70.25], [12.75, 70.25], [15.25, 70.25], [17.75, 70.25], [20.25, 70.25], [22.75, 70.25], [25.25, 70.25], [30.25, 70.25], [32.75, 70.25], [35.25, 70.25], [37.75, 70.25], [40.25, 70.25], [42.75, 70.25], [45.25, 70.25], [47.75, 70.25], [50.25, 70.25], [52.75, 70.25], [55.25, 70.25], [57.75, 70.25], [60.25, 70.25], [160.25, 70.25], [162.75, 70.25], [165.25, 70.25], [167.75, 70.25], [170.25, 70.25], [172.75, 70.25], [175.25, 70.25], [177.75, 70.25], [180.25, 70.25], [182.75, 70.25], [185.25, 70.25], [187.75, 70.25], [190.25, 70.25], [192.75, 70.25], [195.25, 70.25], [197.75, 70.25], [207.75, 70.25], [210.25, 70.25], [212.75, 70.25], [215.25, 70.25], [217.75, 70.25], [220.25, 70.25], [222.75, 70.25], [225.25, 70.25], [227.75, 70.25], [230.25, 70.25], [232.75, 70.25], [235.25, 70.25], [237.75, 70.25], [240.25, 70.25], [292.75, 70.25], [295.25, 70.25], [297.75, 70.25], [300.25, 70.25], [302.75, 70.25], [305.25, 70.25], [307.75, 70.25], [335.25, 70.25], [337.75, 70.25], [340.25, 70.25], [342.75, 70.25], [345.25, 70.25], [347.75, 70.25], [350.25, 70.25], [352.75, 70.25], [355.25, 70.25], [357.75, 70.25], [0.25, 72.75], [2.75, 72.75], [5.25, 72.75], [7.75, 72.75], [10.25, 72.75], [12.75, 72.75], [15.25, 72.75], [17.75, 72.75], [20.25, 72.75], [22.75, 72.75], [25.25, 72.75], [27.75, 72.75], [30.25, 72.75], [32.75, 72.75], [35.25, 72.75], [37.75, 72.75], [40.25, 72.75], [42.75, 72.75], [45.25, 72.75], [47.75, 72.75], [50.25, 72.75], [52.75, 72.75], [57.75, 72.75], [60.25, 72.75], [62.75, 72.75], [65.25, 72.75], [67.75, 72.75], [70.25, 72.75], [72.75, 72.75], [75.25, 72.75], [77.75, 72.75], [80.25, 72.75], [120.25, 72.75], [122.75, 72.75], [130.25, 72.75], [132.75, 72.75], [135.25, 72.75], [137.75, 72.75], [140.25, 72.75], [142.75, 72.75], [145.25, 72.75], [147.75, 72.75], [150.25, 72.75], [152.75, 72.75], [155.25, 72.75], [157.75, 72.75], [160.25, 72.75], [162.75, 72.75], [165.25, 72.75], [167.75, 72.75], [170.25, 72.75], [172.75, 72.75], [175.25, 72.75], [177.75, 72.75], [180.25, 72.75], [182.75, 72.75], [185.25, 72.75], [187.75, 72.75], [190.25, 72.75], [192.75, 72.75], [195.25, 72.75], [197.75, 72.75], [200.25, 72.75], [202.75, 72.75], [205.25, 72.75], [207.75, 72.75], [210.25, 72.75], [212.75, 72.75], [215.25, 72.75], [217.75, 72.75], [220.25, 72.75], [222.75, 72.75], [225.25, 72.75], [227.75, 72.75], [230.25, 72.75], [232.75, 72.75], [235.25, 72.75], [242.75, 72.75], [247.75, 72.75], [250.25, 72.75], [255.25, 72.75], [257.75, 72.75], [262.75, 72.75], [267.75, 72.75], [270.25, 72.75], [282.75, 72.75], [285.25, 72.75], [287.75, 72.75], [290.25, 72.75], [292.75, 72.75], [295.25, 72.75], [297.75, 72.75], [300.25, 72.75], [302.75, 72.75], [337.75, 72.75], [340.25, 72.75], [342.75, 72.75], [345.25, 72.75], [347.75, 72.75], [350.25, 72.75], [352.75, 72.75], [355.25, 72.75], [357.75, 72.75], [0.25, 75.25], [2.75, 75.25], [5.25, 75.25], [7.75, 75.25], [10.25, 75.25], [12.75, 75.25], [15.25, 75.25], [17.75, 75.25], [20.25, 75.25], [22.75, 75.25], [25.25, 75.25], [27.75, 75.25], [30.25, 75.25], [32.75, 75.25], [35.25, 75.25], [37.75, 75.25], [40.25, 75.25], [42.75, 75.25], [45.25, 75.25], [47.75, 75.25], [50.25, 75.25], [52.75, 75.25], [55.25, 75.25], [57.75, 75.25], [60.25, 75.25], [62.75, 75.25], [65.25, 75.25], [67.75, 75.25], [70.25, 75.25], [72.75, 75.25], [75.25, 75.25], [77.75, 75.25], [80.25, 75.25], [82.75, 75.25], [85.25, 75.25], [87.75, 75.25], [90.25, 75.25], [112.75, 75.25], [115.25, 75.25], [117.75, 75.25], [120.25, 75.25], [122.75, 75.25], [125.25, 75.25], [127.75, 75.25], [130.25, 75.25], [132.75, 75.25], [135.25, 75.25], [137.75, 75.25], [142.75, 75.25], [145.25, 75.25], [147.75, 75.25], [150.25, 75.25], [152.75, 75.25], [155.25, 75.25], [157.75, 75.25], [160.25, 75.25], [162.75, 75.25], [165.25, 75.25], [167.75, 75.25], [170.25, 75.25], [172.75, 75.25], [175.25, 75.25], [177.75, 75.25], [180.25, 75.25], [182.75, 75.25], [185.25, 75.25], [187.75, 75.25], [190.25, 75.25], [192.75, 75.25], [195.25, 75.25], [197.75, 75.25], [200.25, 75.25], [202.75, 75.25], [205.25, 75.25], [207.75, 75.25], [210.25, 75.25], [212.75, 75.25], [215.25, 75.25], [217.75, 75.25], [220.25, 75.25], [222.75, 75.25], [225.25, 75.25], [227.75, 75.25], [230.25, 75.25], [232.75, 75.25], [235.25, 75.25], [237.75, 75.25], [240.25, 75.25], [242.75, 75.25], [245.25, 75.25], [247.75, 75.25], [250.25, 75.25], [255.25, 75.25], [257.75, 75.25], [260.25, 75.25], [262.75, 75.25], [267.75, 75.25], [272.75, 75.25], [280.25, 75.25], [282.75, 75.25], [285.25, 75.25], [287.75, 75.25], [290.25, 75.25], [292.75, 75.25], [295.25, 75.25], [297.75, 75.25], [300.25, 75.25], [340.25, 75.25], [342.75, 75.25], [345.25, 75.25], [347.75, 75.25], [350.25, 75.25], [352.75, 75.25], [355.25, 75.25], [357.75, 75.25], [0.25, 77.75], [2.75, 77.75], [5.25, 77.75], [7.75, 77.75], [10.25, 77.75], [12.75, 77.75], [17.75, 77.75], [20.25, 77.75], [22.75, 77.75], [25.25, 77.75], [27.75, 77.75], [30.25, 77.75], [32.75, 77.75], [35.25, 77.75], [37.75, 77.75], [40.25, 77.75], [42.75, 77.75], [45.25, 77.75], [47.75, 77.75], [50.25, 77.75], [52.75, 77.75], [55.25, 77.75], [57.75, 77.75], [60.25, 77.75], [62.75, 77.75], [65.25, 77.75], [67.75, 77.75], [70.25, 77.75], [72.75, 77.75], [75.25, 77.75], [77.75, 77.75], [80.25, 77.75], [82.75, 77.75], [85.25, 77.75], [87.75, 77.75], [90.25, 77.75], [92.75, 77.75], [95.25, 77.75], [97.75, 77.75], [100.25, 77.75], [102.75, 77.75], [105.25, 77.75], [107.75, 77.75], [110.25, 77.75], [112.75, 77.75], [115.25, 77.75], [117.75, 77.75], [120.25, 77.75], [122.75, 77.75], [125.25, 77.75], [127.75, 77.75], [130.25, 77.75], [132.75, 77.75], [135.25, 77.75], [137.75, 77.75], [140.25, 77.75], [142.75, 77.75], [145.25, 77.75], [147.75, 77.75], [150.25, 77.75], [152.75, 77.75], [155.25, 77.75], [157.75, 77.75], [160.25, 77.75], [162.75, 77.75], [165.25, 77.75], [167.75, 77.75], [170.25, 77.75], [172.75, 77.75], [175.25, 77.75], [177.75, 77.75], [180.25, 77.75], [182.75, 77.75], [185.25, 77.75], [187.75, 77.75], [190.25, 77.75], [192.75, 77.75], [195.25, 77.75], [197.75, 77.75], [200.25, 77.75], [202.75, 77.75], [205.25, 77.75], [207.75, 77.75], [210.25, 77.75], [212.75, 77.75], [215.25, 77.75], [217.75, 77.75], [220.25, 77.75], [222.75, 77.75], [225.25, 77.75], [227.75, 77.75], [230.25, 77.75], [232.75, 77.75], [235.25, 77.75], [237.75, 77.75], [240.25, 77.75], [242.75, 77.75], [245.25, 77.75], [247.75, 77.75], [250.25, 77.75], [252.75, 77.75], [255.25, 77.75], [257.75, 77.75], [260.25, 77.75], [262.75, 77.75], [265.25, 77.75], [267.75, 77.75], [270.25, 77.75], [272.75, 77.75], [275.25, 77.75], [282.75, 77.75], [285.25, 77.75], [287.75, 77.75], [290.25, 77.75], [340.25, 77.75], [342.75, 77.75], [345.25, 77.75], [347.75, 77.75], [350.25, 77.75], [352.75, 77.75], [355.25, 77.75], [357.75, 77.75], [0.25, 80.25], [2.75, 80.25], [5.25, 80.25], [7.75, 80.25], [10.25, 80.25], [12.75, 80.25], [15.25, 80.25], [17.75, 80.25], [20.25, 80.25], [22.75, 80.25], [25.25, 80.25], [27.75, 80.25], [30.25, 80.25], [32.75, 80.25], [35.25, 80.25], [37.75, 80.25], [40.25, 80.25], [42.75, 80.25], [45.25, 80.25], [47.75, 80.25], [50.25, 80.25], [52.75, 80.25], [55.25, 80.25], [57.75, 80.25], [60.25, 80.25], [62.75, 80.25], [65.25, 80.25], [67.75, 80.25], [70.25, 80.25], [72.75, 80.25], [75.25, 80.25], [77.75, 80.25], [80.25, 80.25], [82.75, 80.25], [85.25, 80.25], [87.75, 80.25], [90.25, 80.25], [92.75, 80.25], [97.75, 80.25], [100.25, 80.25], [102.75, 80.25], [105.25, 80.25], [107.75, 80.25], [110.25, 80.25], [112.75, 80.25], [115.25, 80.25], [117.75, 80.25], [120.25, 80.25], [122.75, 80.25], [125.25, 80.25], [127.75, 80.25], [130.25, 80.25], [132.75, 80.25], [135.25, 80.25], [137.75, 80.25], [140.25, 80.25], [142.75, 80.25], [145.25, 80.25], [147.75, 80.25], [150.25, 80.25], [152.75, 80.25], [155.25, 80.25], [157.75, 80.25], [160.25, 80.25], [162.75, 80.25], [165.25, 80.25], [167.75, 80.25], [170.25, 80.25], [172.75, 80.25], [175.25, 80.25], [177.75, 80.25], [180.25, 80.25], [182.75, 80.25], [185.25, 80.25], [187.75, 80.25], [190.25, 80.25], [192.75, 80.25], [195.25, 80.25], [197.75, 80.25], [200.25, 80.25], [202.75, 80.25], [205.25, 80.25], [207.75, 80.25], [210.25, 80.25], [212.75, 80.25], [215.25, 80.25], [217.75, 80.25], [220.25, 80.25], [222.75, 80.25], [225.25, 80.25], [227.75, 80.25], [230.25, 80.25], [232.75, 80.25], [235.25, 80.25], [237.75, 80.25], [240.25, 80.25], [242.75, 80.25], [245.25, 80.25], [247.75, 80.25], [250.25, 80.25], [252.75, 80.25], [255.25, 80.25], [257.75, 80.25], [260.25, 80.25], [262.75, 80.25], [265.25, 80.25], [270.25, 80.25], [272.75, 80.25], [275.25, 80.25], [277.75, 80.25], [290.25, 80.25], [292.75, 80.25], [295.25, 80.25], [342.75, 80.25], [345.25, 80.25], [347.75, 80.25], [350.25, 80.25], [352.75, 80.25], [355.25, 80.25], [357.75, 80.25], [0.25, 82.75], [2.75, 82.75], [5.25, 82.75], [7.75, 82.75], [10.25, 82.75], [12.75, 82.75], [15.25, 82.75], [17.75, 82.75], [20.25, 82.75], [22.75, 82.75], [25.25, 82.75], [27.75, 82.75], [30.25, 82.75], [32.75, 82.75], [35.25, 82.75], [37.75, 82.75], [40.25, 82.75], [42.75, 82.75], [45.25, 82.75], [47.75, 82.75], [50.25, 82.75], [52.75, 82.75], [55.25, 82.75], [57.75, 82.75], [60.25, 82.75], [62.75, 82.75], [65.25, 82.75], [67.75, 82.75], [70.25, 82.75], [72.75, 82.75], [75.25, 82.75], [77.75, 82.75], [80.25, 82.75], [82.75, 82.75], [85.25, 82.75], [87.75, 82.75], [90.25, 82.75], [92.75, 82.75], [95.25, 82.75], [97.75, 82.75], [100.25, 82.75], [102.75, 82.75], [105.25, 82.75], [107.75, 82.75], [110.25, 82.75], [112.75, 82.75], [115.25, 82.75], [117.75, 82.75], [120.25, 82.75], [122.75, 82.75], [125.25, 82.75], [127.75, 82.75], [130.25, 82.75], [132.75, 82.75], [135.25, 82.75], [137.75, 82.75], [140.25, 82.75], [142.75, 82.75], [145.25, 82.75], [147.75, 82.75], [150.25, 82.75], [152.75, 82.75], [155.25, 82.75], [157.75, 82.75], [160.25, 82.75], [162.75, 82.75], [165.25, 82.75], [167.75, 82.75], [170.25, 82.75], [172.75, 82.75], [175.25, 82.75], [177.75, 82.75], [180.25, 82.75], [182.75, 82.75], [185.25, 82.75], [187.75, 82.75], [190.25, 82.75], [192.75, 82.75], [195.25, 82.75], [197.75, 82.75], [200.25, 82.75], [202.75, 82.75], [205.25, 82.75], [207.75, 82.75], [210.25, 82.75], [212.75, 82.75], [215.25, 82.75], [217.75, 82.75], [220.25, 82.75], [222.75, 82.75], [225.25, 82.75], [227.75, 82.75], [230.25, 82.75], [232.75, 82.75], [235.25, 82.75], [237.75, 82.75], [240.25, 82.75], [242.75, 82.75], [245.25, 82.75], [247.75, 82.75], [250.25, 82.75], [252.75, 82.75], [255.25, 82.75], [257.75, 82.75], [260.25, 82.75], [262.75, 82.75], [265.25, 82.75], [267.75, 82.75], [270.25, 82.75], [272.75, 82.75], [275.25, 82.75], [277.75, 82.75], [280.25, 82.75], [290.25, 82.75], [292.75, 82.75], [295.25, 82.75], [297.75, 82.75], [300.25, 82.75], [302.75, 82.75], [305.25, 82.75], [307.75, 82.75], [310.25, 82.75], [312.75, 82.75], [337.75, 82.75], [340.25, 82.75], [342.75, 82.75], [345.25, 82.75], [347.75, 82.75], [350.25, 82.75], [352.75, 82.75], [355.25, 82.75], [357.75, 82.75], [0.25, 85.25], [2.75, 85.25], [5.25, 85.25], [7.75, 85.25], [10.25, 85.25], [12.75, 85.25], [15.25, 85.25], [17.75, 85.25], [20.25, 85.25], [22.75, 85.25], [25.25, 85.25], [27.75, 85.25], [30.25, 85.25], [32.75, 85.25], [35.25, 85.25], [37.75, 85.25], [40.25, 85.25], [42.75, 85.25], [45.25, 85.25], [47.75, 85.25], [50.25, 85.25], [52.75, 85.25], [55.25, 85.25], [57.75, 85.25], [60.25, 85.25], [62.75, 85.25], [65.25, 85.25], [67.75, 85.25], [70.25, 85.25], [72.75, 85.25], [75.25, 85.25], [77.75, 85.25], [80.25, 85.25], [82.75, 85.25], [85.25, 85.25], [87.75, 85.25], [90.25, 85.25], [92.75, 85.25], [95.25, 85.25], [97.75, 85.25], [100.25, 85.25], [102.75, 85.25], [105.25, 85.25], [107.75, 85.25], [110.25, 85.25], [112.75, 85.25], [115.25, 85.25], [117.75, 85.25], [120.25, 85.25], [122.75, 85.25], [125.25, 85.25], [127.75, 85.25], [130.25, 85.25], [132.75, 85.25], [135.25, 85.25], [137.75, 85.25], [140.25, 85.25], [142.75, 85.25], [145.25, 85.25], [147.75, 85.25], [150.25, 85.25], [152.75, 85.25], [155.25, 85.25], [157.75, 85.25], [160.25, 85.25], [162.75, 85.25], [165.25, 85.25], [167.75, 85.25], [170.25, 85.25], [172.75, 85.25], [175.25, 85.25], [177.75, 85.25], [180.25, 85.25], [182.75, 85.25], [185.25, 85.25], [187.75, 85.25], [190.25, 85.25], [192.75, 85.25], [195.25, 85.25], [197.75, 85.25], [200.25, 85.25], [202.75, 85.25], [205.25, 85.25], [207.75, 85.25], [210.25, 85.25], [212.75, 85.25], [215.25, 85.25], [217.75, 85.25], [220.25, 85.25], [222.75, 85.25], [225.25, 85.25], [227.75, 85.25], [230.25, 85.25], [232.75, 85.25], [235.25, 85.25], [237.75, 85.25], [240.25, 85.25], [242.75, 85.25], [245.25, 85.25], [247.75, 85.25], [250.25, 85.25], [252.75, 85.25], [255.25, 85.25], [257.75, 85.25], [260.25, 85.25], [262.75, 85.25], [265.25, 85.25], [267.75, 85.25], [270.25, 85.25], [272.75, 85.25], [275.25, 85.25], [277.75, 85.25], [280.25, 85.25], [282.75, 85.25], [285.25, 85.25], [287.75, 85.25], [290.25, 85.25], [292.75, 85.25], [295.25, 85.25], [297.75, 85.25], [300.25, 85.25], [302.75, 85.25], [305.25, 85.25], [307.75, 85.25], [310.25, 85.25], [312.75, 85.25], [315.25, 85.25], [317.75, 85.25], [320.25, 85.25], [322.75, 85.25], [325.25, 85.25], [327.75, 85.25], [330.25, 85.25], [332.75, 85.25], [335.25, 85.25], [337.75, 85.25], [340.25, 85.25], [342.75, 85.25], [345.25, 85.25], [347.75, 85.25], [350.25, 85.25], [352.75, 85.25], [355.25, 85.25], [357.75, 85.25], [0.25, 87.75], [2.75, 87.75], [5.25, 87.75], [7.75, 87.75], [10.25, 87.75], [12.75, 87.75], [15.25, 87.75], [17.75, 87.75], [20.25, 87.75], [22.75, 87.75], [25.25, 87.75], [27.75, 87.75], [30.25, 87.75], [32.75, 87.75], [35.25, 87.75], [37.75, 87.75], [40.25, 87.75], [42.75, 87.75], [45.25, 87.75], [47.75, 87.75], [50.25, 87.75], [52.75, 87.75], [55.25, 87.75], [57.75, 87.75], [60.25, 87.75], [62.75, 87.75], [65.25, 87.75], [67.75, 87.75], [70.25, 87.75], [72.75, 87.75], [75.25, 87.75], [77.75, 87.75], [80.25, 87.75], [82.75, 87.75], [85.25, 87.75], [87.75, 87.75], [90.25, 87.75], [92.75, 87.75], [95.25, 87.75], [97.75, 87.75], [100.25, 87.75], [102.75, 87.75], [105.25, 87.75], [107.75, 87.75], [110.25, 87.75], [112.75, 87.75], [115.25, 87.75], [117.75, 87.75], [120.25, 87.75], [122.75, 87.75], [125.25, 87.75], [127.75, 87.75], [130.25, 87.75], [132.75, 87.75], [135.25, 87.75], [137.75, 87.75], [140.25, 87.75], [142.75, 87.75], [145.25, 87.75], [147.75, 87.75], [150.25, 87.75], [152.75, 87.75], [155.25, 87.75], [157.75, 87.75], [160.25, 87.75], [162.75, 87.75], [165.25, 87.75], [167.75, 87.75], [170.25, 87.75], [172.75, 87.75], [175.25, 87.75], [177.75, 87.75], [180.25, 87.75], [182.75, 87.75], [185.25, 87.75], [187.75, 87.75], [190.25, 87.75], [192.75, 87.75], [195.25, 87.75], [197.75, 87.75], [200.25, 87.75], [202.75, 87.75], [205.25, 87.75], [207.75, 87.75], [210.25, 87.75], [212.75, 87.75], [215.25, 87.75], [217.75, 87.75], [220.25, 87.75], [222.75, 87.75], [225.25, 87.75], [227.75, 87.75], [230.25, 87.75], [232.75, 87.75], [235.25, 87.75], [237.75, 87.75], [240.25, 87.75], [242.75, 87.75], [245.25, 87.75], [247.75, 87.75], [250.25, 87.75], [252.75, 87.75], [255.25, 87.75], [257.75, 87.75], [260.25, 87.75], [262.75, 87.75], [265.25, 87.75], [267.75, 87.75], [270.25, 87.75], [272.75, 87.75], [275.25, 87.75], [277.75, 87.75], [280.25, 87.75], [282.75, 87.75], [285.25, 87.75], [287.75, 87.75], [290.25, 87.75], [292.75, 87.75], [295.25, 87.75], [297.75, 87.75], [300.25, 87.75], [302.75, 87.75], [305.25, 87.75], [307.75, 87.75], [310.25, 87.75], [312.75, 87.75], [315.25, 87.75], [317.75, 87.75], [320.25, 87.75], [322.75, 87.75], [325.25, 87.75], [327.75, 87.75], [330.25, 87.75], [332.75, 87.75], [335.25, 87.75], [337.75, 87.75], [340.25, 87.75], [342.75, 87.75], [345.25, 87.75], [347.75, 87.75], [350.25, 87.75], [352.75, 87.75], [355.25, 87.75], [357.75, 87.75]]\n"
          ]
        }
      ]
    }
  ]
}